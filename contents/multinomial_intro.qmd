---
title: "Introduction to Multinomial Models"
page-layout: full
editor: source
toc-depth: 5
execute:
  message: false
  warning: false
---

## Set up

This section uses the following packages: 

```{r, include = FALSE}
library(patchwork)
```

```{r}
library(tidyverse)
library(janitor)
library(skimr)
library(nnet)
library(broom)
library(gtsummary)
library(sjPlot)
```

## Introduction

In political science, we are often interested in measuring the probability of success for more than two outcomes. For example, we may want to know the probability that an individual will vote for a specific candidate in an election. If our individuals have more than two candidates from which to choose, we can take advantage of multinomial modelling. 

Multinomial models are linear regression models that measure the probability of success across outcomes with more than two options. As with binary outcomes, we can choose between two common approaches to these models: logit and probit regression. Multinomial logit and probit models are an extension of the binary latent variable models we discussed in the previous section. Rather than modelling the choice made between two options - success or failure, vote or not vote - we can efficiently model the choice between many different options, for example: strongly favour, weakly favour, weakly dislike, strongly dislike. 

Unlike previously, the choice between a multinomial logit model and a multinomial probit model is not trivial. This difference centres on the Independence of Irrelevant Alternatives (IIA) assumption. We will discuss this shortly. 

To explore this question, we will look at students' level of support for the Iraq war based on their gender and their party affiliation (measured on a seven-point scale).

Let's load and clean our data:

```{r}
student_survey <- rio::import("/Users/harrietgoers/Downloads/student vote GVPT 729a.dta") |> 
  transmute(warsup = factor(warsup, 
                            levels = c(1, 2, 3, 4),
                            labels = c("Strongly support",
                                       "Somewhat support",
                                       "Somewhat oppose",
                                       "Strongly oppose")), 
            female = factor(female, 
                            levels = c(0, 1), 
                            labels = c("Not female", "Female")), 
            pid = factor(pid,
                         levels = c(1, 2, 3, 4, 5, 6, 7),
                         labels = c("Strong democrat",
                                    "Weak democrat",
                                    "Independent democrat",
                                    "Independent",
                                    "Independent republican",
                                    "Weak republican",
                                    "Strong republican"))) |> 
  labelled::set_variable_labels(warsup = "Support for the war",
                                female = "Female",
                                pid = "Party ID") |> 
  drop_na()

head(student_survey)
```

Let's look at our dataset: 

```{r}
skim(student_survey)
```

## Multinomial logit regression

In a binomial logistic regression, we measure the probability that our outcome, $y$, will take on one of two options: usually $y = 0$ or $y = 1$. For multinomial logit regression, we model the probability that our outcome, $y$, will take on one of more than two options. You need to select a baseline category from which you will compare your probability of success for all other categories.

> A multinomial model is genuinely an extension of binomial logistic regression. We could fit a series of binomial logistic regression models and get the same outcome.[^1]

[^1]: Well... almost the same outcome. Multinomial regression is more efficient. Our errors will be slightly different to those we would obtain if we fit multiple binary regressions. 

Recall that a binary logistic regression is modelled as such:

$$
Pr(Y = 1 | X) = ln(\frac{P}{1-P}) = \beta_0 + \beta_1X_1 + ... = \beta_kX_k
$$

This provides the log odds ratio of success, or $Y = 1$ for a binary outcome. 

For multinomial logistic regression, our outcome is no longer the log odds of success against failure. Instead, we are measuring the log odds of the probability of one category over the probability of a base category. This is called the **relative log odds ratio**.

$$
ln(\frac{P_{category 2}}{P_{category 1}})
$$

Let's explore this by looking at the level of support university students had for the Iraq war. Our outcome variable can take on one of four options: strong opposition, weak opposition, weak support, and strong support for the war. Let's pick strong support as our baseline category. We now need to calculate three equations that model the relative log odds ratio that an individual will express strong opposition, weak opposition, or weak support for the war relative to the probability that they will express strong support for the war: 

$$
Pr(y = strong\_opposition | X) = log(\frac{Pr(y = strong\_opposition)}{Pr(y = strong\_support)})
$$

$$
Pr(y = weak\_opposition | X) = log(\frac{Pr(y = weak\_opposition)}{Pr(y = strong\_support)})
$$

$$
Pr(y = weak\_support | X) = log(\frac{Pr(y = weak\_support)}{Pr(y = strong\_support)})
$$

### Assumptions

#### Independence of Irrelevant Alternatives assumption

Multinomial logit assumes that the relative probability of existing choices is not affected by changes to the choice set. For example, if you remove a choice from your model, that choice's probability of success will be distributed evenly among the remaining choices. Their relative contribution to the probability of success remains the same. This is a very strong assumption. 

For example, I am choosing between four candidates for the Democratic primary. The probability that I will vote for each is defined by my preferences for their policy platforms. They are as below: 

$$
Pr(cand\_1) = \frac{1}{10}
$$

$$
Pr(cand\_2) = \frac{1}{2}
$$

$$
Pr(cand\_3) = \frac{3}{10}
$$

$$
Pr(cand\_4) = \frac{1}{10}
$$

Candidate 3 drops out of the race. They were a rather centerist candidate among Democrats, so their supporters do not flow to one specific candidate among those left. I follow this general trend and the probability that I would vote for that candidate distributes itself eveningly among the remaining candidates. Each gets a third of Candidate 3's probability: $\frac{1}{10}$. The new distribution of the probability that I will vote for the candidates is as follows: 

$$
Pr(cand\_1) = \frac{1}{5}
$$

$$
Pr(cand\_2) = \frac{3}{5}
$$

$$
Pr(cand\_4) = \frac{1}{5}
$$

Because the probability of success depends on the candidate, I should use a multinomial logistic regression to model my preferences across these choices. 

#### Categorical outcome

Like binary logistic regression, this model assumes that the outcome variable is categorical. This outcome can be either ordered or unordered; however, other models are better at estimating ordered outcomes. 

#### The log-odds of the outcome and independent variable have a linear relationship

Like binary logistic regression, this model assumes that the relationship between the log odds of the outcome variable and any continous independent variables is linear. In other words, our latent variable, $z_i$, must follow linear conventions (all $\beta$s must be linear). 

#### Independent errors

All observations should be independent of each other. There should not be any structural clustering. For example, if your data includes measures from the same individual over time, your errors are not independent of each other (they are clustered by individual).

#### No (nearing) multicolinearity

This is a standard warning in linear regression modelling. You should never include perfectly colinear variables in your model. If some of your variables are nearing perfect colinearity, you should exclude some or provide a good theoretical justification for including them in your model.  

### The model

Formally, we define the probability of success for each choice as: 

$$
Pr(y_i = m | x_i) = \frac{e^{x_i\beta(m)}}{\sum^{J}_{j = 1}e^{x_i\beta(j)}}
$$

Where $m$ is the option you have set to the baseline and $j$ is all other options. 

Let's visualise the relationship between support for the war and gender:

```{r}
p1 <- student_survey |> 
  filter(warsup == "Strongly support" | warsup == "Somewhat support") |> 
  ggplot(aes(x = as.numeric(female), y = as.numeric(warsup))) + 
  geom_point() + 
  geom_smooth(method = "loess", se = F) + 
  theme_minimal() + 
  labs(title = "Somewhat support", 
       x = NULL)

p2 <- student_survey |> 
  filter(warsup == "Strongly support" | warsup == "Somewhat oppose") |> 
  ggplot(aes(x = as.numeric(female), y = as.numeric(warsup))) + 
  geom_point() + 
  geom_smooth(method = "loess", se = F) + 
  theme_minimal() + 
  labs(title = "Somewhat oppose", 
       y = NULL)

p3 <- student_survey |> 
  filter(warsup == "Strongly support" | warsup == "Strongly oppose") |> 
  ggplot(aes(x = as.numeric(female), y = as.numeric(warsup))) + 
  geom_point() + 
  geom_smooth(method = "loess", se = F) + 
  theme_minimal() + 
  labs(title = "Strongly oppose", 
       x = NULL,
       y = NULL)

p1 | p2 | p3
```

And for party ID: 

```{r}
p1 <- student_survey |> 
  filter(warsup == "Strongly support" | warsup == "Somewhat support") |> 
  ggplot(aes(x = as.numeric(pid), y = as.numeric(warsup))) + 
  geom_point() + 
  geom_smooth(method = "loess", se = F) + 
  theme_minimal() + 
  labs(title = "Somewhat support", 
       x = NULL)

p2 <- student_survey |> 
  filter(warsup == "Strongly support" | warsup == "Somewhat oppose") |> 
  ggplot(aes(x = as.numeric(pid), y = as.numeric(warsup))) + 
  geom_point() + 
  geom_smooth(method = "loess", se = F) + 
  theme_minimal() + 
  labs(title = "Somewhat oppose", 
       y = NULL)

p3 <- student_survey |> 
  filter(warsup == "Strongly support" | warsup == "Strongly oppose") |> 
  ggplot(aes(x = as.numeric(pid), y = as.numeric(warsup))) + 
  geom_point() + 
  geom_smooth(method = "loess", se = F) + 
  theme_minimal() + 
  labs(title = "Strongly oppose", 
       x = NULL,
       y = NULL)

p1 | p2 | p3
```

We can see here that the relationship between these independent variables and an individual's level of support for the war is different based on the choice at hand. Multinomial modelling allows us to measure this. We are essentially developing a different model for each outcome option: strongly support vs. strong oppose, strongly support vs. weakly oppose, and strongly support vs. weakly support. 

Let's fit our model using `nnet::multinom()` and display the results using `broom::tidy()`:

```{r}
m1 <- multinom(warsup ~ female + pid, data = student_survey)

tidy(m1)
```

Here, we can see that the model has produced different coefficients for each of our independent variables for each outcome option. The first column tells us which outcome option we are  modelling: weak support, weak opposition, and strong opposition.

We can more formally present these results using `gtsummary::tbl_regression()`:

```{r}
tbl_regression(m1, exponentiate = T)
```

And visualise our results using `sjPlot::plot_model()`: 

```{r}
plot_model(m1, show.values = T, value.offset = .3)
```

We will discuss interpreting these coefficients in the next section. 

## Multinomial probit

The process for defining the probability of success for more than two options remains similar to the multinomial logistic regression except that the multinomial probit model allows the errors across choices to be correlated. This means that our assumption that adding or removing choices does not disturb the balance of probabilities of success between the remaining choices does not need to hold. This makes multinomial probit regression a better choice for exploring choice entry and exit.

### A more flexible model

To illustrate, let's consider which type of transport people take to work in the morning. Suppose people can choose between riding a red bus, a train, or driving a car to work. The probabilities are:

$$
Pr(red bus) = \frac{1}{6}
$$

$$
Pr(train) = \frac{2}{6}
$$

$$
Pr(car) = \frac{3}{6}
$$

Therefore, the odds of taking a red bus to train are $\frac{\frac{1}{6}}{\frac{2}{6}} = \frac{1}{2}$. Now, let's add the option of taking a blue bus. Will people split evenly across the blue bus, red bus, train, or car? Unlikely: their transport decision is probably not influenced by the colour of their options. Rather, people will likely be split between the red and blue bus, with people who take the train or drive their car unaffected. 

Therefore, the probabilities change: 

$$
Pr(red bus) = \frac{1}{12}
$$

$$
Pr(blue bus) = \frac{1}{12}
$$

$$
Pr(train) = \frac{2}{6}
$$

$$
Pr(car) = \frac{3}{6}
$$

Therefore, the odds of taking a red bus to train are now $\frac{\frac{1}{12}}{\frac{2}{6}} \ne \frac{1}{2}$. This violates the IIA assumption. We should not, therefore, use multinomial logit regression to model this relationship. 