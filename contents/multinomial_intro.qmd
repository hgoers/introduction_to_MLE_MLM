---
title: "Introduction to Multinomial Models"
page-layout: full
editor: source
toc-depth: 5
execute:
  message: false
  warning: false
---

## Set up

This section uses the following packages: 

```{r, include = FALSE}
library(patchwork)
```

```{r}
library(tidyverse)
library(janitor)
library(skimr)
library(nnet)
library(broom)
library(gtsummary)
library(sjPlot)
```

## Introduction

In political science, we are often interested in measuring the probability of success for more than two outcomes. For example, we may want to know the probability that an individual will vote for a specific candidate in an election. If our individuals have more than two candidates from which to choose, we can take advantage of multinomial modelling. 

Multinomial models are linear regression models that measure the probability of success across outcomes with more than two, unordered options. As with binary outcomes, we can choose between two common approaches to these models: logit and probit regression. Multinomial logit and probit models are an extension of the binary latent variable models we discussed in the previous section. Rather than modelling the choice made between two options - success or failure, vote or not vote - we can efficiently model the choice between many different options. 

Unlike previously, the choice between a multinomial logit model and a multinomial probit model is not trivial. This difference centres on the Independence of Irrelevant Alternatives (IIA) assumption. We will discuss this shortly. 

To explore this question, we will look at which type of schooling program students choose given a set of predictors. This example is provided in the [UCLA Multinomial Logistic Regression post](https://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/). 

Let's load and clean our data:

```{r}
df <- rio::import("https://stats.idre.ucla.edu/stat/data/hsbdemo.dta") |> 
  transmute(
    prog = factor(prog, labels = c("General", "Academic", "Vocational"), ordered = F),
    ses = factor(ses, labels = c("Low", "Middle", "High"), ordered = T),
    write
  )

head(df)
```

The data set contains variables on 200 students. The outcome variable is `prog`, or program type. The predictor variables are social economic status, `ses`, a three-level categorical variable and writing score, `write`, a continuous variable. 

Let's look at our dataset: 

```{r}
skim(df)
```

## Multinomial logit regression

In a binomial logistic regression, we measure the probability that our outcome, $y$, will take on one of two options: usually $y = 0$ or $y = 1$. For multinomial logit regression, we model the probability that our outcome, $y$, will take on one of more than two unordered options. You need to select a baseline category from which you will compare your probability of success for all other categories.

> A multinomial model is genuinely an extension of binomial logistic regression. We could fit a series of binomial logistic regression models and get the same outcome.[^1]

[^1]: Well... almost the same outcome. Multinomial regression is more efficient. Our errors will be slightly different to those we would obtain if we fit multiple binary regressions. 

Recall that a binary logistic regression is modelled as such:

$$
Pr(Y = 1 | X) = ln(\frac{P}{1-P}) = \beta_0 + \beta_1X_1 + ... = \beta_kX_k
$$

This provides the log odds ratio of success ($Y = 1$) for a binary outcome. 

For multinomial logistic regression, our outcome is no longer the log odds of success against failure. Instead, we are measuring the log odds ratio of the probability of one category over the probability of a base category. This is called the **relative log odds ratio**.

$$
ln(\frac{P_{category 2}}{P_{category 1}})
$$

Let's explore this by looking at which program a student is likely to select given two predictors: their socio-economic status and their writing ability. Our outcome variable, `prog`, can take on one of three options: general, academic, or vocational. There is no inherent order among these choices.[^2]

[^2]: If there is some inherent order, you should use ordered logistic regression. See the previous section for more details. 

We need to pick a baseline option. From there, we can estimate the probability that an individual will choose one of the other programs relative to that baseline option. Let's choose the general program as our baseline. Now, we can define our two relative log odds ratios as: 

$$
Pr(prog = academic | X) = log(\frac{Pr(prog = academic)}{Pr(prog = general)})
$$

$$
Pr(prog = vocational | X) = log(\frac{Pr(prog = vocational)}{Pr(prog = general)})
$$

### Assumptions

#### Independence of Irrelevant Alternatives assumption

Multinomial logit assumes that the relative probability of existing choices is not affected by changes to the choice set. For example, if you remove a choice from your model, that choice's probability of success will be distributed evenly among the remaining choices. Their relative contribution to the probability of success remains the same. This is a very strong assumption. 

For example, I am choosing between four candidates for the Democratic primary. The probability that I will vote for each is defined by my preferences for their policy platforms. They are as below: 

$$
Pr(cand\_1) = \frac{1}{10}
$$

$$
Pr(cand\_2) = \frac{1}{2}
$$

$$
Pr(cand\_3) = \frac{3}{10}
$$

$$
Pr(cand\_4) = \frac{1}{10}
$$

Candidate 3 drops out of the race. They were a rather centerist candidate among Democrats, so their supporters do not flow to one specific candidate among those left. I follow this general trend and the probability that I would vote for that candidate distributes itself eveningly among the remaining candidates. Each gets a third of Candidate 3's probability: $\frac{1}{10}$. The new distribution of the probability that I will vote for the candidates is as follows: 

$$
Pr(cand\_1) = \frac{1}{5}
$$

$$
Pr(cand\_2) = \frac{3}{5}
$$

$$
Pr(cand\_4) = \frac{1}{5}
$$

Because the probability of success depends on the candidate, I should use a multinomial logistic regression to model my preferences across these choices. 

#### Categorical outcome

Like binary logistic regression, this model assumes that the outcome variable is categorical. This outcome can be either ordered or unordered; however, other models are better at estimating ordered outcomes. 

#### The log-odds of the outcome and independent variable have a linear relationship

Like binary logistic regression, this model assumes that the relationship between the log odds of the outcome variable and any continous independent variables is linear. In other words, our latent variable, $z_i$, must follow linear conventions (including, all $\beta$s must be linear). 

#### Independent errors

All observations should be independent of each other. There should not be any structural clustering. For example, if your data includes measures from the same individual over time, your errors are not independent of each other (they are clustered by individual).

#### No (nearing) multicolinearity

This is a standard warning in linear regression modelling. You should never include perfectly colinear variables in your model. If some of your variables are nearing perfect colinearity, you should exclude some or provide a good theoretical justification for including them in your model.  

### The model

Formally, we define the probability of success for each choice as: 

$$
Pr(y_i = m | x_i) = \frac{e^{x_i\beta(m)}}{\sum^{J}_{j = 1}e^{x_i\beta(j)}}
$$

Where $m$ is the option you have set to the baseline and $j$ is all other options. 

Let's visualise the relationship between the program selected and the student's socio-economic status:

```{r}
p1 <- df |> 
  filter(prog == "General" | prog == "Academic") |> 
  ggplot(aes(x = as.numeric(ses), y = as.numeric(prog))) + 
  geom_point() + 
  geom_smooth(method = "loess", se = F) + 
  theme_minimal() + 
  labs(title = "Academic", 
       x = NULL)

p2 <- df |> 
  filter(prog == "General" | prog == "Vocational") |> 
  ggplot(aes(x = as.numeric(ses), y = as.numeric(prog))) + 
  geom_point() + 
  geom_smooth(method = "loess", se = F) + 
  theme_minimal() + 
  labs(title = "Vocational", 
       x = NULL)

p1 | p2
```

And for writing scores: 

```{r}
p1 <- df |> 
  filter(prog == "General" | prog == "Academic") |> 
  ggplot(aes(x = write, y = as.numeric(prog))) + 
  geom_point() + 
  geom_smooth(method = "loess", se = F) + 
  theme_minimal() + 
  labs(title = "Academic", 
       x = NULL)

p2 <- df |> 
  filter(prog == "General" | prog == "Vocational") |> 
  ggplot(aes(x = write, y = as.numeric(prog))) + 
  geom_point() + 
  geom_smooth(method = "loess", se = F) + 
  theme_minimal() + 
  labs(title = "Vocational", 
       x = NULL)

p1 | p2
```

We can see here that the relationship between these independent variables and the student's choice of program is different based on the choice at hand. Multinomial modelling allows us to measure this. We are essentially developing a different model for each outcome option: general vs. academic, and general vs. vocational. 

Let's fit our model using `nnet::multinom()` and display the results using `broom::tidy()`:

```{r}
m1 <- multinom(prog ~ ses + write, data = df)

tidy(m1)
```

Here, we can see that the model has produced different coefficients for each of our independent variables for each outcome option. The first column tells us which outcome option we are  modelling: academic or vocational.

We can more formally present these results using `gtsummary::tbl_regression()`:

```{r}
tbl_regression(m1, exponentiate = T)
```

And visualise our results using `sjPlot::plot_model()`: 

```{r}
plot_model(m1, show.values = T, value.offset = .3)
```

We will discuss interpreting these coefficients in the next section. 

> TODO: Likelihood ratio test to test for whether these categories are statistically different from each other. 

## Multinomial probit regression

The process for defining the probability of success for more than two options remains similar to the multinomial logistic regression except that the multinomial probit model allows the errors across choices to be correlated. This means that our assumption that adding or removing choices does not disturb the balance of probabilities of success between the remaining choices does not need to hold. This makes multinomial probit regression a better choice for exploring choice entry and exit.

### A more flexible model

To illustrate, let's consider which type of transport people take to work in the morning. Suppose people can choose between riding a red bus, a train, or driving a car to work. The probabilities are:

$$
Pr(red bus) = \frac{1}{6}
$$

$$
Pr(train) = \frac{2}{6}
$$

$$
Pr(car) = \frac{3}{6}
$$

Therefore, the odds ratio of taking a red bus to train are $\frac{\frac{1}{6}}{\frac{2}{6}} = \frac{1}{2}$. Now, let's add the option of taking a blue bus. Will people split evenly across the blue bus, red bus, train, or car? Unlikely: their transport decision is probably not influenced by the colour of their options. Rather, people will likely be split between the red and blue bus, with people who take the train or drive their car unaffected. 

Therefore, the probabilities change: 

$$
Pr(red bus) = \frac{1}{12}
$$

$$
Pr(blue bus) = \frac{1}{12}
$$

$$
Pr(train) = \frac{2}{6}
$$

$$
Pr(car) = \frac{3}{6}
$$

Therefore, the odds of taking a red bus to train are now $\frac{\frac{1}{12}}{\frac{2}{6}} \ne \frac{1}{2}$. This violates the IIA assumption. We should not, therefore, use multinomial logit regression to model this relationship. 