---
title: "Measuring Substantive Effects on Binary Outcomes"
page-layout: full
editor: source
toc-depth: 5
execute:
  message: false
  warning: false
---

## Set up

This section uses the following packages: 

```{r, include = FALSE}
library(scales)
```

```{r}
library(tidyverse)
library(broom)
library(gtsummary)
library(skimr)
library(mvtnorm)
library(fastDummies)
```

We will use the dataset we set up in the previous section: 

```{r, echo = FALSE}
voters <- rio::import("/Users/harrietgoers/Documents/GVPT729A/class_sets/data/cps00for729a.dta") |> 
  transmute(vote = factor(vote, levels = c(0, 1), labels = c("Did not vote", "Voted")), 
            close, 
            edu7cat = factor(edu7cat), 
            homeown = factor(homeown, levels = c(0, 1), labels = c("Not homeowner", "Homeowner"))) |> 
  labelled::set_variable_labels(vote = "Voted", close = "Registration closing", edu7cat = "Education level", homeown = "Homeownership") |> 
  drop_na()

head(voters)
```

## Introduction 

Previously, we explored the effect of a one-unit change in our predictor on our outcome. A more interesting question focuses not on a one-unit change, but rather a theoretically- or policy-relevant change in our predictor. For example, let's say that a state's legislature is considering how they can increase participation in federal and state elections. They are considering changing their election registration rules to allow individuals to register on election day. Currently, individuals must register at least 20 days prior to election day to be able to vote in that election. What is the effect on individuals' predicted likelihood of voting of changing the date of voting registration from 20 days prior to election day to election day? This section will outline how we calculate this **substantive effect** for binary outcomes. 

As discussed in the previous section, the effect of a change in $x$ on the predicted probability of success depends on your values of $x$. Unlike linear models, the effect of a one unit change in $x$ on the predicted probability of success is not constant. 

## Measuring substantive effects

How do we actually measure the effect of a change from $x_{i1}$ to $x_{i2}$? We predict the estimated probability of success at $x_{i1}$ and at $x_{i2}$ and subtract those probabilities from one another to get the difference. Simple, right? 

However, there are two factors that complicate this process. First, we need to deal with the other variables in our model: what values should they be held at while we change our variable of interest, $x_i$? Second, how do we measure uncertainty surrounding this estimated effect? We will deal with these challenges in turn. 

### What to do with the other independent variables

There are two dominant approaches to solving this challenge: the average case approach and the observed value approach. 

#### Average case approach

This approach sets all other values to their mean (for continuous variables) or mode (for discrete variables).

1.    Find the mean or mode for all independent variables other than your variable of interest. 

2.    Find your predicted probability of success with your first value of $x_i$, holding all other variables at their mean or mode.

3.    Find your predicted probability of success with your second value of $x_i$, holding all other variables at their mean or mode.

4.    Calculate the difference between these predicted probabilities. 

5.    Discuss the substantive significance of this difference. 

To illustrate, let's explore the predicted effect of changing a state's registration voting date from 20 days prior to election day ($close = 20$) to election day ($close = 0$).

First, let's fit a logistic regression as we did in the previous section. Here we will include our controls: `edu7cat` and `homeown`. 

```{r}
m1 <- glm(vote ~ close + edu7cat + homeown, data = voters, family = binomial(link = "logit"))

tbl_regression(m1, intercept = T, exponentiate = T)
```

Now, we create a new dataset that contains the values for `close` we want to test and sets the other independent variables to their mean or mode values: 

```{r}
new_data <- tibble(
  close = c(0, 20),
  edu7cat = voters |> count(edu7cat) |> filter(n == max(n)) |> pull(edu7cat),
  homeown = voters |> count(homeown) |> filter(n == max(n)) |> pull(homeown)
)

new_data
```

Next, we calculate the predicted probability that an individual will vote, given these values for our independent variables:

```{r}
result_av <- augment(m1, newdata = new_data, type.predict = "response")
result_av
```

Next, we calculate the difference between these predicted probabilities, noting that the only thing that changed in our model is the value of `close`: 

```{r}
result_av <- mutate(result_av, diff = .fitted - lead(.fitted))
result_av
```

We predict that the probability that an individual will vote increases `r result_av |> filter(close == 0) |> mutate(diff = diff * 100) |> pull(diff) |> round(2)` percentage points when a state's closing date for voter registration moves from 20 days prior to election day to election day. If our legislature thinks that is a worthwhile increase in turnout, this is a substantively significant result.[^1]

[^1]: This is only one interpretation of substantive significance. This should be theoretically (or policy) driven.

However, we have a bit of a problem. Is this really a generalisable result? Haven't we just estimated the effect of this change in registration day closure for a homeowner with an education level of 4? This is the problem with the average case approach: we are discarding an enormous amount of data from our sample which is potentially compromising the generalisability of our predictions. In fact, sometimes our average case isn't even in our dataset, which means that we are making out-of-sample predictions.

#### Observed value approach

The observed value approach addresses this issue. It sets all other independent variables to their observed values, only aggregating the estimated effect at the end. 

1.    Find your predicted probability of success with your first value of $x_i$, holding all other variables at their observed values. You will get the same number of predictions as you have observations. 

2.    Find your predicted probability of success with your second value of $x_i$, holding all other variables at their observed values.

3.    Calculate the average predicted probability for each of these values of $x_i$. 

4.    Calculate the difference between these averages. 

5.    Discuss the substantive significance of this difference. 

Let's explore the same question as above to illustrate. 

First, find the predicted probability of an individual voting when $close = 20$: 

```{r}
result_20 <- augment(m1, newdata = mutate(voters, close = 20), type.predict = "response")
result_20
```

Next, find the predicted probability of an individual voting when $close = 0$: 

```{r}
result_0 <- augment(m1, newdata = mutate(voters, close = 0), type.predict = "response")
result_0
```

Next, calculate the average predicted probability for $close = 20$ and $close = 0$:

```{r}
result_ov <- result_0 |> 
  bind_rows(result_20) |> 
  group_by(close) |> 
  summarise(.fitted = mean(.fitted)) |> 
  mutate(diff = .fitted - lead(.fitted))

result_ov
```

We expect that the probability that an individual will vote increases `r result_ov |> filter(close == 0) |> mutate(diff = diff * 100) |> pull(diff) |> round(2)` percentage points when a state's closing date for voter registration moves from 20 days prior to election day to election day. If our legislature thinks that is a worthwhile increase in turnout, this is a substantively significant result.[^1]

#### Which approach should you use?

You should use the observed values approach. Hanmer and Kalkan (2013) demonstrate using simulated data that the observed values approach consistently produces estimates closer to the population's true probability than the average case approach. This makes sense: you are using more data to produce your estimated effects.  

### Measuring uncertainty when calculating substantive effects

We calculated these estimates using a model that includes error. We need to understand how this uncertainty impacts our estimated substantive effects. How do we get confidence intervals around our estimated effects? 

Imagine we are trying to estimate the relationship between some binary outcome $Y$ and some independent variables $X$ and $Z$. We take a representative sample from our population and fit a model against that data. If we were to take a different representative sample from our population and fit the same model to that data, we will probably get slightly different estimates for our $\beta$s. This is because of the random error inherent in modelling observed data. We can make some useful assumptions about the distribution of these difference errors produced by these different samples. 

These assumptions depend on whether you are using logistic or probit regression, but the theory is the same. If we were to take many, many, many (say, 1,000) different representative samples from our population and fit many, many, many different models, we would get a set of $\beta$ estimates that follow either a normal (for probit) or inverse logistic (for logit) distribution. Your original estimated $\beta$ will be within this distribution.  

We can use this to generate our confidence intervals. We just need to simulate fitting these 1,000 different models.  

> It is critical that your sample is representative of your population. We are not simulating drawing 1,000 different samples. Rather, we are taking our one model fitted against our one sample and drawing estimates around those $\beta$s. 

1.    Simulate fitting 1,000 different models by drawing 1,000 different $\beta_i$ around your estimated $\beta_i$ following your (logistic or probit) model's distribution.

2.    Predict the probability of success for $x_{i1}$ and $x_{i2}$ using these 1,000 different model estimates.

3.    Calculate the difference between those predictions. 

4.    Calculate the lower and upper confidence intervals and the mean of those differences. 

Let's illustrate this by looking at our question above. 

Recall our logistic regression model from above:

```{r}
tbl_regression(m1, intercept = T, exponentiate = T)
```

Let's collect those coefficients using `broom::tidy()`: 

```{r}
coefs <- tidy(m1) |> pull(estimate)
coefs
```

Next, simulate fitting 1,000 different models using these estimates as our center-point using `mvtnorm::rmvnorm()`:

> TODO: Learn more about the sigma. 

```{r}
coefs_sim <- rmvnorm(n = 1000, mean = coefs, sigma = vcov(m1)) |> 
  as_tibble() |> 
  set_names(tidy(m1) |> mutate(term = paste0(term, "_beta")) |> pull(term))

head(coefs_sim)
```

Let's look at the distribution of these simulated $\beta$s for one of our variables: `close`:

```{r}
ggplot(coefs_sim, aes(x = close_beta)) + 
  geom_histogram() + 
  geom_vline(xintercept = tidy(m1) |> filter(term == "close") |> pull(estimate)) + 
  theme_minimal()
```

This draw follows the inverse logit distribution and centered around our estimated $\beta_{close}$ (highlighted by the black line).

Next, we need to predict the probability that an individual will vote when $close = 20$ and when $close = 0$ using these 1,000 different model estimates. To do this, we need to convert our categorical variables into dummy variables so that we can fit the correct $\beta$s to them. 

We can use `fastDummies::dummy_col()` to do this: 

```{r}
trans_data <- voters |> 
  dummy_cols(select_columns = "edu7cat", remove_first_dummy = TRUE) |> 
  mutate(homeown = as.integer(homeown))

head(trans_data)
```

> $edu7cat = 1$ is our reference category. We can remove it because it is not directly included in our model.  

Now, we include our `close` variable, set to 0 and 20 for each of these `r nrow(voters) |> comma()` observations. We should get a dataset of length `r comma(nrow(voters) * 2)`: one set of observations for $close = 20$ and one set for $close = 0$.

```{r}
new_data <- trans_data |> 
  mutate(close = 0) |> 
  bind_rows(mutate(trans_data, close = 20)) |> 
  group_by(close) |> 
  mutate(id = row_number()) |> 
  ungroup()

nrow(new_data)
```

Next, we need to join our datasets together, so we can calculate our predicted probability for each observation for each simulated model coefficient. We should get a dataframe with a length of 2 x 1,000 x `r nrow(voters) |> comma()` (number of different variables of interest x number of models x number of observations).

```{r}
sim_data <- coefs_sim |> 
  mutate(sim_round = row_number()) |> 
  full_join(new_data, by = character())

nrow(sim_data)
```

We can now estimate our logistic regression model using the 1,000 different estimated $\beta$s for all `r nrow(voters) |> comma()` different observations. You need to first fit the linear model, then find the inverse logit of those results using `plogis()`.

```{r}
results <- sim_data |> 
  mutate(
    .fitted = `(Intercept)_beta` +
      close_beta * close +
      edu7cat2_beta * edu7cat_2 +
      edu7cat3_beta * edu7cat_3 +
      edu7cat4_beta * edu7cat_4 +
      edu7cat5_beta * edu7cat_5 +
      edu7cat6_beta * edu7cat_6 +
      edu7cat7_beta * edu7cat_7 +
      homeownHomeowner_beta * homeown,
    .fitted = plogis(.fitted)
  ) |> 
  arrange(sim_round, id, close)
```

We then calculate the difference between the predicted probabilities for each observation when $close = 0$ and $close = 20$:

```{r}
results |>
  group_by(sim_round, id) |> 
  mutate(diff = (.fitted - lead(.fitted)) * 100) |> 
  drop_na(diff) |> 
  ungroup() |> 
  summarise(`Lower bound` = quantile(diff, 0.025),
            `Mean` = quantile(diff, 0.5),
            `Upper bound` = quantile(diff, 0.975))
```

This simulated result is very close to the difference we calculated using both the average case (`r result_av |> filter(close == 0) |> mutate(diff = diff * 100) |> pull(diff) |> round(2)` percentage points) and observed value (`r result_ov |> filter(close == 0) |> mutate(diff = diff * 100) |> pull(diff) |> round(2)`) approaches. But now we have a confidence interval around this estimate. Because this confidence interval crosses through 0, we cannot reject the null hypothesis that this substantive effect is caused by random error. 