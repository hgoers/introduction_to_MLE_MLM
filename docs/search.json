[
  {
    "objectID": "contents/binary_substantive_effects.html",
    "href": "contents/binary_substantive_effects.html",
    "title": "Measuring Substantive Effects on Binary Outcomes",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(gtsummary)\nlibrary(skimr)\nlibrary(mvtnorm)\n\nWe will use the dataset we set up in the previous section:\n\n\n   vote close edu7cat       homeown\n1 Voted    10       6     Homeowner\n2 Voted    29       6     Homeowner\n3 Voted    28       4     Homeowner\n4 Voted     0       5     Homeowner\n5 Voted    25       7 Not homeowner\n6 Voted    25       5 Not homeowner"
  },
  {
    "objectID": "contents/binary_substantive_effects.html#introduction",
    "href": "contents/binary_substantive_effects.html#introduction",
    "title": "Measuring Substantive Effects on Binary Outcomes",
    "section": "Introduction",
    "text": "Introduction\nPreviously, we explored the effect of a given value of \\(x_i\\) on our expected probability. We are rarely interested in this effect in political science. Rather, we are interested in the effect of a meaningful change in \\(x_i\\) on our expected probability. For example, what is the effect of changing the date of voting registration from 20 days prior to election day to election day? This section will outline how we calculate this substantive effect for binary outcomes.\nFor non-linear models, we need to specify our values carefully. Unlike linear models, the effect of a one unit change in \\(x\\) on \\(y\\) is not constant. To illustrate, let’s look at the effect on the probability of success of moving from \\(x = 2\\) to \\(x = 3\\) compared to the effect of moving from \\(x = 4\\) to \\(x = 5\\):\n\n\n\n\n\nMoving from \\(x = 2\\) (highlighted in light blue) to \\(x = 3\\) (highlighted in dark blue) increases the probability of success by 0.066, from 0.034 to 0.099. Moving the same interval of one unit from \\(x = 4\\) (highlighted in pink) to \\(x = 5\\) (highlighted in red) increases the probability of success by 0.267, from 0.259 to 0.526. That’s a 4.058 times increase in the effect of a one unit change in \\(x\\).\nTherefore, to measure the effect of moving from one value of \\(x\\) to another in a non-linear model, we need to know which values of \\(x\\) we are moving between. This should be theoretically driven: what is an interesting interval for the phenomenon you are measuring?"
  },
  {
    "objectID": "contents/binary_substantive_effects.html#measuring-substantive-effects",
    "href": "contents/binary_substantive_effects.html#measuring-substantive-effects",
    "title": "Measuring Substantive Effects on Binary Outcomes",
    "section": "Measuring substantive effects",
    "text": "Measuring substantive effects\nHow do we actually measure the effect of a change from \\(x_{i1}\\) to \\(x_{i2}\\)? We predict the estimated probability of success at \\(x_{i1}\\) and at \\(x_{i2}\\) and subtract those probabilities from one another to get the difference. Simple, right? However, there are two factors that complicate this process. First, we need to deal with the other variables in our model: what values should they be held at while we change our variable of interest, \\(x_i\\)? Second, how do we measure uncertainty surrounding this estimated effect? We will deal with these challenges in turn.\n\nWhat to do with the other independent variables\nThere are two dominant approaches to solving this challenge: the average case approach and the observed value approach.\n\nAverage case approach\nThis approach sets all other values to their mean (for continuous variables) or mode (for discrete variables).\n\nFind the mean or mode for all independent variables other than your variable of interest.\nFind your predicted probability of success with your first value of \\(x_i\\), holding all other variables at their mean or mode.\nFind your predicted probability of success with your second value of \\(x_i\\), holding all other variables at their mean or mode.\nCalculate the difference between these predicted probabilities.\nDiscuss the substantive significance of this difference.\n\nTo illustrate, let’s explore the predicted effect of changing a state’s registration voting date from 20 days prior to election day (\\(close = 20\\)) to election day (\\(close = 0\\)).\nFirst, let’s fit a logistic regression as we did in the previous section:\n\nm1 <- glm(vote ~ close + edu7cat + homeown, data = voters, family = binomial(link = \"logit\"))\n\ntbl_regression(m1, intercept = T, exponentiate = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.45\n0.11, 1.56\n0.2\n    Registration closing\n0.99\n0.98, 1.00\n0.2\n    Education level\n\n\n\n    1\n—\n—\n\n    2\n1.59\n0.43, 6.60\n0.5\n    3\n1.24\n0.35, 4.96\n0.7\n    4\n1.66\n0.48, 6.54\n0.4\n    5\n3.46\n1.00, 13.6\n0.055\n    6\n6.86\n1.95, 27.4\n0.003\n    7\n9.13\n2.49, 38.0\n0.001\n    Homeownership\n\n\n\n    Not homeowner\n—\n—\n\n    Homeowner\n2.28\n1.85, 2.83\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nNow, we create a new dataset that contains the values for close we want to test and sets the other independent variables to their mean or mode values:\n\nnew_data <- tibble(\n  close = c(0, 20),\n  edu7cat = voters |> count(edu7cat) |> filter(n == max(n)) |> pull(edu7cat),\n  homeown = voters |> count(homeown) |> filter(n == max(n)) |> pull(homeown)\n)\n\nnew_data\n\n# A tibble: 2 × 3\n  close edu7cat homeown  \n  <dbl> <fct>   <fct>    \n1     0 4       Homeowner\n2    20 4       Homeowner\n\n\nNext, we calculate the predicted probability that an individual will vote, given these values for our independent variables:\n\nresult <- augment(m1, newdata = new_data, type.predict = \"response\")\nresult\n\n# A tibble: 2 × 4\n  close edu7cat homeown   .fitted\n  <dbl> <fct>   <fct>       <dbl>\n1     0 4       Homeowner   0.629\n2    20 4       Homeowner   0.601\n\n\nNext, we calculate the difference between these predicted probabilities, noting that the only thing that changed in our model is the value of close:\n\nresult <- mutate(result, diff = (.fitted - lead(.fitted)) * 100)\nresult\n\n# A tibble: 2 × 5\n  close edu7cat homeown   .fitted  diff\n  <dbl> <fct>   <fct>       <dbl> <dbl>\n1     0 4       Homeowner   0.629  2.77\n2    20 4       Homeowner   0.601 NA   \n\n\nWe expect that the probability that an individual will vote increases 2.77 percentage points when a state’s closing date for voter registration moves from 20 days prior to election day to election day. If increasing turnout by 2.77 percentage points would swing an election in a candidate’s favour, this is a substantively significant result.1\nHowever, we have a bit of a problem. Is this really a generalisable result? Haven’t we just estimated the effect of this change in registration day closure for a homeowner with an education level of 4? This is the problem with the average case approach: we are discarding an enormous amount of rich data from our sample which is potentially compromising the generalisability of our estimates. In fact, sometimes our average case isn’t even in our dataset, which means that we are making out-of-sample predictions.\n\n\nObserved value approach\nThe observed value approach addresses this issue. It sets all other independent variables to their observed values, only aggregating the estimated effect at the end.\n\nFind your predicted probability of success with your first value of \\(x_i\\), holding all other variables at their observed values. You will get the same number of predictions as you have observations.\nFind your predicted probability of success with your second value of \\(x_i\\), holding all other variables at their observed values.\nCalculate the average predicted probability for each of these values of \\(x_i\\).\nCalculate the difference between these averages.\nDiscuss the substantive significance of this difference.\n\nLet’s explore the same question as above to illustrate.\nFirst, find the predicted probability of an individual voting when \\(close = 20\\):\n\nresult_20 <- augment(m1, newdata = mutate(voters, close = 20), type.predict = \"response\")\nresult_20\n\n# A tibble: 2,188 × 5\n   vote         close edu7cat homeown       .fitted\n   <fct>        <dbl> <fct>   <fct>           <dbl>\n 1 Voted           20 6       Homeowner       0.861\n 2 Voted           20 6       Homeowner       0.861\n 3 Voted           20 4       Homeowner       0.601\n 4 Voted           20 5       Homeowner       0.758\n 5 Voted           20 7       Not homeowner   0.784\n 6 Voted           20 5       Not homeowner   0.579\n 7 Voted           20 7       Homeowner       0.892\n 8 Voted           20 6       Homeowner       0.861\n 9 Voted           20 2       Homeowner       0.590\n10 Did not vote    20 5       Homeowner       0.758\n# … with 2,178 more rows\n\n\nNext, find the predicted probability of an individual voting when \\(close = 0\\):\n\nresult_0 <- augment(m1, newdata = mutate(voters, close = 0), type.predict = \"response\")\nresult_0\n\n# A tibble: 2,188 × 5\n   vote         close edu7cat homeown       .fitted\n   <fct>        <dbl> <fct>   <fct>           <dbl>\n 1 Voted            0 6       Homeowner       0.875\n 2 Voted            0 6       Homeowner       0.875\n 3 Voted            0 4       Homeowner       0.629\n 4 Voted            0 5       Homeowner       0.779\n 5 Voted            0 7       Not homeowner   0.803\n 6 Voted            0 5       Not homeowner   0.607\n 7 Voted            0 7       Homeowner       0.903\n 8 Voted            0 6       Homeowner       0.875\n 9 Voted            0 2       Homeowner       0.618\n10 Did not vote     0 5       Homeowner       0.779\n# … with 2,178 more rows\n\n\nNext, calculate the average predicted probability for \\(close = 20\\) and \\(close = 0\\):\n\nresult <- result_0 |> \n  bind_rows(result_20) |> \n  group_by(close) |> \n  summarise(.fitted = mean(.fitted)) |> \n  mutate(diff = (.fitted - lead(.fitted)) * 100)\n\nresult\n\n# A tibble: 2 × 3\n  close .fitted  diff\n  <dbl>   <dbl> <dbl>\n1     0   0.688  2.28\n2    20   0.666 NA   \n\n\nWe expect that the probability that an individual will vote increases 2.28 percentage points when a state’s closing date for voter registration moves from 20 days prior to election day to election day. If this increase in turnout would swing an election in a candidate’s favour, this is a substantively significant result.\n\n\nWhich approach should you use?\nYou should use the observed values approach. Hanmer and Kalkan (2013) demonstrate using simulated data that the observed values approach consistently produces estimates closer to the population’s true probability than the average case approach. This makes sense: you are using more data to produce your estimated effects.\n\n\n\nMeasuring uncertainty when calculating substantive effects\nWe calculated these estimates using a model that includes error. We need to understand how this uncertainty impacts our estimated substantive effects. How do we get confidence intervals around our estimated effects?\nFirst, we need to go back to the fundamentals of our logistic or probit regression models. Remember from the first section that the latent variable approach assumes there is some continuous process that gets us from \\(y = 0\\) to \\(y = 1\\) based on some set of independent variables \\(X\\). This latent variable, \\(z_i\\), is modelled as:\n\\[\nz_i = \\beta_0 + X_i\\beta_i + \\epsilon_i\n\\]\nIt contains an error term: \\(\\epsilon_i\\). What that error looks like depends on whether you use a logistic or probit regression, but they are very similar. That error term applies to both \\(X_i\\) and \\(\\beta_i\\). So, we need to generate a series of estimates for both the \\(X_i\\) and \\(\\beta_i\\) to generate our confidence intervals.\nImagine we are trying to estimate the relationship between some binary outcome \\(y\\) and some independent variables \\(x\\) and \\(z\\). We take a representative sample from our population and fit a model against that data. If we were to take a different representative sample from our population and fit the same model to that data, we will probably get slightly different estimates for our \\(\\beta\\)s. This is because of the random error inherent in observational modelling. The idea behind logistic and probit regression is that if we were to do this many, many, many times (say, take 1,000 different representative samples from our population and use that data to fit 1,000 models) we would get a set of \\(\\beta\\) estimates that follow a normal (if you’re using a probit model) or inverse logistic (if you’re using a logistic model) distribution. Your original estimated \\(\\beta\\) will be within this distribution.\nWe can use this assumption to generate our confidence intervals. We just need to simulate fitting these 1,000 different models.\n\nIt is critical that your sample is representative of your population. We are not simulating drawing 1,000 different samples. Rather, we are taking our one model fitted against our one sample and drawing estimates around those \\(\\beta\\)s.\n\n\nSimulate fitting 1,000 different models by drawing 1,000 different \\(\\beta_i\\) around your estimated \\(\\beta_i\\) following your (logistic or probit) model’s distribution.\nPredict the probability of success for \\(x_{i1}\\) and \\(x_{i2}\\) using these 1,000 different model estimates.\nCalculate the difference between those predictions.\nCalculate the lower and upper confidence intervals and the mean of those differences.\n\nLet’s illustrate this by looking at our question above.\n\nLogistic regression\nRecall our logistic regression model from above:\n\ntbl_regression(m1, intercept = T, exponentiate = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.45\n0.11, 1.56\n0.2\n    Registration closing\n0.99\n0.98, 1.00\n0.2\n    Education level\n\n\n\n    1\n—\n—\n\n    2\n1.59\n0.43, 6.60\n0.5\n    3\n1.24\n0.35, 4.96\n0.7\n    4\n1.66\n0.48, 6.54\n0.4\n    5\n3.46\n1.00, 13.6\n0.055\n    6\n6.86\n1.95, 27.4\n0.003\n    7\n9.13\n2.49, 38.0\n0.001\n    Homeownership\n\n\n\n    Not homeowner\n—\n—\n\n    Homeowner\n2.28\n1.85, 2.83\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nLet’s collect those coefficients using broom::tidy():\n\ncoefs <- tidy(m1) |> pull(estimate)\ncoefs\n\n[1] -0.80698206 -0.00585002  0.46146385  0.21142686  0.50831506  1.24130230\n[7]  1.92500744  2.21140696  0.82613712\n\n\nNext, simulate fitting 1,000 different models using these estimates as our center-point using mvtnorm::rmvnorm():\n\nTODO: Learn more about the sigma.\n\n\ncoefs_sim <- rmvnorm(n = 1000, mean = coefs, sigma = vcov(m1)) |> \n  as_tibble() |> \n  set_names(tidy(m1) |> mutate(term = paste0(term, \"_beta\")) |> pull(term))\n\nhead(coefs_sim)\n\n# A tibble: 6 × 9\n  (Intercept)…¹ close_…² edu7c…³ edu7c…⁴ edu7c…⁵ edu7c…⁶ edu7c…⁷ edu7c…⁸ homeo…⁹\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1        -1.93   0.00913   1.85   0.967    1.35    2.11    2.75    2.63    0.767\n2        -2.71  -0.0127    2.52   2.41     2.52    3.11    3.76    4.42    0.870\n3        -0.686 -0.00903   0.521  0.0579   0.362   1.22    2.21    2.15    0.840\n4        -1.62  -0.00940   1.46   1.07     1.20    1.90    2.85    2.56    1.01 \n5         0.569 -0.00465  -1.39  -1.41    -1.06   -0.260   0.434   0.815   0.937\n6        -1.63   0.00130   0.967  1.03     1.26    1.83    2.81    2.83    0.837\n# … with abbreviated variable names ¹​`(Intercept)_beta`, ²​close_beta,\n#   ³​edu7cat2_beta, ⁴​edu7cat3_beta, ⁵​edu7cat4_beta, ⁶​edu7cat5_beta,\n#   ⁷​edu7cat6_beta, ⁸​edu7cat7_beta, ⁹​homeownHomeowner_beta\n\n\nLet’s look at the distribution of these simulated \\(\\beta\\)s for one of our variables: close:\n\nggplot(coefs_sim, aes(x = close_beta)) + \n  geom_histogram() + \n  geom_vline(xintercept = tidy(m1) |> filter(term == \"close\") |> pull(estimate)) + \n  theme_minimal()\n\n\n\n\nAs expected, this draw follows the inverse logit distribution and centered around our estimated \\(\\beta_{close}\\) (highlighted by the black line).\nNext, we need to predict the probability that an individual will vote when \\(close = 20\\) and when \\(close = 0\\) using these 1,000 different model estimates. To do this, we need to convert our categorical variables into dummy variables so that we can fit the correct \\(\\beta\\)s to them:\n\ntrans_data <- voters |> \n  transmute(\n    edu7cat2 = if_else(edu7cat == 2, 1, 0),\n    edu7cat3 = if_else(edu7cat == 3, 1, 0),\n    edu7cat4 = if_else(edu7cat == 4, 1, 0),\n    edu7cat5 = if_else(edu7cat == 5, 1, 0),\n    edu7cat6 = if_else(edu7cat == 6, 1, 0),\n    edu7cat7 = if_else(edu7cat == 7, 1, 0),\n    homeown = as.numeric(homeown) - 1\n  )\n\nhead(trans_data)\n\n  edu7cat2 edu7cat3 edu7cat4 edu7cat5 edu7cat6 edu7cat7 homeown\n1        0        0        0        0        1        0       1\n2        0        0        0        0        1        0       1\n3        0        0        1        0        0        0       1\n4        0        0        0        1        0        0       1\n5        0        0        0        0        0        1       0\n6        0        0        0        1        0        0       0\n\n\nNow, we include our close variable, set to 0 and 20 for each of these 2,188 observations. We should get a dataset of length 4,376: one set of observations for \\(close = 20\\) and one set for \\(close = 0\\).\n\nnew_data <- trans_data |> \n  mutate(close = 0) |> \n  bind_rows(mutate(trans_data, close = 20)) |> \n  group_by(close) |> \n  mutate(id = row_number()) |> \n  ungroup()\n\nnrow(new_data)\n\n[1] 4376\n\n\nNext, we need to join our datasets together, so we can calculate our predicted probability for each observation for each simulated model coefficient. We should get a dataframe with a length of 2 x 1,000 x 2,188 (number of different variables of interest x number of models x number of observations).\n\nsim_data <- coefs_sim |> \n  mutate(sim_round = row_number()) |> \n  full_join(new_data, by = character())\n\nnrow(sim_data)\n\n[1] 4376000\n\n\nWe can now estimate our logistic regression model using the 1,000 different estimated \\(\\beta\\)s for all 2,188 different observations. You need to first fit the linear model, then find the inverse logit of those results using plogis().\n\nresults <- sim_data |> \n  mutate(\n    .fitted = `(Intercept)_beta` +\n      close_beta * close +\n      edu7cat2_beta * edu7cat2 +\n      edu7cat3_beta * edu7cat3 +\n      edu7cat4_beta * edu7cat4 +\n      edu7cat5_beta * edu7cat5 +\n      edu7cat6_beta * edu7cat6 +\n      edu7cat7_beta * edu7cat7 +\n      homeownHomeowner_beta * homeown,\n    .fitted = plogis(.fitted)\n  ) |> \n  arrange(sim_round, id, close)\n\nhead(results)\n\n# A tibble: 6 × 20\n  (Intercept)_…¹ close…² edu7c…³ edu7c…⁴ edu7c…⁵ edu7c…⁶ edu7c…⁷ edu7c…⁸ homeo…⁹\n           <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1          -1.93 0.00913    1.85   0.967    1.35    2.11    2.75    2.63   0.767\n2          -1.93 0.00913    1.85   0.967    1.35    2.11    2.75    2.63   0.767\n3          -1.93 0.00913    1.85   0.967    1.35    2.11    2.75    2.63   0.767\n4          -1.93 0.00913    1.85   0.967    1.35    2.11    2.75    2.63   0.767\n5          -1.93 0.00913    1.85   0.967    1.35    2.11    2.75    2.63   0.767\n6          -1.93 0.00913    1.85   0.967    1.35    2.11    2.75    2.63   0.767\n# … with 11 more variables: sim_round <int>, edu7cat2 <dbl>, edu7cat3 <dbl>,\n#   edu7cat4 <dbl>, edu7cat5 <dbl>, edu7cat6 <dbl>, edu7cat7 <dbl>,\n#   homeown <dbl>, close <dbl>, id <int>, .fitted <dbl>, and abbreviated\n#   variable names ¹​`(Intercept)_beta`, ²​close_beta, ³​edu7cat2_beta,\n#   ⁴​edu7cat3_beta, ⁵​edu7cat4_beta, ⁶​edu7cat5_beta, ⁷​edu7cat6_beta,\n#   ⁸​edu7cat7_beta, ⁹​homeownHomeowner_beta\n\n\nWe then calculate the difference between the predicted probabilities for each observation when \\(close = 0\\) and \\(close = 20\\):\n\nresults |>\n  group_by(sim_round, id) |> \n  mutate(diff = (.fitted - lead(.fitted)) * 100) |> \n  drop_na(diff) |> \n  ungroup() |> \n  summarise(`Lower bound` = quantile(diff, 0.025),\n            `Mean` = quantile(diff, 0.5),\n            `Upper bound` = quantile(diff, 0.975))\n\n# A tibble: 1 × 3\n  `Lower bound`  Mean `Upper bound`\n          <dbl> <dbl>         <dbl>\n1         -1.74  1.96          6.55\n\n\nBecause this confidence interval crosses through 0, we cannot reject the null hypothesis that this substantive effect is caused by random error.\n\n\nProbit regression\nFirst, let’s fit our probit model:\n\nm2 <- glm(vote ~ close + edu7cat + homeown, data = voters, family = binomial(link = \"probit\"))\n\ntbl_regression(m2, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-0.49\n-1.3, 0.28\n0.2\n    Registration closing\n0.00\n-0.01, 0.00\n0.2\n    Education level\n\n\n\n    1\n—\n—\n\n    2\n0.28\n-0.52, 1.1\n0.5\n    3\n0.13\n-0.64, 0.93\n0.7\n    4\n0.31\n-0.44, 1.1\n0.4\n    5\n0.76\n0.00, 1.5\n0.054\n    6\n1.2\n0.39, 1.9\n0.004\n    7\n1.3\n0.52, 2.1\n0.001\n    Homeownership\n\n\n\n    Not homeowner\n—\n—\n\n    Homeowner\n0.50\n0.37, 0.63\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nLet’s collect those coefficients using broom::tidy():\n\ncoefs <- tidy(m2) |> pull(estimate)\ncoefs\n\n[1] -0.489578055 -0.003628296  0.283498003  0.128636563  0.314455793\n[6]  0.761436748  1.153567104  1.308027905  0.501008446\n\n\nNext, simulate fitting 1,000 different models using these estimates as our center-point using mvtnorm::rmvnorm():\n\ncoefs_sim <- rmvnorm(n = 1000, mean = coefs, sigma = vcov(m2)) |> \n  as_tibble() |> \n  set_names(tidy(m2) |> mutate(term = paste0(term, \"_beta\")) |> pull(term))\n\nhead(coefs_sim)\n\n# A tibble: 6 × 9\n  (Intercept…¹ close_…² edu7c…³ edu7ca…⁴ edu7c…⁵ edu7c…⁶ edu7c…⁷ edu7c…⁸ homeo…⁹\n         <dbl>    <dbl>   <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1      -0.960  -1.74e-3 1.07     0.573    0.854    1.31    1.70    2.02    0.429\n2       0.0778 -6.85e-3 0.00333 -0.132   -0.0851   0.369   0.696   0.644   0.451\n3      -1.14   -3.15e-4 0.852    0.631    0.789    1.23    1.53    1.62    0.622\n4      -0.347  -7.43e-3 0.165    0.109    0.277    0.790   1.17    1.27    0.452\n5      -0.838  -5.25e-3 0.702    0.336    0.662    1.02    1.50    1.75    0.561\n6      -0.510  -3.69e-3 0.545   -0.00604  0.406    0.758   1.13    1.07    0.518\n# … with abbreviated variable names ¹​`(Intercept)_beta`, ²​close_beta,\n#   ³​edu7cat2_beta, ⁴​edu7cat3_beta, ⁵​edu7cat4_beta, ⁶​edu7cat5_beta,\n#   ⁷​edu7cat6_beta, ⁸​edu7cat7_beta, ⁹​homeownHomeowner_beta\n\n\nLet’s look at the distribution of these simulated \\(\\beta\\)s for one of our variables: close:\n\nggplot(coefs_sim, aes(x = close_beta)) + \n  geom_histogram() + \n  geom_vline(xintercept = tidy(m2) |> filter(term == \"close\") |> pull(estimate)) + \n  theme_minimal()\n\n\n\n\nAs expected, this draw is normally distributed and centered around our estimated \\(\\beta_{close}\\).\nNext, we need to predict the probability that an individual will vote when \\(close = 20\\) and when \\(close = 0\\) using these 1,000 different model estimates. To do this, we need to convert our categorical variables into dummy variables so that we can fit the correct \\(\\beta\\)s to them:\n\ntrans_data <- voters |> \n  transmute(\n    edu7cat2 = if_else(edu7cat == 2, 1, 0),\n    edu7cat3 = if_else(edu7cat == 3, 1, 0),\n    edu7cat4 = if_else(edu7cat == 4, 1, 0),\n    edu7cat5 = if_else(edu7cat == 5, 1, 0),\n    edu7cat6 = if_else(edu7cat == 6, 1, 0),\n    edu7cat7 = if_else(edu7cat == 7, 1, 0),\n    homeown = as.numeric(homeown) - 1\n  )\n\nhead(trans_data)\n\n  edu7cat2 edu7cat3 edu7cat4 edu7cat5 edu7cat6 edu7cat7 homeown\n1        0        0        0        0        1        0       1\n2        0        0        0        0        1        0       1\n3        0        0        1        0        0        0       1\n4        0        0        0        1        0        0       1\n5        0        0        0        0        0        1       0\n6        0        0        0        1        0        0       0\n\n\nNow, we include our close variable, set to 0 and 20 for each of these 2,188 observations. We should get a dataset of length 4,376: one set of observations for \\(close = 20\\) and one set for \\(close = 0\\).\n\nnew_data <- trans_data |> \n  mutate(close = 0) |> \n  bind_rows(mutate(trans_data, close = 20)) |> \n  group_by(close) |> \n  mutate(id = row_number()) |> \n  ungroup()\n\nnrow(new_data)\n\n[1] 4376\n\n\nNext, we need to join our datasets together, so we can calculate our predicted probability for each observation for each simulated model coefficients. We should get a dataframe with a length of 2 x 1,000 x 2,188 (number of different variables of interest x number of models x number of observations).\n\nsim_data <- coefs_sim |> \n  mutate(sim_round = row_number()) |> \n  full_join(new_data, by = character())\n\nnrow(sim_data)\n\n[1] 4376000\n\n\nWe can now estimate our probit model using the 1,000 different estimated \\(\\beta\\)s for all 2,188 different observations. You need to first fit the linear model, then find the probit of those results using pnorm().\n\nresults <- sim_data |> \n  mutate(\n    .fitted = `(Intercept)_beta` +\n      close_beta * close +\n      edu7cat2_beta * edu7cat2 +\n      edu7cat3_beta * edu7cat3 +\n      edu7cat4_beta * edu7cat4 +\n      edu7cat5_beta * edu7cat5 +\n      edu7cat6_beta * edu7cat6 +\n      edu7cat7_beta * edu7cat7 +\n      homeownHomeowner_beta * homeown,\n    .fitted = pnorm(.fitted)\n  ) |> \n  arrange(sim_round, id, close)\n\nhead(results)\n\n# A tibble: 6 × 20\n  (Intercept)…¹ close_…² edu7c…³ edu7c…⁴ edu7c…⁵ edu7c…⁶ edu7c…⁷ edu7c…⁸ homeo…⁹\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1        -0.960 -0.00174    1.07   0.573   0.854    1.31    1.70    2.02   0.429\n2        -0.960 -0.00174    1.07   0.573   0.854    1.31    1.70    2.02   0.429\n3        -0.960 -0.00174    1.07   0.573   0.854    1.31    1.70    2.02   0.429\n4        -0.960 -0.00174    1.07   0.573   0.854    1.31    1.70    2.02   0.429\n5        -0.960 -0.00174    1.07   0.573   0.854    1.31    1.70    2.02   0.429\n6        -0.960 -0.00174    1.07   0.573   0.854    1.31    1.70    2.02   0.429\n# … with 11 more variables: sim_round <int>, edu7cat2 <dbl>, edu7cat3 <dbl>,\n#   edu7cat4 <dbl>, edu7cat5 <dbl>, edu7cat6 <dbl>, edu7cat7 <dbl>,\n#   homeown <dbl>, close <dbl>, id <int>, .fitted <dbl>, and abbreviated\n#   variable names ¹​`(Intercept)_beta`, ²​close_beta, ³​edu7cat2_beta,\n#   ⁴​edu7cat3_beta, ⁵​edu7cat4_beta, ⁶​edu7cat5_beta, ⁷​edu7cat6_beta,\n#   ⁸​edu7cat7_beta, ⁹​homeownHomeowner_beta\n\n\nWe then calculate the difference between the predicted probabilities for each observation when \\(close = 0\\) and \\(close = 20\\):\n\nresults |>\n  group_by(sim_round, id) |> \n  mutate(diff = (.fitted - lead(.fitted)) * 100) |> \n  drop_na(diff) |> \n  ungroup() |> \n  summarise(`Lower bound` = quantile(diff, 0.025),\n            `Mean` = quantile(diff, 0.5),\n            `Upper bound` = quantile(diff, 0.975))\n\n# A tibble: 1 × 3\n  `Lower bound`  Mean `Upper bound`\n          <dbl> <dbl>         <dbl>\n1         -1.49  2.24          6.48\n\n\nBecause this confidence interval crosses through 0, we cannot reject the null hypothesis that this substantive effect is caused by random error."
  }
]