[
  {
    "objectID": "contents/ordered_intro.html",
    "href": "contents/ordered_intro.html",
    "title": "Introduction to Ordered Models",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(skimr)\n\nTo understand ordered models, we will explore data that describe individuals’ decision to apply to graduate school.\nFirst, we read in our data using rio::import() and clean it up:\n\ndf <- import(\"https://stats.idre.ucla.edu/stat/data/ologit.dta\") |> \n  mutate(apply = factor(apply, \n                        labels = c(\"Unlikely\", \"Somewhat likely\", \"Very likely\"),\n                        ordered = T),\n         pared = factor(pared),\n         public = factor(public))\n\nhead(df)\n\n            apply pared public  gpa\n1     Very likely     0      0 3.26\n2 Somewhat likely     1      0 3.21\n3        Unlikely     1      1 3.94\n4 Somewhat likely     0      0 2.81\n5 Somewhat likely     0      0 2.53\n6        Unlikely     0      1 2.59\n\n\nLet’s take a quick look at the data using skimr::skim():\n\nskim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n400\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\napply\n0\n1\nTRUE\n3\nUnl: 220, Som: 140, Ver: 40\n\n\npared\n0\n1\nFALSE\n2\n0: 337, 1: 63\n\n\npublic\n0\n1\nFALSE\n2\n0: 343, 1: 57\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ngpa\n0\n1\n3\n0.4\n1.9\n2.72\n2.99\n3.27\n4\n▁▅▇▆▂\n\n\n\n\n\nThis data is used in the UCLA Ordinal Logistic Regression post. It contains a level variable called apply, with levels “unlikely”, “somewhat likely”, and “very likely”. This is our outcome variable. We also have three variables that we will use as predictors: pared, which is a binary variable indicating whether at least one parent has a graduate degree; public, which is a binary variable where 1 indicates that the undergraduate institution is public and 0 private, and gpa, which is the student’s grade point average."
  },
  {
    "objectID": "contents/ordered_intro.html#introduction",
    "href": "contents/ordered_intro.html#introduction",
    "title": "Introduction to Ordered Models",
    "section": "Introduction",
    "text": "Introduction\nWe often want to know the relationship between some independent variables and an ordered categorical outcome. These are outcomes for which the order is clear but the distance between different categories is not uniform or known. For example, a survey asks respondents whether they strongly support, somewhat support, somewhat oppose, or strongly oppose some event. Here, we know that strongly support \\(>\\) somewhat support, but we can’t really measure the distance between strongly support and somewhat support in a meaningful way."
  },
  {
    "objectID": "contents/ordered_intro.html#ordered-models",
    "href": "contents/ordered_intro.html#ordered-models",
    "title": "Introduction to Ordered Models",
    "section": "Ordered models",
    "text": "Ordered models\nWe can use ordered models to estimate the probability that an individual observation will fall into one of the three or more categories of our outcome variable. How do we do this?\nOrdered models follow the latent variable approach introduced in earlier sections. Let’s assume that our ordered categories can be understood as some continuous, latent variable, \\(Y^*\\), that ranges from \\(-\\infty\\) to \\(\\infty\\). So when we talk about someone strongly supporting, weakly supporting, weakly opposing, or strongly opposing something, we aren’t really talking about categories. People’s beliefs sit on some continous spectrum. However, we can’t observe or measure values along this spectrum so we have to resort to categories. If we could observe this continuous variable we would, but we can’t so we use categories.\n\nThink about when we convert continuous variables into binned categorical variables. For example, we might not use individuals’ actual incomes. Instead we use brackets: below $50,000/year, between $50,001 and $80,000/year, above $80,000/year. Let’s imagine we can’t collect individuals’ actual incomes. All we can collect is the bracket in which they fall. The same idea applies to our latent variable of strongly support to strongly oppose but we get a bit more philosophical about it. We are assuming that there is a some continuous variable of people’s level of support (similar to the continuous variable of individuals’ income). We just can’t observe this continuous variable of support. All we can get are these categories.\n\nThere exist some thresholds, or cut points, along this continuous variable that define our observed categories. For example, there is some continuous latent variable that defines the likelihood that an individual will apply to graduate school (\\(Y^*\\)) given some set of predictors (\\(X\\)). An individual who is unlikely to apply has a value of \\(Y^*\\) that sits between \\(-\\infty\\) and some threshold \\(\\tau_{unlikely}\\). An individual who is somewhat likely to apply has a value of \\(Y^*\\) that sits between that point, \\(\\tau_{unlikely}\\), and some greater threshold \\(\\tau_{somewhatlikely}\\). Finally, an individual who is very likely to apply has a value of \\(Y^*\\) that sits between \\(\\tau_{somewhatlikely}\\) and \\(\\infty\\). These thresholds translate our unobservable latent variable \\(Y^*\\) to our observed ordered categorical variable \\(Y\\).\nWe now have the building blocks for our model. We need to estimate some latent variable, \\(Y^*\\), and the thresholds that define our ordered categories.\n\nThe latent variable\nLet’s assume that the latent variable is linear such that:\n\\[\nY^* = X\\beta + \\epsilon\n\\]\nAs with our other latent variable models, we need to make an assumption about the distribution of that error. We will return to our familiar distributions: normal and logistic cumulative distributions that define the probit or logit models with which we are familiar."
  },
  {
    "objectID": "contents/multinomial_intro.html",
    "href": "contents/multinomial_intro.html",
    "title": "Introduction to Multinomial Models",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(nnet)\nlibrary(broom)\nlibrary(gtsummary)\nlibrary(sjPlot)"
  },
  {
    "objectID": "contents/multinomial_intro.html#introduction",
    "href": "contents/multinomial_intro.html#introduction",
    "title": "Introduction to Multinomial Models",
    "section": "Introduction",
    "text": "Introduction\nIn political science, we are often interested in measuring the probability of success for more than two outcomes. For example, we may want to know the probability that an individual will vote for a specific candidate in an election. If our individuals have more than two candidates from which to choose, we can take advantage of multinomial modelling.\nMultinomial models are linear regression models that measure the probability of success across outcomes with more than two, unordered options. As with binary outcomes, we can choose between two common approaches to these models: logit and probit regression. Multinomial logit and probit models are an extension of the binary latent variable models we discussed in the previous section. Rather than modelling the choice made between two options - success or failure, vote or not vote - we can efficiently model the choice between many different options.\nUnlike previously, the choice between a multinomial logit model and a multinomial probit model is not trivial. This difference centres on the Independence of Irrelevant Alternatives (IIA) assumption. We will discuss this shortly.\nTo explore this question, we will look at which type of schooling program students choose given a set of predictors. This example is provided in the UCLA Multinomial Logistic Regression post.\nLet’s load and clean our data:\n\ndf <- rio::import(\"https://stats.idre.ucla.edu/stat/data/hsbdemo.dta\") |> \n  transmute(\n    prog = factor(prog, labels = c(\"General\", \"Academic\", \"Vocational\"), ordered = F),\n    ses = factor(ses, labels = c(\"Low\", \"Middle\", \"High\"), ordered = T),\n    write\n  )\n\nhead(df)\n\n        prog    ses write\n1 Vocational    Low    35\n2    General Middle    33\n3 Vocational   High    39\n4 Vocational    Low    37\n5 Vocational Middle    31\n6    General   High    36\n\n\nThe data set contains variables on 200 students. The outcome variable is prog, or program type. The predictor variables are social economic status, ses, a three-level categorical variable and writing score, write, a continuous variable.\nLet’s look at our dataset:\n\nskim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n200\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nprog\n0\n1\nFALSE\n3\nAca: 105, Voc: 50, Gen: 45\n\n\nses\n0\n1\nTRUE\n3\nMid: 95, Hig: 58, Low: 47\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nwrite\n0\n1\n52.78\n9.48\n31\n45.75\n54\n60\n67\n▂▅▆▇▇"
  },
  {
    "objectID": "contents/multinomial_intro.html#multinomial-logit-regression",
    "href": "contents/multinomial_intro.html#multinomial-logit-regression",
    "title": "Introduction to Multinomial Models",
    "section": "Multinomial logit regression",
    "text": "Multinomial logit regression\nIn a binomial logistic regression, we measure the probability that our outcome, \\(y\\), will take on one of two options: usually \\(y = 0\\) or \\(y = 1\\). For multinomial logit regression, we model the probability that our outcome, \\(y\\), will take on one of more than two unordered options. You need to select a baseline category from which you will compare your probability of success for all other categories.\n\nA multinomial model is genuinely an extension of binomial logistic regression. We could fit a series of binomial logistic regression models and get the same outcome.1\n\nRecall that a binary logistic regression is modelled as such:\n\\[\nPr(Y = 1 | X) = ln(\\frac{P}{1-P}) = \\beta_0 + \\beta_1X_1 + ... = \\beta_kX_k\n\\]\nThis provides the log odds ratio of success (\\(Y = 1\\)) for a binary outcome.\nFor multinomial logistic regression, our outcome is no longer the log odds of success against failure. Instead, we are measuring the log odds ratio of the probability of one category over the probability of a base category. This is called the relative log odds ratio.\n\\[\nln(\\frac{P_{category 2}}{P_{category 1}})\n\\]\nLet’s explore this by looking at which program a student is likely to select given two predictors: their socio-economic status and their writing ability. Our outcome variable, prog, can take on one of three options: general, academic, or vocational. There is no inherent order among these choices.2\nWe need to pick a baseline option. From there, we can estimate the probability that an individual will choose one of the other programs relative to that baseline option. Let’s choose the general program as our baseline. Now, we can define our two relative log odds ratios as:\n\\[\nPr(prog = academic | X) = log(\\frac{Pr(prog = academic)}{Pr(prog = general)})\n\\]\n\\[\nPr(prog = vocational | X) = log(\\frac{Pr(prog = vocational)}{Pr(prog = general)})\n\\]\n\nAssumptions\n\nIndependence of Irrelevant Alternatives assumption\nMultinomial logit assumes that the relative probability of existing choices is not affected by changes to the choice set. For example, if you remove a choice from your model, that choice’s probability of success will be distributed evenly among the remaining choices. Their relative contribution to the probability of success remains the same. This is a very strong assumption.\nFor example, I am choosing between four candidates for the Democratic primary. The probability that I will vote for each is defined by my preferences for their policy platforms. They are as below:\n\\[\nPr(cand\\_1) = \\frac{1}{10}\n\\]\n\\[\nPr(cand\\_2) = \\frac{1}{2}\n\\]\n\\[\nPr(cand\\_3) = \\frac{3}{10}\n\\]\n\\[\nPr(cand\\_4) = \\frac{1}{10}\n\\]\nCandidate 3 drops out of the race. They were a rather centerist candidate among Democrats, so their supporters do not flow to one specific candidate among those left. I follow this general trend and the probability that I would vote for that candidate distributes itself eveningly among the remaining candidates. Each gets a third of Candidate 3’s probability: \\(\\frac{1}{10}\\). The new distribution of the probability that I will vote for the candidates is as follows:\n\\[\nPr(cand\\_1) = \\frac{1}{5}\n\\]\n\\[\nPr(cand\\_2) = \\frac{3}{5}\n\\]\n\\[\nPr(cand\\_4) = \\frac{1}{5}\n\\]\nBecause the probability of success depends on the candidate, I should use a multinomial logistic regression to model my preferences across these choices.\n\n\nCategorical outcome\nLike binary logistic regression, this model assumes that the outcome variable is categorical. This outcome can be either ordered or unordered; however, other models are better at estimating ordered outcomes.\n\n\nThe log-odds of the outcome and independent variable have a linear relationship\nLike binary logistic regression, this model assumes that the relationship between the log odds of the outcome variable and any continous independent variables is linear. In other words, our latent variable, \\(z_i\\), must follow linear conventions (including, all \\(\\beta\\)s must be linear).\n\n\nIndependent errors\nAll observations should be independent of each other. There should not be any structural clustering. For example, if your data includes measures from the same individual over time, your errors are not independent of each other (they are clustered by individual).\n\n\nNo (nearing) multicolinearity\nThis is a standard warning in linear regression modelling. You should never include perfectly colinear variables in your model. If some of your variables are nearing perfect colinearity, you should exclude some or provide a good theoretical justification for including them in your model.\n\n\n\nThe model\nFormally, we define the probability of success for each choice as:\n\\[\nPr(y_i = m | x_i) = \\frac{e^{x_i\\beta(m)}}{\\sum^{J}_{j = 1}e^{x_i\\beta(j)}}\n\\]\nWhere \\(m\\) is the option you have set to the baseline and \\(j\\) is all other options.\nLet’s visualise the relationship between the program selected and the student’s socio-economic status:\n\np1 <- df |> \n  filter(prog == \"General\" | prog == \"Academic\") |> \n  ggplot(aes(x = as.numeric(ses), y = as.numeric(prog))) + \n  geom_point() + \n  geom_smooth(method = \"loess\", se = F) + \n  theme_minimal() + \n  labs(title = \"Academic\", \n       x = NULL)\n\np2 <- df |> \n  filter(prog == \"General\" | prog == \"Vocational\") |> \n  ggplot(aes(x = as.numeric(ses), y = as.numeric(prog))) + \n  geom_point() + \n  geom_smooth(method = \"loess\", se = F) + \n  theme_minimal() + \n  labs(title = \"Vocational\", \n       x = NULL)\n\np1 | p2\n\n\n\n\nAnd for writing scores:\n\np1 <- df |> \n  filter(prog == \"General\" | prog == \"Academic\") |> \n  ggplot(aes(x = write, y = as.numeric(prog))) + \n  geom_point() + \n  geom_smooth(method = \"loess\", se = F) + \n  theme_minimal() + \n  labs(title = \"Academic\", \n       x = NULL)\n\np2 <- df |> \n  filter(prog == \"General\" | prog == \"Vocational\") |> \n  ggplot(aes(x = write, y = as.numeric(prog))) + \n  geom_point() + \n  geom_smooth(method = \"loess\", se = F) + \n  theme_minimal() + \n  labs(title = \"Vocational\", \n       x = NULL)\n\np1 | p2\n\n\n\n\nWe can see here that the relationship between these independent variables and the student’s choice of program is different based on the choice at hand. Multinomial modelling allows us to measure this. We are essentially developing a different model for each outcome option: general vs. academic, and general vs. vocational.\nLet’s fit our model using nnet::multinom() and display the results using broom::tidy():\n\nm1 <- multinom(prog ~ ses + write, data = df)\n\n# weights:  15 (8 variable)\ninitial  value 219.722458 \niter  10 value 179.991227\nfinal  value 179.981726 \nconverged\n\ntidy(m1)\n\n# A tibble: 8 × 6\n  y.level    term        estimate std.error statistic p.value\n  <chr>      <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 Academic   (Intercept)  -2.29      1.16      -1.98  0.0479 \n2 Academic   ses.L         0.822     0.364      2.26  0.0237 \n3 Academic   ses.Q         0.0393    0.306      0.128 0.898  \n4 Academic   write         0.0579    0.0214     2.71  0.00682\n5 Vocational (Intercept)   2.70      1.17       2.31  0.0211 \n6 Vocational ses.L         0.127     0.459      0.278 0.781  \n7 Vocational ses.Q        -0.600     0.356     -1.68  0.0922 \n8 Vocational write        -0.0557    0.0233    -2.39  0.0170 \n\n\nHere, we can see that the model has produced different coefficients for each of our independent variables for each outcome option. The first column tells us which outcome option we are modelling: academic or vocational.\nWe can more formally present these results using gtsummary::tbl_regression():\n\ntbl_regression(m1, exponentiate = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    \n      Academic\n    \n    ses\n\n\n\n    ses.L\n2.28\n1.12, 4.64\n0.024\n    ses.Q\n1.04\n0.57, 1.89\n0.9\n    writing score\n1.06\n1.02, 1.11\n0.007\n    \n      Vocational\n    \n    ses\n\n\n\n    ses.L\n1.14\n0.46, 2.79\n0.8\n    ses.Q\n0.55\n0.27, 1.10\n0.092\n    writing score\n0.95\n0.90, 0.99\n0.017\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nAnd visualise our results using sjPlot::plot_model():\n\nplot_model(m1, show.values = T, value.offset = .3)\n\n\n\n\nWe will discuss interpreting these coefficients in the next section.\n\nTODO: Likelihood ratio test to test for whether these categories are statistically different from each other."
  },
  {
    "objectID": "contents/multinomial_intro.html#multinomial-probit-regression",
    "href": "contents/multinomial_intro.html#multinomial-probit-regression",
    "title": "Introduction to Multinomial Models",
    "section": "Multinomial probit regression",
    "text": "Multinomial probit regression\nThe process for defining the probability of success for more than two options remains similar to the multinomial logistic regression except that the multinomial probit model allows the errors across choices to be correlated. This means that our assumption that adding or removing choices does not disturb the balance of probabilities of success between the remaining choices does not need to hold. This makes multinomial probit regression a better choice for exploring choice entry and exit.\n\nA more flexible model\nTo illustrate, let’s consider which type of transport people take to work in the morning. Suppose people can choose between riding a red bus, a train, or driving a car to work. The probabilities are:\n\\[\nPr(red bus) = \\frac{1}{6}\n\\]\n\\[\nPr(train) = \\frac{2}{6}\n\\]\n\\[\nPr(car) = \\frac{3}{6}\n\\]\nTherefore, the odds ratio of taking a red bus to train are \\(\\frac{\\frac{1}{6}}{\\frac{2}{6}} = \\frac{1}{2}\\). Now, let’s add the option of taking a blue bus. Will people split evenly across the blue bus, red bus, train, or car? Unlikely: their transport decision is probably not influenced by the colour of their options. Rather, people will likely be split between the red and blue bus, with people who take the train or drive their car unaffected.\nTherefore, the probabilities change:\n\\[\nPr(red bus) = \\frac{1}{12}\n\\]\n\\[\nPr(blue bus) = \\frac{1}{12}\n\\]\n\\[\nPr(train) = \\frac{2}{6}\n\\]\n\\[\nPr(car) = \\frac{3}{6}\n\\]\nTherefore, the odds of taking a red bus to train are now \\(\\frac{\\frac{1}{12}}{\\frac{2}{6}} \\ne \\frac{1}{2}\\). This violates the IIA assumption. We should not, therefore, use multinomial logit regression to model this relationship."
  },
  {
    "objectID": "contents/count_intro.html",
    "href": "contents/count_intro.html",
    "title": "Introduction to Count Models",
    "section": "",
    "text": "A count dependent model is non-negative, an integer, and often skewed. OLS does not model this well."
  },
  {
    "objectID": "contents/count_intro.html#count-models",
    "href": "contents/count_intro.html#count-models",
    "title": "Introduction to Count Models",
    "section": "Count models",
    "text": "Count models\nCount models follow the poisson distribution.\nCount models are theoretically unbounded.\n\nIf you are measuring a bounded phenomenon and your count model is predicted out of scope results, you may want to use an ordered model instead. This is not a very efficient model for count events: you are throwing away a lot of information. However, you will get more realistic predictions.\n\n\nPoisson Regression Model\nExpected count:\n\\[\n\\mu_i = E(y_i | x_i) = e^{x_i\\beta}\n\\]\n\nPotential problems\nThe model is not very good at predicting 0. We will under-predict 0s.\nThe model can often only predict events that have already taken place. It will not predict the first instance of an event.\nOverdispersion occurs when the conditional mean is less than the conditional variance. This occurs when:\n\nAssumes events are independent. This is a very (and often too) strong assumption.\nThere is heterogeneity: the \\(\\mu\\) differs across cases.\n\nOverdispersion produces estimates that are consistent and inefficient. The standard errors are biased downwards. It is, therefore, easier to find statistical significance.\n\nYou have consistent results, as you add more data you will get closer to the true relationship between the dependent and independent variables.\n\n\n\nInterpretation\nThe coefficients give the change in the log of the expected count for a unit change in \\(x_i\\).\nYou can also estimate the count of your variable of interest.\nYou can also estimate changes in the expected count across a discrete difference in \\(x_i\\).\n\n\n\nNegative binomial models\nThis model accounts for overdispersion. It does this by adding a dispersion parameter, \\(\\alpha\\), that allows for heterogeneity and non-independence across events.\n\nGenerally, you should use this model. There will generally be non-independence or heterogeneity across events.\n\nThe expected count is the same as that produced by a poisson regression model. However, the variance differs. #### Interpretation\nSimilar to Poisson:\n\\[\nE(y | x) = e^{X\\beta}\n\\]\n\n\nZero Inflated Models\nThis model allows you to explore very rare events. There needs to be a theoretical expectation that some unknown types of cases will only produce zeros. It is not sufficient for there to simply be a lot of zeros. It is important that you don’t know why these groups can only produce zero. If you know, you should remove them from your dataset. They are a restricted group.\nThis model explores the probability that you never have a count \\(> 0\\). Estimate a binary model for \\(P(y = 0)\\) and use that to weight your data.\n\n\nExposure\nYou need to think about whether exposure differs across cases. Are events more likely to occur in a subset of your data due to some other factor? For example, number of attacks might be greater simply because a group has been around longer. To solve for this, you control for exposure. For example, add ln(exposure) and constrain the coefficient to 1."
  },
  {
    "objectID": "contents/binary_intro_binary.html",
    "href": "contents/binary_intro_binary.html",
    "title": "Introduction to Binary Response Modelling",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(gtsummary)"
  },
  {
    "objectID": "contents/binary_intro_binary.html#introduction",
    "href": "contents/binary_intro_binary.html#introduction",
    "title": "Introduction to Binary Response Modelling",
    "section": "Introduction",
    "text": "Introduction\nWe often want to better understand binary outcomes in political science.\nLet’s start with simulated data to illustrate the theory. Let’s create some data:\n\ndf <- tibble(x = runif(1000, 0, 10)) |> \n  mutate(y = if_else(x < 5, \n                     sample(0:1, 1000, replace = T, prob = c(0.95, 0.05)),\n                     sample(0:1, 1000, replace = T, prob = c(0.05, 0.95))))\n\nhead(df)\n\n# A tibble: 6 × 2\n      x     y\n  <dbl> <int>\n1  5.44     1\n2  3.59     0\n3  7.05     1\n4  5.71     1\n5  3.33     0\n6  3.49     0\n\n\nNow, let’s plot the relationship between our binary dependent variable, y, and our independent variable of interest, x.\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  theme_minimal()\n\n\n\n\nThere seems to be a pretty clear relationship between y and x. When x is less than 5, you are very likely (in fact 95 percent likely) to get a y of 0. But how do we formally measure this?\nWhen working with binary outcomes, we want to understand the probability that you will get an outcome (here, \\(Y = 0\\) or \\(Y = 1\\)) for any given value of your independent variable(s). From there, you can make an informed guess as to the outcome for given values of \\(X\\). For example, where the predicted probability of success is greater than 50 percent, you can predict that \\(y = 1\\). You can also better understand how changing the value of your independent variable impacts the likelihood that you will get a different outcome.\nIn our research, we may want to better understand how some policy choice will impact the likelihood of many different relevant phenomena, including that individuals will vote, countries will go to war, or democracies will become autocracies."
  },
  {
    "objectID": "contents/binary_intro_binary.html#linear-probability-model",
    "href": "contents/binary_intro_binary.html#linear-probability-model",
    "title": "Introduction to Binary Response Modelling",
    "section": "Linear Probability Model",
    "text": "Linear Probability Model\nLet’s start off simple. Let’s draw a straight line between these two clusters and see what we get.\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nThis is our usual linear model:\n\nm_lpr <- lm(y ~ x, data = df)\n\ntbl_regression(m_lpr, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-0.17\n-0.21, -0.13\n<0.001\n    x\n0.13\n0.13, 0.14\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nOur estimation of \\(y_i\\) can (and often does) take on values other than 0 or 1. This is because we can interpret the coefficients of this model as differences in the probability of success (\\(y = 1\\)). We can see that increasing \\(x\\) by one unit increases the probability that \\(y = 1\\) by 13.2%. We have a very high level of confidence in this, with \\(p<0.001\\).\n\nIssues with LPM\nWe run into difficulties using LPM for prediction. First, our model can predict probabilities of success less than 0 and greater than 1. Second, and relatedly, we lose information treating these discrete outcomes (0 or 1) as continuous."
  },
  {
    "objectID": "contents/binary_intro_binary.html#latent-variable-approach",
    "href": "contents/binary_intro_binary.html#latent-variable-approach",
    "title": "Introduction to Binary Response Modelling",
    "section": "Latent Variable Approach",
    "text": "Latent Variable Approach\nWe can only observe one of two outcomes: \\(y = 0\\) (failure) or \\(y = 1\\) (success). The linear model provided above does not account for this very well. How can we improve this model? The latent variable approach assumes a continuous relationship exists between our observed outcome (\\(Y\\)) and our independent variables (\\(X\\)). This continuous relationship is driven by an unobserved outcome: \\(Z\\).\n\\[\nz_i = X_i\\beta + \\epsilon_i\n\\]\nThis set up is familiar to us: it’s a linear model. Critically, though, we need to understand the shape of that independent error term, \\(\\epsilon_i\\). This defines the shape of the continuous relationship that takes us from \\(y = 0\\) to \\(y = 1\\). We have two common options to pick from: logistic or probit.\n\nLogistic Regression\nThe inverse logistic function suits our needs well. First, it is bounded between outcomes of 0 and 1. Second, it allows for a varying impact of a change in \\(X\\) on the probability that \\(Y = 1\\).\nFormally, the inverse logistic function is:\n\\[\nPr(Y = 1 | X) = logit^{-1}(X) = \\frac{e^X}{1 + e^X}\n\\]\nLet’s look at the shape of the inverse logistic function:\n\ntibble(x = seq(-10, 10, by = 0.5)) |> \n  mutate(y = plogis(x)) |> \n  ggplot(aes(x = x, y = y)) + \n  geom_line() + \n  theme_minimal()\n\n\n\n\n\nThe function plogis() gives you the inverse log of a number. For example, plogis(1) returns 0.7310586.\n\nHere, we have simply calculated and then plotted the inverse logit of all values sitting at 0.5 intervals between -10 and 10.\n\nThe model\nLet’s fit a logistic regression line against our data:\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"glm\", se = F, method.args = list(family = binomial(link = \"logit\"))) + \n  theme_minimal()\n\n\n\n\nOur model maps the relationship between our outcome (\\(y\\)) and our independent variable (\\(x\\)). We can interpret it as mapping the probability that \\(y = 1\\) for a given value of \\(x\\), otherwise written as \\(Pr(y = 1|x)\\).\nWe can fit this model as such:\n\nm_lr <- glm(y ~ x, data = df, family = binomial(link = \"logit\"))\n\ntbl_regression(m_lr, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      log(OR)1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-5.3\n-6.0, -4.7\n<0.001\n    x\n1.1\n0.93, 1.2\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nWe will explore how to interpret these coefficients and uncertainty in the next section.\n\n\n\nProbit Regression\nAn alternative approach is probit regression. This model is also bounded between outcomes of 0 and 1 and allows for a varying impact of a change in \\(x\\) on \\(y\\). The only real difference between the logistic and probit regression models are the ways they model the error term, \\(\\epsilon_i\\), in our latent variable \\(z_i\\). Probit replaces the logistic distribution with the normal distribution. This changes how we can interpret the coefficients on our independent variables. We will discuss that in the next section.\nFormally, the probit model is:\n\\[\nPr(Y = 1|X) = \\Phi(X\\beta)\n\\]\nLet’s look at the shape of the probit function:\n\ntibble(x = seq(-10, 10, by = 0.5)) |> \n  mutate(y = pnorm(x)) |> \n  ggplot(aes(x = x, y = y)) + \n  geom_line() + \n  theme_minimal()\n\n\n\n\n\nThe function pnorm() gives you the corresponding value for the normal cumulative distribution function. For example, pnorm(1.96) returns 0.9750021 (think confidence intervals!).\n\n\nThe model\nLet’s fit a probit regression line against our data:\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"glm\", se = F, method.args = list(family = binomial(link = \"logit\"))) + \n  theme_minimal()\n\n\n\n\nQuickly, let’s compare this probit regression (in red) to our logistic regression (in blue):\n\n\n\n\n\nThey both fit very similar models; however, the logistic regression produces fatter tails.\nOur probit model maps the relationship between our outcome (\\(y\\)) and our independent variable (\\(x\\)). Like the logistic regression, we can interpret it as mapping the probability that \\(y = 1\\) for a given value of \\(x\\), otherwise written as \\(Pr(y = 1|x)\\).\nWe can fit this model as such:\n\nm_pr <- glm(y ~ x, data = df, family = binomial(link = \"probit\"))\n\ntbl_regression(m_pr, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-2.7\n-3.0, -2.4\n<0.001\n    x\n0.53\n0.48, 0.58\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nWe will explore how to interpret these coefficients and uncertainty in the next section."
  },
  {
    "objectID": "contents/multinomial_marginal_effects.html",
    "href": "contents/multinomial_marginal_effects.html",
    "title": "Measuring Marginal Effects on Multiple Outcomes",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(sjPlot)\nlibrary(nnet)\nlibrary(broom)\nlibrary(marginaleffects)\nlibrary(gtsummary)\nlibrary(ggeffects)\n\nWe will use the same data as the previous section:\n\n\n        prog    ses write\n1 Vocational    Low    35\n2    General Middle    33\n3 Vocational   High    39\n4 Vocational    Low    37\n5 Vocational Middle    31\n6    General   High    36"
  },
  {
    "objectID": "contents/multinomial_marginal_effects.html#multinomial-logit-regression",
    "href": "contents/multinomial_marginal_effects.html#multinomial-logit-regression",
    "title": "Measuring Marginal Effects on Multiple Outcomes",
    "section": "Multinomial logit regression",
    "text": "Multinomial logit regression\n\nInterpreting the coefficients\nLet’s take another look at the model we created previously:\n\nm1 <- multinom(prog ~ ses + write, data = df)\n\n# weights:  15 (8 variable)\ninitial  value 219.722458 \niter  10 value 179.985215\nfinal  value 179.981726 \nconverged\n\ntbl_regression(m1, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      log(OR)1\n      95% CI1\n      p-value\n    \n  \n  \n    \n      Academic\n    \n    (Intercept)\n-2.9\n-5.1, -0.57\n0.014\n    ses\n\n\n\n    Low\n—\n—\n\n    Middle\n0.53\n-0.34, 1.4\n0.2\n    High\n1.2\n0.15, 2.2\n0.024\n    writing score\n0.06\n0.02, 0.10\n0.007\n    \n      Vocational\n    \n    (Intercept)\n2.4\n0.06, 4.7\n0.044\n    ses\n\n\n\n    Low\n—\n—\n\n    Middle\n0.82\n-0.14, 1.8\n0.092\n    High\n0.18\n-1.1, 1.5\n0.8\n    writing score\n-0.06\n-0.10, -0.01\n0.017\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nWe have three options for interpreting coefficients in multinomial logistic regression.\n\nRelative log odds\nSimilar to log odds ratios in binary logistic regression, these can be interpreted as the effect of a one-unit change in \\(x_i\\) on the log odds of being in category 2 compared to being in category 1. For example, increasing yoru writing score by one point is associated with a 0.06 change in the log odds ratio of choosing the academic program compared to the general program.\n\n\nRelative risk ratios\nSimilar to odds ratios in binary logistic regression, these can be interpreted as the effect of a one-unit change in \\(x_i\\) on the probability of being in category 2 compared to being in category 1. For example, increasing yoru writing score by one point is associated with a 1.06 change in the odds ratio of choosing the academic program compared to the general program.\nYou can get this using broom::tidy():\n\ntidy(m1, exponentiate = T)\n\n# A tibble: 8 × 6\n  y.level    term        estimate std.error statistic p.value\n  <chr>      <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 Academic   (Intercept)   0.0577    1.17      -2.45  0.0145 \n2 Academic   sesMiddle     1.70      0.444      1.20  0.229  \n3 Academic   sesHigh       3.20      0.514      2.26  0.0237 \n4 Academic   write         1.06      0.0214     2.71  0.00682\n5 Vocational (Intercept)  10.7       1.17       2.01  0.0439 \n6 Vocational sesMiddle     2.28      0.490      1.68  0.0925 \n7 Vocational sesHigh       1.20      0.648      0.278 0.781  \n8 Vocational write         0.946     0.0233    -2.39  0.0170 \n\n\nOr plot your results using sjPlot::plot_model():\n\nplot_model(m1, show.values = T, value.offset = .3, order.terms = T)\n\n\n\n\n\n\nPredicted probabilities\nThis describes the probability that an observation will be in a category for a given a set of observed values. For example, the predicted probability that student with a high socio-economic status and the average writing score of 52.78 will choose an academic program is 70%. The predicted probability that they will choose the general program is 18%. Finally, the predicted probability that they will choose the vocational program is 12%.\nbroom::augment() does not currently support multinomial regression. Instead, we can use base R’s predict():\n\npredict(m1, newdata = tibble(ses = \"High\", write = mean(df$write)), type = \"probs\")\n\n   General   Academic Vocational \n 0.1784891  0.7008979  0.1206130 \n\n\nWe can plot these predicted probabilities across our outcomes and variable options using ggeffects::ggeffect():\n\nggeffect(m1, terms = \"ses\") |> \n  plot()\n\n\n\n\n\nggeffect(m1, terms = \"write\") |> \n  plot()\n\n\n\n\n\n\n\nMarginal effects\nWe can interpret the marginal effects of a one-unit change in \\(x_i\\) on the change in probability that an observation will fall into one category. For example, moving from a high to a low socio-economic status decreases the probability that an individual will select the general program by 16%.\nYou can calculate the marginal effects of each change using marginaleffects::marginaleffects():\n\nmarginaleffects(m1, variables = \"ses\", type = \"probs\") |> \n  summary()\n\n       Group Term     Contrast   Effect Std. Error z value Pr(>|z|)    2.5 %\n1    General  ses Middle - Low -0.12447    0.07993 -1.5572 0.119428 -0.28113\n2    General  ses   High - Low -0.16006    0.08689 -1.8420 0.065470 -0.33036\n3   Academic  ses Middle - Low  0.03577    0.08406  0.4255 0.670469 -0.12899\n4   Academic  ses   High - Low  0.23181    0.09370  2.4738 0.013368  0.04815\n5 Vocational  ses Middle - Low  0.08870    0.06982  1.2703 0.203962 -0.04815\n6 Vocational  ses   High - Low -0.07175    0.07433 -0.9652 0.334425 -0.21743\n   97.5 %\n1 0.03219\n2 0.01025\n3 0.20053\n4 0.41546\n5 0.22555\n6 0.07394\n\nModel type:  multinom \nPrediction type:  probs"
  },
  {
    "objectID": "contents/binary_substantive_effects.html",
    "href": "contents/binary_substantive_effects.html",
    "title": "Measuring Substantive Effects on Binary Outcomes",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(gtsummary)\nlibrary(skimr)\nlibrary(mvtnorm)\nlibrary(fastDummies)\n\nWe will use the dataset we set up in the previous section:\n\n\n   vote close edu7cat       homeown\n1 Voted    10       6     Homeowner\n2 Voted    29       6     Homeowner\n3 Voted    28       4     Homeowner\n4 Voted     0       5     Homeowner\n5 Voted    25       7 Not homeowner\n6 Voted    25       5 Not homeowner"
  },
  {
    "objectID": "contents/binary_substantive_effects.html#introduction",
    "href": "contents/binary_substantive_effects.html#introduction",
    "title": "Measuring Substantive Effects on Binary Outcomes",
    "section": "Introduction",
    "text": "Introduction\nPreviously, we explored the effect of a one-unit change in our predictor on our outcome. A more interesting question focuses not on a one-unit change, but rather a theoretically- or policy-relevant change in our predictor. For example, let’s say that a state’s legislature is considering how they can increase participation in federal and state elections. They are considering changing their election registration rules to allow individuals to register on election day. Currently, individuals must register at least 20 days prior to election day to be able to vote in that election. What is the effect on individuals’ predicted likelihood of voting of changing the date of voting registration from 20 days prior to election day to election day? This section will outline how we calculate this substantive effect for binary outcomes.\nAs discussed in the previous section, the effect of a change in \\(x\\) on the predicted probability of success depends on your values of \\(x\\). Unlike linear models, the effect of a one unit change in \\(x\\) on the predicted probability of success is not constant."
  },
  {
    "objectID": "contents/binary_substantive_effects.html#measuring-substantive-effects",
    "href": "contents/binary_substantive_effects.html#measuring-substantive-effects",
    "title": "Measuring Substantive Effects on Binary Outcomes",
    "section": "Measuring substantive effects",
    "text": "Measuring substantive effects\nHow do we actually measure the effect of a change from \\(x_{i1}\\) to \\(x_{i2}\\)? We predict the estimated probability of success at \\(x_{i1}\\) and at \\(x_{i2}\\) and subtract those probabilities from one another to get the difference. Simple, right?\nHowever, there are two factors that complicate this process. First, we need to deal with the other variables in our model: what values should they be held at while we change our variable of interest, \\(x_i\\)? Second, how do we measure uncertainty surrounding this estimated effect? We will deal with these challenges in turn.\n\nWhat to do with the other independent variables\nThere are two dominant approaches to solving this challenge: the average case approach and the observed value approach.\n\nAverage case approach\nThis approach sets all other values to their mean (for continuous variables) or mode (for discrete variables).\n\nFind the mean or mode for all independent variables other than your variable of interest.\nFind your predicted probability of success with your first value of \\(x_i\\), holding all other variables at their mean or mode.\nFind your predicted probability of success with your second value of \\(x_i\\), holding all other variables at their mean or mode.\nCalculate the difference between these predicted probabilities.\nDiscuss the substantive significance of this difference.\n\nTo illustrate, let’s explore the predicted effect of changing a state’s registration voting date from 20 days prior to election day (\\(close = 20\\)) to election day (\\(close = 0\\)).\nFirst, let’s fit a logistic regression as we did in the previous section. Here we will include our controls: edu7cat and homeown.\n\nm1 <- glm(vote ~ close + edu7cat + homeown, data = voters, family = binomial(link = \"logit\"))\n\ntbl_regression(m1, intercept = T, exponentiate = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.45\n0.11, 1.56\n0.2\n    Registration closing\n0.99\n0.98, 1.00\n0.2\n    Education level\n\n\n\n    1\n—\n—\n\n    2\n1.59\n0.43, 6.60\n0.5\n    3\n1.24\n0.35, 4.96\n0.7\n    4\n1.66\n0.48, 6.54\n0.4\n    5\n3.46\n1.00, 13.6\n0.055\n    6\n6.86\n1.95, 27.4\n0.003\n    7\n9.13\n2.49, 38.0\n0.001\n    Homeownership\n\n\n\n    Not homeowner\n—\n—\n\n    Homeowner\n2.28\n1.85, 2.83\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nNow, we create a new dataset that contains the values for close we want to test and sets the other independent variables to their mean or mode values:\n\nnew_data <- tibble(\n  close = c(0, 20),\n  edu7cat = voters |> count(edu7cat) |> filter(n == max(n)) |> pull(edu7cat),\n  homeown = voters |> count(homeown) |> filter(n == max(n)) |> pull(homeown)\n)\n\nnew_data\n\n# A tibble: 2 × 3\n  close edu7cat homeown  \n  <dbl> <fct>   <fct>    \n1     0 4       Homeowner\n2    20 4       Homeowner\n\n\nNext, we calculate the predicted probability that an individual will vote, given these values for our independent variables:\n\nresult_av <- augment(m1, newdata = new_data, type.predict = \"response\")\nresult_av\n\n# A tibble: 2 × 4\n  close edu7cat homeown   .fitted\n  <dbl> <fct>   <fct>       <dbl>\n1     0 4       Homeowner   0.629\n2    20 4       Homeowner   0.601\n\n\nNext, we calculate the difference between these predicted probabilities, noting that the only thing that changed in our model is the value of close:\n\nresult_av <- mutate(result_av, diff = .fitted - lead(.fitted))\nresult_av\n\n# A tibble: 2 × 5\n  close edu7cat homeown   .fitted    diff\n  <dbl> <fct>   <fct>       <dbl>   <dbl>\n1     0 4       Homeowner   0.629  0.0277\n2    20 4       Homeowner   0.601 NA     \n\n\nWe predict that the probability that an individual will vote increases 2.77 percentage points when a state’s closing date for voter registration moves from 20 days prior to election day to election day. If our legislature thinks that is a worthwhile increase in turnout, this is a substantively significant result.1\nHowever, we have a bit of a problem. Is this really a generalisable result? Haven’t we just estimated the effect of this change in registration day closure for a homeowner with an education level of 4? This is the problem with the average case approach: we are discarding an enormous amount of data from our sample which is potentially compromising the generalisability of our predictions. In fact, sometimes our average case isn’t even in our dataset, which means that we are making out-of-sample predictions.\n\n\nObserved value approach\nThe observed value approach addresses this issue. It sets all other independent variables to their observed values, only aggregating the estimated effect at the end.\n\nFind your predicted probability of success with your first value of \\(x_i\\), holding all other variables at their observed values. You will get the same number of predictions as you have observations.\nFind your predicted probability of success with your second value of \\(x_i\\), holding all other variables at their observed values.\nCalculate the average predicted probability for each of these values of \\(x_i\\).\nCalculate the difference between these averages.\nDiscuss the substantive significance of this difference.\n\nLet’s explore the same question as above to illustrate.\nFirst, find the predicted probability of an individual voting when \\(close = 20\\):\n\nresult_20 <- augment(m1, newdata = mutate(voters, close = 20), type.predict = \"response\")\nresult_20\n\n# A tibble: 2,188 × 5\n   vote         close edu7cat homeown       .fitted\n   <fct>        <dbl> <fct>   <fct>           <dbl>\n 1 Voted           20 6       Homeowner       0.861\n 2 Voted           20 6       Homeowner       0.861\n 3 Voted           20 4       Homeowner       0.601\n 4 Voted           20 5       Homeowner       0.758\n 5 Voted           20 7       Not homeowner   0.784\n 6 Voted           20 5       Not homeowner   0.579\n 7 Voted           20 7       Homeowner       0.892\n 8 Voted           20 6       Homeowner       0.861\n 9 Voted           20 2       Homeowner       0.590\n10 Did not vote    20 5       Homeowner       0.758\n# … with 2,178 more rows\n\n\nNext, find the predicted probability of an individual voting when \\(close = 0\\):\n\nresult_0 <- augment(m1, newdata = mutate(voters, close = 0), type.predict = \"response\")\nresult_0\n\n# A tibble: 2,188 × 5\n   vote         close edu7cat homeown       .fitted\n   <fct>        <dbl> <fct>   <fct>           <dbl>\n 1 Voted            0 6       Homeowner       0.875\n 2 Voted            0 6       Homeowner       0.875\n 3 Voted            0 4       Homeowner       0.629\n 4 Voted            0 5       Homeowner       0.779\n 5 Voted            0 7       Not homeowner   0.803\n 6 Voted            0 5       Not homeowner   0.607\n 7 Voted            0 7       Homeowner       0.903\n 8 Voted            0 6       Homeowner       0.875\n 9 Voted            0 2       Homeowner       0.618\n10 Did not vote     0 5       Homeowner       0.779\n# … with 2,178 more rows\n\n\nNext, calculate the average predicted probability for \\(close = 20\\) and \\(close = 0\\):\n\nresult_ov <- result_0 |> \n  bind_rows(result_20) |> \n  group_by(close) |> \n  summarise(.fitted = mean(.fitted)) |> \n  mutate(diff = .fitted - lead(.fitted))\n\nresult_ov\n\n# A tibble: 2 × 3\n  close .fitted    diff\n  <dbl>   <dbl>   <dbl>\n1     0   0.688  0.0228\n2    20   0.666 NA     \n\n\nWe expect that the probability that an individual will vote increases 2.28 percentage points when a state’s closing date for voter registration moves from 20 days prior to election day to election day. If our legislature thinks that is a worthwhile increase in turnout, this is a substantively significant result.2\n\n\nWhich approach should you use?\nYou should use the observed values approach. Hanmer and Kalkan (2013) demonstrate using simulated data that the observed values approach consistently produces estimates closer to the population’s true probability than the average case approach. This makes sense: you are using more data to produce your estimated effects.\n\n\n\nMeasuring uncertainty when calculating substantive effects\nWe calculated these estimates using a model that includes error. We need to understand how this uncertainty impacts our estimated substantive effects. How do we get confidence intervals around our estimated effects?\nImagine we are trying to estimate the relationship between some binary outcome \\(Y\\) and some independent variables \\(X\\) and \\(Z\\). We take a representative sample from our population and fit a model against that data. If we were to take a different representative sample from our population and fit the same model to that data, we will probably get slightly different estimates for our \\(\\beta\\)s. This is because of the random error inherent in modelling observed data. We can make some useful assumptions about the distribution of these difference errors produced by these different samples.\nThese assumptions depend on whether you are using logistic or probit regression, but the theory is the same. If we were to take many, many, many (say, 1,000) different representative samples from our population and fit many, many, many different models, we would get a set of \\(\\beta\\) estimates that follow either a normal (for probit) or inverse logistic (for logit) distribution. Your original estimated \\(\\beta\\) will be within this distribution.\nWe can use this to generate our confidence intervals. We just need to simulate fitting these 1,000 different models.\n\nIt is critical that your sample is representative of your population. We are not simulating drawing 1,000 different samples. Rather, we are taking our one model fitted against our one sample and drawing estimates around those \\(\\beta\\)s.\n\n\nSimulate fitting 1,000 different models by drawing 1,000 different \\(\\beta_i\\) around your estimated \\(\\beta_i\\) following your (logistic or probit) model’s distribution.\nPredict the probability of success for \\(x_{i1}\\) and \\(x_{i2}\\) using these 1,000 different model estimates.\nCalculate the difference between those predictions.\nCalculate the lower and upper confidence intervals and the mean of those differences.\n\nLet’s illustrate this by looking at our question above.\nRecall our logistic regression model from above:\n\ntbl_regression(m1, intercept = T, exponentiate = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.45\n0.11, 1.56\n0.2\n    Registration closing\n0.99\n0.98, 1.00\n0.2\n    Education level\n\n\n\n    1\n—\n—\n\n    2\n1.59\n0.43, 6.60\n0.5\n    3\n1.24\n0.35, 4.96\n0.7\n    4\n1.66\n0.48, 6.54\n0.4\n    5\n3.46\n1.00, 13.6\n0.055\n    6\n6.86\n1.95, 27.4\n0.003\n    7\n9.13\n2.49, 38.0\n0.001\n    Homeownership\n\n\n\n    Not homeowner\n—\n—\n\n    Homeowner\n2.28\n1.85, 2.83\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nLet’s collect those coefficients using broom::tidy():\n\ncoefs <- tidy(m1) |> pull(estimate)\ncoefs\n\n[1] -0.80698206 -0.00585002  0.46146385  0.21142686  0.50831506  1.24130230\n[7]  1.92500744  2.21140696  0.82613712\n\n\nNext, simulate fitting 1,000 different models using these estimates as our center-point using mvtnorm::rmvnorm():\n\nTODO: Learn more about the sigma.\n\n\ncoefs_sim <- rmvnorm(n = 1000, mean = coefs, sigma = vcov(m1)) |> \n  as_tibble() |> \n  set_names(tidy(m1) |> mutate(term = paste0(term, \"_beta\")) |> pull(term))\n\nhead(coefs_sim)\n\n# A tibble: 6 × 9\n  (Intercept…¹ close_…² edu7ca…³ edu7c…⁴ edu7c…⁵ edu7c…⁶ edu7c…⁷ edu7c…⁸ homeo…⁹\n         <dbl>    <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1     -0.350   -0.00601 -0.505   -0.280   0.141    0.817   1.28     1.88   0.644\n2      0.00668 -0.0205   0.333   -0.109   0.0168   0.745   1.61     1.40   0.784\n3     -0.978   -0.00649  0.695    0.549   0.577    1.23    2.17     2.22   0.952\n4     -0.920   -0.00127  0.312    0.404   0.637    1.18    2.19     2.56   0.619\n5     -0.138    0.00172 -0.580   -0.560  -0.381    0.249   0.973    1.15   0.924\n6     -0.810   -0.00216  0.00705 -0.0751  0.299    1.14    1.68     2.18   0.973\n# … with abbreviated variable names ¹​`(Intercept)_beta`, ²​close_beta,\n#   ³​edu7cat2_beta, ⁴​edu7cat3_beta, ⁵​edu7cat4_beta, ⁶​edu7cat5_beta,\n#   ⁷​edu7cat6_beta, ⁸​edu7cat7_beta, ⁹​homeownHomeowner_beta\n\n\nLet’s look at the distribution of these simulated \\(\\beta\\)s for one of our variables: close:\n\nggplot(coefs_sim, aes(x = close_beta)) + \n  geom_histogram() + \n  geom_vline(xintercept = tidy(m1) |> filter(term == \"close\") |> pull(estimate)) + \n  theme_minimal()\n\n\n\n\nThis draw follows the inverse logit distribution and centered around our estimated \\(\\beta_{close}\\) (highlighted by the black line).\nNext, we need to predict the probability that an individual will vote when \\(close = 20\\) and when \\(close = 0\\) using these 1,000 different model estimates. To do this, we need to convert our categorical variables into dummy variables so that we can fit the correct \\(\\beta\\)s to them.\nWe can use fastDummies::dummy_col() to do this:\n\ntrans_data <- voters |> \n  dummy_cols(select_columns = \"edu7cat\", remove_first_dummy = TRUE) |> \n  mutate(homeown = as.integer(homeown))\n\nhead(trans_data)\n\n   vote close edu7cat homeown edu7cat_2 edu7cat_3 edu7cat_4 edu7cat_5 edu7cat_6\n1 Voted    10       6       2         0         0         0         0         1\n2 Voted    29       6       2         0         0         0         0         1\n3 Voted    28       4       2         0         0         1         0         0\n4 Voted     0       5       2         0         0         0         1         0\n5 Voted    25       7       1         0         0         0         0         0\n6 Voted    25       5       1         0         0         0         1         0\n  edu7cat_7\n1         0\n2         0\n3         0\n4         0\n5         1\n6         0\n\n\n\n\\(edu7cat = 1\\) is our reference category. We can remove it because it is not directly included in our model.\n\nNow, we include our close variable, set to 0 and 20 for each of these 2,188 observations. We should get a dataset of length 4,376: one set of observations for \\(close = 20\\) and one set for \\(close = 0\\).\n\nnew_data <- trans_data |> \n  mutate(close = 0) |> \n  bind_rows(mutate(trans_data, close = 20)) |> \n  group_by(close) |> \n  mutate(id = row_number()) |> \n  ungroup()\n\nnrow(new_data)\n\n[1] 4376\n\n\nNext, we need to join our datasets together, so we can calculate our predicted probability for each observation for each simulated model coefficient. We should get a dataframe with a length of 2 x 1,000 x 2,188 (number of different variables of interest x number of models x number of observations).\n\nsim_data <- coefs_sim |> \n  mutate(sim_round = row_number()) |> \n  full_join(new_data, by = character())\n\nnrow(sim_data)\n\n[1] 4376000\n\n\nWe can now estimate our logistic regression model using the 1,000 different estimated \\(\\beta\\)s for all 2,188 different observations. You need to first fit the linear model, then find the inverse logit of those results using plogis().\n\nresults <- sim_data |> \n  mutate(\n    .fitted = `(Intercept)_beta` +\n      close_beta * close +\n      edu7cat2_beta * edu7cat_2 +\n      edu7cat3_beta * edu7cat_3 +\n      edu7cat4_beta * edu7cat_4 +\n      edu7cat5_beta * edu7cat_5 +\n      edu7cat6_beta * edu7cat_6 +\n      edu7cat7_beta * edu7cat_7 +\n      homeownHomeowner_beta * homeown,\n    .fitted = plogis(.fitted)\n  ) |> \n  arrange(sim_round, id, close)\n\nWe then calculate the difference between the predicted probabilities for each observation when \\(close = 0\\) and \\(close = 20\\):\n\nresults |>\n  group_by(sim_round, id) |> \n  mutate(diff = (.fitted - lead(.fitted)) * 100) |> \n  drop_na(diff) |> \n  ungroup() |> \n  summarise(`Lower bound` = quantile(diff, 0.025),\n            `Mean` = quantile(diff, 0.5),\n            `Upper bound` = quantile(diff, 0.975))\n\n# A tibble: 1 × 3\n  `Lower bound`  Mean `Upper bound`\n          <dbl> <dbl>         <dbl>\n1         -1.15  1.39          5.42\n\n\nThis simulated result is very close to the difference we calculated using both the average case (2.77 percentage points) and observed value (2.28) approaches. But now we have a confidence interval around this estimate. Because this confidence interval crosses through 0, we cannot reject the null hypothesis that this substantive effect is caused by random error."
  },
  {
    "objectID": "contents/binary_marginal_effects.html",
    "href": "contents/binary_marginal_effects.html",
    "title": "Measuring Marginal Effects on Binary Outcomes",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(skimr)\nlibrary(broom)\nlibrary(gtsummary)\n\nLet’s explore marginal and substantive effects using real-world data. How is a person’s decision to vote influenced by the closing date of registration to vote in US elections? Suppose we hypothesize that the further from election day registration closes, the less likely an individual is to vote. We also believe that there are other socio-economic factors that influence a person’s decision to vote for which we need to control. These are their level of education, and whether they are a homeowner.\nLet’s explore our data. First, we need to load it in. I will use rio::import().\n\nrio allows you to import or export many different file types with a single command. This makes your code robust to changes in file types: if your data source changes the file type for your data, your code will not break. I recommend it for interactive coding.1 The package documentation can be found here.\n\n\nvoters_raw <- import(\"/Users/harrietgoers/Documents/GVPT729A/class_sets/data/cps00for729a.dta\")\n\nThis dataset contains individual-level survey data of US adults. It includes: information on whether they voted (vote); the number of days prior to election day their state closes registration (close); their level of education recorded in seven, ordered categories (edu7cat); and whether or not they are a homeowner (homeown).\nNext, we need to clean this data up:\n\nvoters <- voters_raw |> \n  transmute(vote = factor(vote, levels = c(0, 1), labels = c(\"Did not vote\", \"Voted\")), \n            close = as.integer(close), \n            edu7cat = factor(edu7cat), \n            homeown = factor(homeown, levels = c(0, 1), labels = c(\"Not homeowner\", \"Homeowner\"))) |> \n  labelled::set_variable_labels(vote = \"Voted\", close = \"Registration closing\", edu7cat = \"Education level\", homeown = \"Homeownership\") |> \n  drop_na()\n\nhead(voters)\n\n   vote close edu7cat       homeown\n1 Voted    10       6     Homeowner\n2 Voted    29       6     Homeowner\n3 Voted    28       4     Homeowner\n4 Voted     0       5     Homeowner\n5 Voted    25       7 Not homeowner\n6 Voted    25       5 Not homeowner\n\n\nNote that we have removed any observations with missing values.\n\nIf your categorical variables are stored as numeric data in your dataset, your model will treat them as continuous numeric variables. It will not exclude a base category. This will cause significant problems with your model. Always convert categorical variables to factors.2\n\nNow, let’s look at a summary of our data using skimr::skim():\n\nskim(voters)\n\n\nData summary\n\n\nName\nvoters\n\n\nNumber of rows\n2188\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nvote\n0\n1\nFALSE\n2\nVot: 1450, Did: 738\n\n\nedu7cat\n0\n1\nFALSE\n7\n4: 724, 5: 623, 6: 380, 7: 186\n\n\nhomeown\n0\n1\nFALSE\n2\nHom: 1665, Not: 523\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nclose\n0\n1\n22.29\n9.87\n0\n15\n29\n30\n30\n▁▁▂▁▇\n\n\n\n\n\nOur dataset contains 2,188 observations and 4 variables. Of those 2,188 individuals, 66% voted."
  },
  {
    "objectID": "contents/binary_marginal_effects.html#introduction",
    "href": "contents/binary_marginal_effects.html#introduction",
    "title": "Measuring Marginal Effects on Binary Outcomes",
    "section": "Introduction",
    "text": "Introduction\nOur goal is to make inferences from the sample to the population about how changes in our independent variable of interest, \\(x\\), influences the probability of success in our outcome of interest, \\(y\\). We can calculate this effect for each known value of \\(x\\), or the marginal effect. We can also calculate this effect for a meaningful change in the value of \\(x\\), or the substantive effect. We will discuss this in the next section.\nThe marginal effect measures the change in the probability of success gained (or lost) from changing \\(x_i\\) by one unit. In our example, we want to understand the marginal effect of close, or how increasing the number of days prior to an election registration closes by one impacts the likelihood that an individual will vote."
  },
  {
    "objectID": "contents/binary_marginal_effects.html#linear-probability-models",
    "href": "contents/binary_marginal_effects.html#linear-probability-models",
    "title": "Measuring Marginal Effects on Binary Outcomes",
    "section": "Linear probability models",
    "text": "Linear probability models\nIn linear models, this effect is constant. To illustrate, let’s fit a linear probability model to our voting data. We will focus only on the effect of close on vote for now.\n\nNote that lm() will not allow us to fit a linear model to binary data!\n\n\nvoters_linear <- mutate(voters, vote = as.integer(vote) - 1)\n\nm1 <- lm(vote ~ close, data = voters_linear)\n\ntbl_regression(m1, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.71\n0.66, 0.75\n<0.001\n    Registration closing\n0.00\n0.00, 0.00\n0.060\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\nggplot(voters_linear, aes(x = close, y = vote)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\n\nInterpreting the coefficients\nThis model suggests that increasing the registration closing day by one decreases the likelihood than individual will vote by 0.192%, on average. However, this result is not statistically significant. A individual registered in a state with election day registration (\\(close = 0\\)) will vote 71% of the time, on average.\nThe important take away from this is that the marginal effect of close on vote is constant across all values of close. If your state changes from election day registration to closing registration one day prior to election day, your probability of voting decreases 0.192%. Similarly, if your state changes from 20 days prior to 21 days prior to election day, your probability of voting decreases 0.192%.\n\n\nPredicting outcomes using your LPM\nLet’s take a closer look at this. We can use broom::augment() to see what our model predicts the probability of an individual to vote to be for each of the plausible closing dates (0 to 30 days prior to an election):\n\npred_m1 <- augment(m1, newdata = tibble(close = 0:30), type.predict = \"response\")\nhead(pred_m1)\n\n# A tibble: 6 × 2\n  close .fitted\n  <int>   <dbl>\n1     0   0.706\n2     1   0.704\n3     2   0.702\n4     3   0.700\n5     4   0.698\n6     5   0.696\n\n\n\nggplot(pred_m1, aes(x = close, y = .fitted * 100)) + \n  geom_line() + \n  theme_minimal() + \n  labs(x = \"Closing date (days)\",\n       y = \"Predicted probability of voting (%)\")\n\n\n\n\n\n\nMarginal effects\nFinally, let’s look at the difference in the predicted probability of voting by chnaging the closing date by one day for all plausible values of close:\n\nme_m1 <- pred_m1 |> \n  arrange(close) |> \n  mutate(diff = .fitted - lag(.fitted))\n\nhead(me_m1)\n\n# A tibble: 6 × 3\n  close .fitted     diff\n  <int>   <dbl>    <dbl>\n1     0   0.706 NA      \n2     1   0.704 -0.00192\n3     2   0.702 -0.00192\n4     3   0.700 -0.00192\n5     4   0.698 -0.00192\n6     5   0.696 -0.00192\n\n\n\nggplot(me_m1, aes(x = close, y = diff * 100)) + \n  geom_line() + \n  geom_hline(yintercept = 0, colour = \"darkgrey\") + \n  theme_minimal() + \n  scale_y_continuous(limits = c(-0.3, 0.3)) + \n  labs(x = \"Closing date (days)\",\n       y = \"Difference in predicted probability of voting (%)\")\n\n\n\n\nAs expected, this difference is constant. It is, in fact, the close coefficient. In LPMs, the variable coefficients are their marginal effects."
  },
  {
    "objectID": "contents/binary_marginal_effects.html#latent-variable-models",
    "href": "contents/binary_marginal_effects.html#latent-variable-models",
    "title": "Measuring Marginal Effects on Binary Outcomes",
    "section": "Latent variable models",
    "text": "Latent variable models\nBoth logit and probit models are curved. Therefore, the effect of a one-unit increase in \\(x_i\\) on the probability that \\(Y=1\\) is not constant. In fact, the marginal effect of a one-unit change in \\(x_i\\) depends on the starting value of \\(x_i\\). As demonstrated in the figure below, the steepest change for both the logit (blue) and probit (red) models occurs around the middle values of \\(x\\).\n\n\n\n\n\nIn fact, these marginal effects are bell-shaped:\n\n\n\n\n\nTo illustrate, let’s look at the effect on the probability of success of moving from \\(x = 2\\) to \\(x = 3\\) compared to the effect of moving from \\(x = 4\\) to \\(x = 5\\):\n\n\n\n\n\nMoving from \\(x = 2\\) (highlighted in light blue) to \\(x = 3\\) (highlighted in dark blue) increases the probability of success by 0.061, from 0.032 to 0.093. Moving the same interval of one unit from \\(x = 4\\) (highlighted in pink) to \\(x = 5\\) (highlighted in red) increases the probability of success by 0.258, from 0.243 to 0.501. That’s a 4.209 times increase in the effect of a one unit change in \\(x\\).\nTherefore, to measure the effect of moving from one value of \\(x\\) to another in a non-linear model, we need to know which values of \\(x\\) we are moving between.\nWe will look at logit and probit in turn.\n\nLogistic Regression\nFirst, let’s fit our logit model, focusing only on the effect of close on vote:\n\nm2 <- glm(vote ~ close, data = voters, family = binomial(link = \"logit\"))\n\ntbl_regression(m2, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      log(OR)1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.87\n0.65, 1.1\n<0.001\n    Registration closing\n-0.01\n-0.02, 0.00\n0.061\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\n\nInterpreting the coefficients\nThe coefficients presented above are log odds ratios. We can easily interpret their statistical significance and their sign. For example, we know that our independent variable of interest, close, is not statistically significant (\\(p = 0.061\\)). We also know that its effect on an individual’s decision to vote is negative: as days before an election the date of voter registration closes increases, the likelihood that an individual will vote decreases.\nRemember that logit (and probit) models are simply transformed linear models. We take our linear model of the relationship between close and vote and reshape it to better predict probabilities (bound it between 0 and 1, and reshape it to reflect varying effects of changes of \\(x\\) on \\(y\\)). We can reshape these coefficients to make them more interpretable.\n\nOdds ratios\nThe regression coefficient provided above is a log odds ratio. Log-transformed variables are really hard to interpret. So, let’s get rid of that log. The opposite operation to taking the logarithum of a number is to exponentiate it. To demonstrate:\n\ntibble(x = 1:5,\n       log_x = log(x),\n       exp_log_x = exp(log_x))\n\n# A tibble: 5 × 3\n      x log_x exp_log_x\n  <int> <dbl>     <dbl>\n1     1 0             1\n2     2 0.693         2\n3     3 1.10          3\n4     4 1.39          4\n5     5 1.61          5\n\n\nSo, exponentiating the log odd ratio will get us the odds ratio. The odds ratio is much easier to interpret. If the probability of success of an outcome is \\(p\\) and, therefore, the probability of failure is \\(1-p\\), the the odds of success is \\(\\frac{p}{1-p}\\). Now, dividing two odds by each other gives you their odds ratio. For example, if two outcomes have the odds \\(\\frac{p_1}{1-p_1}\\) and \\(\\frac{p_2}{1-p_2}\\), then these outcomes have an odds ratio of \\(\\frac{\\frac{p_1}{1-p_1}}{\\frac{p_2}{1-p_2}}\\).\nThis is particularly useful for comparing the probability of success and failure for a given value of \\(x_i\\). When the odds ratio is 1, the odds of success are the same as the odds of failure (\\(\\frac{0.5}{0.5} = 1\\)). When the odds ratio is greater than 1, the odds of success are greater than the odds of failure (for example, \\(\\frac{0.8}{0.2} = 4\\)). Where the odds ratio of success is four, the odds of success are four times higher than the odds of failure.\nGetting back to our model, the odds ratio that an individual will vote is:\n\\[\ne^{-0.01} = 0.99\n\\]\nSo, increasing the closing date by one day makes the decreases the odds that an individual will vote by 0.01 (or \\(1 - 0.99\\)).\nHappily, gtsummary::tbl_regression() can easily present these results for us:\n\ntbl_regression(m2, intercept = T, exponentiate = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n2.39\n1.91, 3.00\n<0.001\n    Registration closing\n0.99\n0.98, 1.00\n0.061\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nAnd we can also get these results programmatically using broom::tidy():\n\ntidy(m2, exponentiate = T)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    2.39    0.115        7.60 2.90e-14\n2 close          0.991   0.00467     -1.88 6.07e- 2\n\n\n\n\nPredicted probabilities\nOdds ratios are certainly easier to interpret than log odds ratios, but they are still a bit awkward. It is easier again to discuss the effects of changing our independent variables of interest in terms of probabilities, rather than odds.\nRemember, the odds ratio is simply the odds of success divided by the odds of failure:\n\\[\nOR = \\frac{\\frac{p}{1-p}}{\\frac{1-p}{p}}\n\\]\nWe are interested in getting the odds of success (\\(p\\)). To get this:\n\\[\np = \\frac{OR}{1 + OR}\n\\]\nMore generally:\n\\[\nPr(Y = 1|X) = \\frac{e^{X\\beta}}{1 + e^{X\\beta}}\n\\]\nFor our simple model, this is:\n\\[\nPr(vote = 1 | close) = \\frac{e^{0.872 - 0.01close + \\epsilon}}{1 + e^{0.872 - 0.01close + \\epsilon}}\n\\]\nWhere \\(vote = 0.872 - 0.01 close + \\epsilon\\) is our logit model. Critically, the predicted probability still includes close. We need to provide a value for close to calculate a predicted probability.\n\n\n\nInterpreting the intercept\nAs usual, the intercept should be interpreted as the expected value when all independent variables are set to 0. This is simple to interpret in terms of the probability of success. Remember:\n\\[\nPr(Y = 1 | X) = \\frac{e^{X\\beta}}{1 + e^{X\\beta}}\n\\]\nTherefore, for our voter model:\n\\[\n\\frac{e^{\\beta_0}}{1 + e^{\\beta_0}} = \\frac{e^{0.87}}{1 + e^{0.87}} = \\frac{2.39}{3.39} = 0.705\n\\]\nThe probability that an individual in a state with election day registration will vote is 70.5%, on average.\n\n\nPredicting outcomes\nYou now have a couple of different options in terms of how you present and interpret your predicted outcomes from your logit model: log odds ratios, odds ratios, and predicted probability.\nI find predicted probabilities the easiest to interpret and with which to work. Using augment::broom(), we can easily produce our predicted probabilities for each of the plausible values of close:\n\npred_m2 <- augment(m2, newdata = tibble(close = 0:30), type.predict = \"response\")\npred_m2\n\n# A tibble: 31 × 2\n   close .fitted\n   <int>   <dbl>\n 1     0   0.705\n 2     1   0.703\n 3     2   0.701\n 4     3   0.700\n 5     4   0.698\n 6     5   0.696\n 7     6   0.694\n 8     7   0.692\n 9     8   0.690\n10     9   0.688\n# … with 21 more rows\n\n\nAll this function is doing, is calculating \\(Pr(Y = 1 | X) = \\frac{e^{X\\beta}}{1 + e^{X\\beta}}\\) for each of these values of close. You can check this yourself:\n\npred_prob_logit <- function(close) {\n  \n  intercept <- tidy(m2) |> \n    filter(term == \"(Intercept)\") |> \n    pull(estimate)\n\n  beta_close <- tidy(m2) |> \n    filter(term == \"close\") |> \n    pull(estimate)\n\n  exp(intercept + (beta_close * close)) / (1 + exp(intercept + (beta_close * close)))\n  \n}\n\n# Calculate the predicted probability an individual will vote when closing date is \n# three days prior to the election\npred_prob_logit(3)\n\n[1] 0.6996091\n\n\nVisually:\n\nggplot(pred_m2, aes(x = close, y = .fitted * 100)) + \n  geom_line() + \n  theme_minimal() + \n  labs(x = \"Closing date (days)\",\n       y = \"Predicted probability of voting (%)\")\n\n\n\n\nOkay, so this looks linear but I promise it is not! We will get to that shortly. This graph shows the predicted probability that an individual will vote for each plausible closing date.\n\n\nMarginal effects\nFinally, let’s look at the difference in the predicted probability of voting by changing the closing date by one day for all plausible values of close:\n\nme_m2 <- pred_m2 |> \n  arrange(close) |> \n  mutate(diff = .fitted - lag(.fitted))\n\nhead(me_m2)\n\n# A tibble: 6 × 3\n  close .fitted     diff\n  <int>   <dbl>    <dbl>\n1     0   0.705 NA      \n2     1   0.703 -0.00182\n3     2   0.701 -0.00183\n4     3   0.700 -0.00184\n5     4   0.698 -0.00184\n6     5   0.696 -0.00185\n\n\n\nggplot(me_m2, aes(x = close, y = diff * 100)) + \n  geom_line() + \n  geom_hline(yintercept = 0, colour = \"darkgrey\") + \n  theme_minimal() + \n  scale_y_continuous(limits = c(-0.3, 0.3)) + \n  labs(x = \"Closing date (days)\",\n       y = \"Difference in predicted probability of voting (%)\")\n\n\n\n\nThe effect change is not constant. Moving from \\(close = 0\\) to \\(close = 1\\) decreases the predicted probability that an individual will vote by 0.182%. Moving from \\(close = 20\\) to \\(close = 21\\) decreases the predicted probability that an individual will vote by 0.195%.\nThese marginal effects are really small! Does changing the registration closing date in a state have a substantial impact on the likelihood that an individual will vote? We will discuss interpreting substantive significance in the next chapter.\n\n\n\nProbit Regression\nNext, let’s fit a probit model, focusing only on the effect of close on vote:\n\nm3 <- glm(vote ~ close, data = voters, family = binomial(link = \"probit\"))\n\ntbl_regression(m3, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.54\n0.40, 0.68\n<0.001\n    Registration closing\n-0.01\n-0.01, 0.00\n0.059\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\nInterpreting the coefficients\nThe regression coefficients presented above give the change in the z-score or probit index for a one unit change in the predictor. These can be interpreted as the change in the probability of success for a one unit change in the predictor. Unlike logit, we get straight to the point.\n\n\nInterpreting the intercept\n\nTODO\n\n\n\nPredicting outcomes\nUsing augment::broom(), we can easily produce our predicted probabilities for each of the plausible values of close:\n\npred_m3 <- augment(m3, newdata = tibble(close = 0:30), type.predict = \"response\")\npred_m3\n\n# A tibble: 31 × 2\n   close .fitted\n   <int>   <dbl>\n 1     0   0.705\n 2     1   0.703\n 3     2   0.702\n 4     3   0.700\n 5     4   0.698\n 6     5   0.696\n 7     6   0.694\n 8     7   0.692\n 9     8   0.690\n10     9   0.688\n# … with 21 more rows\n\n\nVisually:\n\nggplot(pred_m3, aes(x = close, y = .fitted * 100)) + \n  geom_line() + \n  theme_minimal() + \n  labs(x = \"Closing date (days)\",\n       y = \"Predicted probability of voting (%)\")\n\n\n\n\n\n\nMarginal effects\nFinally, let’s look at the difference in the predicted probability of voting by changing the closing date by one day for all plausible values of close:\n\nme_m3 <- pred_m3 |> \n  arrange(close) |> \n  mutate(diff = .fitted - lag(.fitted))\n\nme_m3\n\n# A tibble: 31 × 3\n   close .fitted     diff\n   <int>   <dbl>    <dbl>\n 1     0   0.705 NA      \n 2     1   0.703 -0.00185\n 3     2   0.702 -0.00185\n 4     3   0.700 -0.00186\n 5     4   0.698 -0.00186\n 6     5   0.696 -0.00187\n 7     6   0.694 -0.00187\n 8     7   0.692 -0.00188\n 9     8   0.690 -0.00188\n10     9   0.688 -0.00189\n# … with 21 more rows\n\n\n\nggplot(me_m3, aes(x = close, y = diff * 100)) + \n  geom_line() + \n  geom_hline(yintercept = 0, colour = \"darkgrey\") + \n  theme_minimal() + \n  scale_y_continuous(limits = c(-0.3, 0.3)) + \n  labs(x = \"Closing date (days)\",\n       y = \"Difference in predicted probability of voting (%)\")\n\n\n\n\nAs with our logit model, the effect change is not constant. Moving from \\(close = 0\\) to \\(close = 1\\) decreases the predicted probability that an individual will vote by 0.185%. Moving from \\(close = 20\\) to \\(close = 21\\) decreases the predicted probability that an individual will vote by 0.195%."
  },
  {
    "objectID": "contents/ordered_logistic.html",
    "href": "contents/ordered_logistic.html",
    "title": "Ordered Logistic Regression",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(broom)\n\n\nI recommend loading MASS first, or using it directly (MASS::polr()). Super annoyingly, MASS masks dplyr’s select() function. This can lead to errors.\n\nWe will use the dataset we discussed in the previous section:\n\n\n            apply pared public  gpa\n1     Very likely     0      0 3.26\n2 Somewhat likely     1      0 3.21\n3        Unlikely     1      1 3.94\n4 Somewhat likely     0      0 2.81\n5 Somewhat likely     0      0 2.53\n6        Unlikely     0      1 2.59"
  },
  {
    "objectID": "contents/ordered_logistic.html#introduction",
    "href": "contents/ordered_logistic.html#introduction",
    "title": "Ordered Logistic Regression",
    "section": "Introduction",
    "text": "Introduction\nOrdered logistic regression models estimate the log odds ratio of an outcome, \\(Y^*\\), being at any level up to and including a specified level, \\(\\tau_i\\). Recall that \\(\\tau_i\\) is the cut off point at which we define the end of an ordered category.\n\\[\nln(\\frac{Pr(Y^* \\leq \\tau_i)}{Pr(Y^* > \\tau_i)})\n\\]\nNote that we need to develop a model for each threshold, or \\(\\tau_i\\). We are estimating a series of binary logistic regression models that provide us with the relative probability that our outcome \\(Y^*\\) sits at or below the threshold for each category compared to the probability that it sits above this threshold. For an outcome with three categories, we will get two different models (we have two different thresholds).\n\nWe will always get \\(J - 1\\) models for our \\(J-1\\) thresholds where \\(J\\) is the number of categories in our dependent variable.\n\nLet’s return to our illustrative example. How likely is an individual to apply for graduate school given three factors: their parents’ level of education; whether they went to a public or private university for their undergraduate degree; and their grade point average?\nLet’s fit our ordered logistic regression using MASS::polr():\n\nm1 <- polr(apply ~ pared + public + gpa, data = df, Hess = T)\n\nsummary(m1)\n\nCall:\npolr(formula = apply ~ pared + public + gpa, data = df, Hess = T)\n\nCoefficients:\n           Value Std. Error t value\npared1   1.04769     0.2658  3.9418\npublic1 -0.05879     0.2979 -0.1974\ngpa      0.61594     0.2606  2.3632\n\nIntercepts:\n                            Value   Std. Error t value\nUnlikely|Somewhat likely     2.2039  0.7795     2.8272\nSomewhat likely|Very likely  4.2994  0.8043     5.3453\n\nResidual Deviance: 717.0249 \nAIC: 727.0249 \n\n\n\nWe specify Hess = TRUE to have the model return the observed information matrix from optimization (called the Hessian) which is used to get standard errors.\n\n\nInterpreting the intercepts\nAs expected, we get two different intercepts. One for the threshold sitting at the boundary of unlikely and somewhat likely and another at the boundary of somewhat likely and very likely.\nWe can access these intercepts using broom::tidy():\n\ntidy(m1) |> \n  filter(coef.type == \"scale\")\n\n# A tibble: 2 × 5\n  term                        estimate std.error statistic coef.type\n  <chr>                          <dbl>     <dbl>     <dbl> <chr>    \n1 Unlikely|Somewhat likely        2.20     0.780      2.83 scale    \n2 Somewhat likely|Very likely     4.30     0.804      5.35 scale    \n\n\nWe can get the odds ratio using exponentiate = TRUE:\n\ntidy(m1, exponentiate = T) |> \n  filter(coef.type == \"scale\")\n\n# A tibble: 2 × 5\n  term                        estimate std.error statistic coef.type\n  <chr>                          <dbl>     <dbl>     <dbl> <chr>    \n1 Unlikely|Somewhat likely        9.06     0.780      2.83 scale    \n2 Somewhat likely|Very likely    73.7      0.804      5.35 scale    \n\n\nLet’s convert these intercepts to predicted probabilities, which are the easiest to interpret. Remember from binary logistic regression that the predicted probability is calculated using:\n\\[\nPr(Y = 1 | X) = \\frac{e^{X\\beta}}{1 + e^{X\\beta}}\n\\]\nFor the intercept, this is simply:\n\nm1_pred_prob <- tidy(m1) |> \n  filter(coef.type == \"scale\") |> \n  mutate(pred_prob = exp(estimate) / (1 + exp(estimate))) |> \n  select(term, pred_prob)\n\nm1_pred_prob\n\n# A tibble: 2 × 2\n  term                        pred_prob\n  <chr>                           <dbl>\n1 Unlikely|Somewhat likely        0.901\n2 Somewhat likely|Very likely     0.987\n\n\n\nRemember, the intercept provides us with the predicted probability of an outcome when all other predictors are set to 0.\n\nSo, the probability that an individual with no parents who went to graduate school, who went to a private school for their undergraduate degree, and who has a GPA of 0 is unlikely to apply to college is 90%. The probability that they are either unlikely to somewhat likely or apply for graduate school is 99%.\n\nNote, we don’t need to estimate the third threshold because this is always equal to 1.\n\nThe predicted probability that an individual is unlikely to apply to college is \\(Pr(Y^* \\leq \\tau_{unlikely})\\), which is equal to 90%.\nThe predicted probability that an individual is somewhat likely to apply to college is the predicted probability that they either are unlikely or somewhat likely to apply minus the probability that they are unlikely to apply. This is \\(Pr(Y^* \\leq \\tau_{somewhatlikely}) - Pr(Y^* \\leq \\tau_{unlikely})\\), which is equal to \\(0.987 - 0.901\\), or 9%.\nFinally, the probability that such an individual is very likely to apply is 1 (or 100%, or certainty) minus the predicted probability that they either are unlikely or somewhat likely to apply. This is \\(Pr(Y^* > \\tau_{somewhatlikely})\\) or \\(1 - Pr(Y^* \\leq \\tau_{somewhatlikely})\\). This is equal to 1%.\n\n\nInterpreting the coefficients\nThe coefficients are the log odds ratio for each predictor. We can access them using broom::tidy():\n\ntidy(m1) |> \n  filter(coef.type == \"coefficient\")\n\n# A tibble: 3 × 5\n  term    estimate std.error statistic coef.type  \n  <chr>      <dbl>     <dbl>     <dbl> <chr>      \n1 pared1    1.05       0.266     3.94  coefficient\n2 public1  -0.0588     0.298    -0.197 coefficient\n3 gpa       0.616      0.261     2.36  coefficient\n\n\nWe can get the more easily interpretable odds ratios using exponentiate = TRUE:\n\ntidy(m1, exponentiate = TRUE) |> \n  filter(coef.type == \"coefficient\")\n\n# A tibble: 3 × 5\n  term    estimate std.error statistic coef.type  \n  <chr>      <dbl>     <dbl>     <dbl> <chr>      \n1 pared1     2.85      0.266     3.94  coefficient\n2 public1    0.943     0.298    -0.197 coefficient\n3 gpa        1.85      0.261     2.36  coefficient\n\n\nUnlike our intercepts, we only have one set of coefficients for each of our predictors. This is because ordered logistic (and probit) models assume that the effect of \\(x_i\\) on the outcome \\(Y^*\\) is the same across each different category \\(J\\). This assumption is called the proportional odds assumption: it implies that the odds ratio for any predictor, \\(x_i\\), is the same for all categories contained in \\(Y\\). No matter where you split \\(Y^*\\), the resulting odds of any value of \\(x_i\\) are proportional.\nThis makes interpretation of our coefficients a little awkward. Let’s power through. An individual with a parent who has attended graduate school (\\(pared = 1\\)) is 2.85 times more likely to be in a higher category of likelihood of applying to graduate school than an individual with no parents who attended graduate school (\\(pared = 0\\)).\nAn individual who attended a public university for their undergraduate degree (\\(public = 1\\)) is 0.94 times as likely to be in a higher category of likelihood of applying to graduate school than an individual who attended a private school.\nAn individual with one point higher in their GPA is 1.85 times as likely to be in a higher category of likelihood of applying to graduate school than an individual with a GPA of one point less.\nAs with binary logistic regression, estimating predicted probabilities depends on the values of our predictors in which we are interested."
  },
  {
    "objectID": "contents/event_intro.html",
    "href": "contents/event_intro.html",
    "title": "Introduction to Duration Models",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "contents/event_intro.html#introduction",
    "href": "contents/event_intro.html#introduction",
    "title": "Introduction to Duration Models",
    "section": "Introduction",
    "text": "Introduction\nOutcome is the time until an event occurs.\n\nAlso known as event history or survival models.\n\nOLS is problematic:\n\nNegative predicted values: can predict negative outcomes, but time (our outcome of interest) cannot be negative.\nNormality: assumes duration times are normally distributed, but long durations are often right skewed.\nCensoring: does not easily distinguish between drop out and the end of the study.\nTime varying covariates: cannot easily accommodation independent variables that change value over time."
  },
  {
    "objectID": "contents/event_intro.html#censoring",
    "href": "contents/event_intro.html#censoring",
    "title": "Introduction to Duration Models",
    "section": "Censoring",
    "text": "Censoring\nRight censoring: the subject participates in the study for a time and is no longer observed.\n\nThe failure event has not yet occurred by the end of the study.\nSubject withdraws. Think of individuals who drop out of a survey. Or political groups that dissolve.\n\nWe know the the subject survived from the start of the study to some time, \\(t\\)."
  },
  {
    "objectID": "contents/event_intro.html#survival-functions",
    "href": "contents/event_intro.html#survival-functions",
    "title": "Introduction to Duration Models",
    "section": "Survival functions",
    "text": "Survival functions\nOur outcome is \\(T\\), a non-negative random variable denoting the time to a failure event.\nWe want to understand the probability that an actor will survive past some time \\(t\\).\nFirst, find the probability that they will survive to time \\(t\\). Then work out the probability that won’t occur:\n\\[\nS(t) = 1 - Pr(T \\leq t)\n\\]\n\\(S(t) = 0\\) at \\(t = 0\\). It declines from there."
  },
  {
    "objectID": "contents/event_intro.html#hazard-function",
    "href": "contents/event_intro.html#hazard-function",
    "title": "Introduction to Duration Models",
    "section": "Hazard function",
    "text": "Hazard function\nDescribes the risk that the failure event will occur. This is the instantaneous rate of failure.\nThis is a conditional idea: what is the risk of failure given survival to time \\(t\\)?\nRanges from 0 (no risk of failure) to infinity (the certainty of failure).\n\\[\nrisk = \\frac{chance that some thing happens}{change that it hasn't happened yet} = \\frac{Pr(failure)}{Pr(survival)}\n\\]\nFor example, duration of peace. Given that a country has been in peace for 14 years, what is the rik that it will be at war next year.\n\nCox proportional hazard model\nWe have a baseline hazard function, \\(h_0(t)\\), which must be positive. We discuss risk relative to this baseline.\n\\[\nh_i(t) = h_0(t)e^{X\\beta}\n\\]\nThis does not assume knowledge about absolute risk: everything is relative. You want to compare the hazard rates of observations that have different covariates. Why did this country adopt this policy before this other country?\nYou can change the values of your covariates to test how the relative risk changes.\n\nLook at slides.\n\nThis means that hazard functions should be strictly parallel. I.e. the shape of the hazard is the same for \\(x=1\\) and \\(x=2\\). Is this the case?\nThe intercept is always 0: the probability of survival at the start of the study is 100%.\n\n\nTime dependence in binary data\nLogit with time dummies or splines (Beck, Katz, and Tucker (1998)).\nTime dummies lead to errors in estimation. Not a lot of variance within time periods, leads to perfect estimation. A problem. If you drop those observations, you are missing what you were trying to study. Carter and Signorino (2010).\nAlso leads to identification issues if you have a lot of time dummies. Carter and Signorino (2010).\nSplines designate similar time periods.\nCan overcome above problem, but need to be defined carefully with theory. Carter and Signorino (2010).\nCarter and Signorino (2010) say: run a logit model, but include variables for \\(t\\), \\(t^2\\), and \\(t^3\\).\n\nWhy does this work?\n\n\n\nThinking about time\nIs it discrete or continuous? Need to be careful."
  }
]