[
  {
    "objectID": "contents/multinomial_intro.html",
    "href": "contents/multinomial_intro.html",
    "title": "Introduction to Multinomial Models",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(nnet)\nlibrary(broom)\nlibrary(gtsummary)\nlibrary(sjPlot)"
  },
  {
    "objectID": "contents/multinomial_intro.html#introduction",
    "href": "contents/multinomial_intro.html#introduction",
    "title": "Introduction to Multinomial Models",
    "section": "Introduction",
    "text": "Introduction\nIn political science, we are often interested in measuring the probability of success for more than two outcomes. For example, we may want to know the probability that an individual will vote for a specific candidate in an election. If our individuals have more than two candidates from which to choose, we can take advantage of multinomial modelling.\nMultinomial models are linear regression models that measure the probability of success across outcomes with more than two options. As with binary outcomes, we can choose between two common approaches to these models: logit and probit regression. Multinomial logit and probit models are an extension of the binary latent variable models we discussed in the previous section. Rather than modelling the choice made between two options - success or failure, vote or not vote - we can efficiently model the choice between many different options, for example: strongly favour, weakly favour, weakly dislike, strongly dislike.\nUnlike previously, the choice between a multinomial logit model and a multinomial probit model is not trivial. This difference centres on the Independence of Irrelevant Alternatives (IIA) assumption. We will discuss this shortly.\nTo explore this question, we will look at students’ level of support for the Iraq war based on their gender and their party affiliation (measured on a seven-point scale).\nLet’s load and clean our data:\n\nstudent_survey <- rio::import(\"/Users/harrietgoers/Downloads/student vote GVPT 729a.dta\") |> \n  transmute(warsup = factor(warsup, \n                            levels = c(1, 2, 3, 4),\n                            labels = c(\"Strongly support\",\n                                       \"Somewhat support\",\n                                       \"Somewhat oppose\",\n                                       \"Strongly oppose\")), \n            female = factor(female, \n                            levels = c(0, 1), \n                            labels = c(\"Not female\", \"Female\")), \n            pid = factor(pid,\n                         levels = c(1, 2, 3, 4, 5, 6, 7),\n                         labels = c(\"Strong democrat\",\n                                    \"Weak democrat\",\n                                    \"Independent democrat\",\n                                    \"Independent\",\n                                    \"Independent republican\",\n                                    \"Weak republican\",\n                                    \"Strong republican\"))) |> \n  labelled::set_variable_labels(warsup = \"Support for the war\",\n                                female = \"Female\",\n                                pid = \"Party ID\") |> \n  drop_na()\n\nhead(student_survey)\n\n            warsup     female                    pid\n1  Somewhat oppose     Female          Weak democrat\n2 Somewhat support     Female      Strong republican\n3 Strongly support Not female            Independent\n4 Strongly support     Female      Strong republican\n5 Strongly support Not female Independent republican\n6 Strongly support     Female      Strong republican\n\n\nLet’s look at our dataset:\n\nskim(student_survey)\n\n\nData summary\n\n\nName\nstudent_survey\n\n\nNumber of rows\n1154\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nwarsup\n0\n1\nFALSE\n4\nStr: 379, Som: 275, Str: 272, Som: 228\n\n\nfemale\n0\n1\nFALSE\n2\nFem: 649, Not: 505\n\n\npid\n0\n1\nFALSE\n7\nStr: 244, Str: 218, Wea: 192, Ind: 164"
  },
  {
    "objectID": "contents/multinomial_intro.html#multinomial-logit-regression",
    "href": "contents/multinomial_intro.html#multinomial-logit-regression",
    "title": "Introduction to Multinomial Models",
    "section": "Multinomial logit regression",
    "text": "Multinomial logit regression\nIn a binomial logistic regression, we measure the probability that our outcome, \\(y\\), will take on one of two options: usually \\(y = 0\\) or \\(y = 1\\). For multinomial logit regression, we model the probability that our outcome, \\(y\\), will take on one of more than two options. You need to select a baseline category from which you will compare your probability of success for all other categories.\n\nA multinomial model is genuinely an extension of binomial logistic regression. We could fit a series of binomial logistic regression models and get the same outcome.1\n\nRecall that a binary logistic regression is modelled as such:\n\\[\nPr(Y = 1 | X) = ln(\\frac{P}{1-P}) = \\beta_0 + \\beta_1X_1 + ... = \\beta_kX_k\n\\]\nThis provides the log odds ratio of success, or \\(Y = 1\\) for a binary outcome.\nFor multinomial logistic regression, our outcome is no longer the log odds of success against failure. Instead, we are measuring the log odds of the probability of one category over the probability of a base category. This is called the relative log odds ratio.\n\\[\nln(\\frac{P_{category 2}}{P_{category 1}})\n\\]\nLet’s explore this by looking at the level of support university students had for the Iraq war. Our outcome variable can take on one of four options: strong opposition, weak opposition, weak support, and strong support for the war. Let’s pick strong support as our baseline category. We now need to calculate three equations that model the relative log odds ratio that an individual will express strong opposition, weak opposition, or weak support for the war relative to the probability that they will express strong support for the war:\n\\[\nPr(y = strong\\_opposition | X) = log(\\frac{Pr(y = strong\\_opposition)}{Pr(y = strong\\_support)})\n\\]\n\\[\nPr(y = weak\\_opposition | X) = log(\\frac{Pr(y = weak\\_opposition)}{Pr(y = strong\\_support)})\n\\]\n\\[\nPr(y = weak\\_support | X) = log(\\frac{Pr(y = weak\\_support)}{Pr(y = strong\\_support)})\n\\]\n\nAssumptions\n\nIndependence of Irrelevant Alternatives assumption\nMultinomial logit assumes that the relative probability of existing choices is not affected by changes to the choice set. For example, if you remove a choice from your model, that choice’s probability of success will be distributed evenly among the remaining choices. Their relative contribution to the probability of success remains the same. This is a very strong assumption.\nFor example, I am choosing between four candidates for the Democratic primary. The probability that I will vote for each is defined by my preferences for their policy platforms. They are as below:\n\\[\nPr(cand\\_1) = \\frac{1}{10}\n\\]\n\\[\nPr(cand\\_2) = \\frac{1}{2}\n\\]\n\\[\nPr(cand\\_3) = \\frac{3}{10}\n\\]\n\\[\nPr(cand\\_4) = \\frac{1}{10}\n\\]\nCandidate 3 drops out of the race. They were a rather centerist candidate among Democrats, so their supporters do not flow to one specific candidate among those left. I follow this general trend and the probability that I would vote for that candidate distributes itself eveningly among the remaining candidates. Each gets a third of Candidate 3’s probability: \\(\\frac{1}{10}\\). The new distribution of the probability that I will vote for the candidates is as follows:\n\\[\nPr(cand\\_1) = \\frac{1}{5}\n\\]\n\\[\nPr(cand\\_2) = \\frac{3}{5}\n\\]\n\\[\nPr(cand\\_4) = \\frac{1}{5}\n\\]\nBecause the probability of success depends on the candidate, I should use a multinomial logistic regression to model my preferences across these choices.\n\n\nCategorical outcome\nLike binary logistic regression, this model assumes that the outcome variable is categorical. This outcome can be either ordered or unordered; however, other models are better at estimating ordered outcomes.\n\n\nThe log-odds of the outcome and independent variable have a linear relationship\nLike binary logistic regression, this model assumes that the relationship between the log odds of the outcome variable and any continous independent variables is linear. In other words, our latent variable, \\(z_i\\), must follow linear conventions (all \\(\\beta\\)s must be linear).\n\n\nIndependent errors\nAll observations should be independent of each other. There should not be any structural clustering. For example, if your data includes measures from the same individual over time, your errors are not independent of each other (they are clustered by individual).\n\n\nNo (nearing) multicolinearity\nThis is a standard warning in linear regression modelling. You should never include perfectly colinear variables in your model. If some of your variables are nearing perfect colinearity, you should exclude some or provide a good theoretical justification for including them in your model.\n\n\n\nThe model\nFormally, we define the probability of success for each choice as:\n\\[\nPr(y_i = m | x_i) = \\frac{e^{x_i\\beta(m)}}{\\sum^{J}_{j = 1}e^{x_i\\beta(j)}}\n\\]\nWhere \\(m\\) is the option you have set to the baseline and \\(j\\) is all other options.\nLet’s visualise the relationship between support for the war and gender:\n\np1 <- student_survey |> \n  filter(warsup == \"Strongly support\" | warsup == \"Somewhat support\") |> \n  ggplot(aes(x = as.numeric(female), y = as.numeric(warsup))) + \n  geom_point() + \n  geom_smooth(method = \"loess\", se = F) + \n  theme_minimal() + \n  labs(title = \"Somewhat support\", \n       x = NULL)\n\np2 <- student_survey |> \n  filter(warsup == \"Strongly support\" | warsup == \"Somewhat oppose\") |> \n  ggplot(aes(x = as.numeric(female), y = as.numeric(warsup))) + \n  geom_point() + \n  geom_smooth(method = \"loess\", se = F) + \n  theme_minimal() + \n  labs(title = \"Somewhat oppose\", \n       y = NULL)\n\np3 <- student_survey |> \n  filter(warsup == \"Strongly support\" | warsup == \"Strongly oppose\") |> \n  ggplot(aes(x = as.numeric(female), y = as.numeric(warsup))) + \n  geom_point() + \n  geom_smooth(method = \"loess\", se = F) + \n  theme_minimal() + \n  labs(title = \"Strongly oppose\", \n       x = NULL,\n       y = NULL)\n\np1 | p2 | p3\n\n\n\n\nAnd for party ID:\n\np1 <- student_survey |> \n  filter(warsup == \"Strongly support\" | warsup == \"Somewhat support\") |> \n  ggplot(aes(x = as.numeric(pid), y = as.numeric(warsup))) + \n  geom_point() + \n  geom_smooth(method = \"loess\", se = F) + \n  theme_minimal() + \n  labs(title = \"Somewhat support\", \n       x = NULL)\n\np2 <- student_survey |> \n  filter(warsup == \"Strongly support\" | warsup == \"Somewhat oppose\") |> \n  ggplot(aes(x = as.numeric(pid), y = as.numeric(warsup))) + \n  geom_point() + \n  geom_smooth(method = \"loess\", se = F) + \n  theme_minimal() + \n  labs(title = \"Somewhat oppose\", \n       y = NULL)\n\np3 <- student_survey |> \n  filter(warsup == \"Strongly support\" | warsup == \"Strongly oppose\") |> \n  ggplot(aes(x = as.numeric(pid), y = as.numeric(warsup))) + \n  geom_point() + \n  geom_smooth(method = \"loess\", se = F) + \n  theme_minimal() + \n  labs(title = \"Strongly oppose\", \n       x = NULL,\n       y = NULL)\n\np1 | p2 | p3\n\n\n\n\nWe can see here that the relationship between these independent variables and an individual’s level of support for the war is different based on the choice at hand. Multinomial modelling allows us to measure this. We are essentially developing a different model for each outcome option: strongly support vs. strong oppose, strongly support vs. weakly oppose, and strongly support vs. weakly support.\nLet’s fit our model using nnet::multinom() and display the results using broom::tidy():\n\nm1 <- multinom(warsup ~ female + pid, data = student_survey)\n\n# weights:  36 (24 variable)\ninitial  value 1599.783693 \niter  10 value 1269.729784\niter  20 value 1238.972501\niter  30 value 1232.894042\nfinal  value 1232.880028 \nconverged\n\ntidy(m1)\n\n# A tibble: 24 × 6\n   y.level          term                      estimate std.error stati…¹ p.value\n   <chr>            <chr>                        <dbl>     <dbl>   <dbl>   <dbl>\n 1 Somewhat support (Intercept)                  0.239     0.427   0.560 5.76e-1\n 2 Somewhat support femaleFemale                 0.655     0.181   3.61  3.06e-4\n 3 Somewhat support pidWeak democrat             0.535     0.540   0.991 3.22e-1\n 4 Somewhat support pidIndependent democrat      0.691     0.578   1.20  2.32e-1\n 5 Somewhat support pidIndependent              -0.284     0.531  -0.535 5.93e-1\n 6 Somewhat support pidIndependent republican   -0.323     0.484  -0.667 5.04e-1\n 7 Somewhat support pidWeak republican          -0.406     0.450  -0.902 3.67e-1\n 8 Somewhat support pidStrong republican        -1.30      0.439  -2.97  3.01e-3\n 9 Somewhat oppose  (Intercept)                  1.70      0.377   4.52  6.30e-6\n10 Somewhat oppose  femaleFemale                 0.468     0.222   2.11  3.49e-2\n# … with 14 more rows, and abbreviated variable name ¹​statistic\n\n\nHere, we can see that the model has produced different coefficients for each of our independent variables for each outcome option. The first column tells us which outcome option we are modelling: weak support, weak opposition, and strong opposition.\nWe can more formally present these results using gtsummary::tbl_regression():\n\ntbl_regression(m1, exponentiate = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    \n      Somewhat support\n    \n    Female\n\n\n\n    Not female\n—\n—\n\n    Female\n1.92\n1.35, 2.75\n<0.001\n    Party ID\n\n\n\n    Strong democrat\n—\n—\n\n    Weak democrat\n1.71\n0.59, 4.92\n0.3\n    Independent democrat\n2.00\n0.64, 6.19\n0.2\n    Independent\n0.75\n0.27, 2.13\n0.6\n    Independent republican\n0.72\n0.28, 1.87\n0.5\n    Weak republican\n0.67\n0.28, 1.61\n0.4\n    Strong republican\n0.27\n0.11, 0.64\n0.003\n    \n      Somewhat oppose\n    \n    Female\n\n\n\n    Not female\n—\n—\n\n    Female\n1.60\n1.03, 2.47\n0.035\n    Party ID\n\n\n\n    Strong democrat\n—\n—\n\n    Weak democrat\n0.86\n0.33, 2.21\n0.7\n    Independent democrat\n0.90\n0.32, 2.51\n0.8\n    Independent\n0.08\n0.03, 0.24\n<0.001\n    Independent republican\n0.11\n0.04, 0.26\n<0.001\n    Weak republican\n0.03\n0.01, 0.08\n<0.001\n    Strong republican\n0.01\n0.00, 0.02\n<0.001\n    \n      Strongly oppose\n    \n    Female\n\n\n\n    Not female\n—\n—\n\n    Female\n1.97\n1.30, 2.98\n0.001\n    Party ID\n\n\n\n    Strong democrat\n—\n—\n\n    Weak democrat\n0.45\n0.18, 1.14\n0.092\n    Independent democrat\n0.67\n0.25, 1.80\n0.4\n    Independent\n0.14\n0.06, 0.34\n<0.001\n    Independent republican\n0.03\n0.01, 0.08\n<0.001\n    Weak republican\n0.01\n0.00, 0.02\n<0.001\n    Strong republican\n0.00\n0.00, 0.01\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nAnd visualise our results using sjPlot::plot_model():\n\nplot_model(m1, show.values = T, value.offset = .3)\n\n\n\n\nWe will discuss interpreting these coefficients in the next section.\n\nTODO: Likelihood ratio test to test for whether these categories are statistically different from each other."
  },
  {
    "objectID": "contents/multinomial_intro.html#multinomial-probit",
    "href": "contents/multinomial_intro.html#multinomial-probit",
    "title": "Introduction to Multinomial Models",
    "section": "Multinomial probit",
    "text": "Multinomial probit\nThe process for defining the probability of success for more than two options remains similar to the multinomial logistic regression except that the multinomial probit model allows the errors across choices to be correlated. This means that our assumption that adding or removing choices does not disturb the balance of probabilities of success between the remaining choices does not need to hold. This makes multinomial probit regression a better choice for exploring choice entry and exit.\n\nA more flexible model\nTo illustrate, let’s consider which type of transport people take to work in the morning. Suppose people can choose between riding a red bus, a train, or driving a car to work. The probabilities are:\n\\[\nPr(red bus) = \\frac{1}{6}\n\\]\n\\[\nPr(train) = \\frac{2}{6}\n\\]\n\\[\nPr(car) = \\frac{3}{6}\n\\]\nTherefore, the odds of taking a red bus to train are \\(\\frac{\\frac{1}{6}}{\\frac{2}{6}} = \\frac{1}{2}\\). Now, let’s add the option of taking a blue bus. Will people split evenly across the blue bus, red bus, train, or car? Unlikely: their transport decision is probably not influenced by the colour of their options. Rather, people will likely be split between the red and blue bus, with people who take the train or drive their car unaffected.\nTherefore, the probabilities change:\n\\[\nPr(red bus) = \\frac{1}{12}\n\\]\n\\[\nPr(blue bus) = \\frac{1}{12}\n\\]\n\\[\nPr(train) = \\frac{2}{6}\n\\]\n\\[\nPr(car) = \\frac{3}{6}\n\\]\nTherefore, the odds of taking a red bus to train are now \\(\\frac{\\frac{1}{12}}{\\frac{2}{6}} \\ne \\frac{1}{2}\\). This violates the IIA assumption. We should not, therefore, use multinomial logit regression to model this relationship."
  },
  {
    "objectID": "contents/count_intro.html",
    "href": "contents/count_intro.html",
    "title": "Introduction to Count Models",
    "section": "",
    "text": "A count dependent model is non-negative, an integer, and often skewed. OLS does not model this well."
  },
  {
    "objectID": "contents/count_intro.html#count-models",
    "href": "contents/count_intro.html#count-models",
    "title": "Introduction to Count Models",
    "section": "Count models",
    "text": "Count models\nCount models follow the poisson distribution.\nCount models are theoretically unbounded.\n\nIf you are measuring a bounded phenomenon and your count model is predicted out of scope results, you may want to use an ordered model instead. This is not a very efficient model for count events: you are throwing away a lot of information. However, you will get more realistic predictions.\n\n\nPoisson Regression Model\nExpected count:\n\\[\n\\mu_i = E(y_i | x_i) = e^{x_i\\beta}\n\\]\n\nPotential problems\nThe model is not very good at predicting 0. We will under-predict 0s.\nThe model can often only predict events that have already taken place. It will not predict the first instance of an event.\nOverdispersion occurs when the conditional mean is less than the conditional variance. This occurs when:\n\nAssumes events are independent. This is a very (and often too) strong assumption.\nThere is heterogeneity: the \\(\\mu\\) differs across cases.\n\nOverdispersion produces estimates that are consistent and inefficient. The standard errors are biased downwards. It is, therefore, easier to find statistical significance.\n\nYou have consistent results, as you add more data you will get closer to the true relationship between the dependent and independent variables.\n\n\n\nInterpretation\nThe coefficients give the change in the log of the expected count for a unit change in \\(x_i\\).\nYou can also estimate the count of your variable of interest.\nYou can also estimate changes in the expected count across a discrete difference in \\(x_i\\).\n\n\n\nNegative binomial models\nThis model accounts for overdispersion. It does this by adding a dispersion parameter, \\(\\alpha\\), that allows for heterogeneity and non-independence across events.\n\nGenerally, you should use this model. There will generally be non-independence or heterogeneity across events.\n\nThe expected count is the same as that produced by a poisson regression model. However, the variance differs. #### Interpretation\nSimilar to Poisson:\n\\[\nE(y | x) = e^{X\\beta}\n\\]\n\n\nZero Inflated Models\nThis model allows you to explore very rare events. There needs to be a theoretical expectation that some unknown types of cases will only produce zeros. It is not sufficient for there to simply be a lot of zeros. It is important that you don’t know why these groups can only produce zero. If you know, you should remove them from your dataset. They are a restricted group.\nThis model explores the probability that you never have a count \\(> 0\\). Estimate a binary model for \\(P(y = 0)\\) and use that to weight your data.\n\n\nExposure\nYou need to think about whether exposure differs across cases. Are events more likely to occur in a subset of your data due to some other factor? For example, number of attacks might be greater simply because a group has been around longer. To solve for this, you control for exposure. For example, add ln(exposure) and constrain the coefficient to 1."
  },
  {
    "objectID": "contents/binary_intro_binary.html",
    "href": "contents/binary_intro_binary.html",
    "title": "Introduction to Binary Response Modelling",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(gtsummary)"
  },
  {
    "objectID": "contents/binary_intro_binary.html#introduction",
    "href": "contents/binary_intro_binary.html#introduction",
    "title": "Introduction to Binary Response Modelling",
    "section": "Introduction",
    "text": "Introduction\nWe often want to better understand binary outcomes in political science.\nLet’s start with simulated data to illustrate the theory. Let’s create some data:\n\ndf <- tibble(x = runif(1000, 0, 10)) |> \n  mutate(y = if_else(x < 5, \n                     sample(0:1, 1000, replace = T, prob = c(0.95, 0.05)),\n                     sample(0:1, 1000, replace = T, prob = c(0.05, 0.95))))\n\nhead(df)\n\n# A tibble: 6 × 2\n      x     y\n  <dbl> <int>\n1  9.59     1\n2  8.87     1\n3  1.65     0\n4  6.55     1\n5  3.36     0\n6  2.04     0\n\n\nNow, let’s plot the relationship between our binary dependent variable, y, and our independent variable of interest, x.\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  theme_minimal()\n\n\n\n\nThere seems to be a pretty clear relationship between y and x. When x is less than 5, you are very likely (in fact 95 percent likely) to get a y of 0. But how do we formally measure this?\nWhen working with binary outcomes, we want to understand the probability that you will get an outcome (here, \\(Y = 0\\) or \\(Y = 1\\)) for any given value of your independent variable(s). From there, you can make an informed guess as to the outcome for given values of \\(X\\). For example, where the predicted probability of success is greater than 50 percent, you can predict that \\(y = 1\\). You can also better understand how changing the value of your independent variable impacts the likelihood that you will get a different outcome.\nIn our research, we may want to better understand how some policy choice will impact the likelihood of many different relevant phenomena, including that individuals will vote, countries will go to war, or democracies will become autocracies."
  },
  {
    "objectID": "contents/binary_intro_binary.html#linear-probability-model",
    "href": "contents/binary_intro_binary.html#linear-probability-model",
    "title": "Introduction to Binary Response Modelling",
    "section": "Linear Probability Model",
    "text": "Linear Probability Model\nLet’s start off simple. Let’s draw a straight line between these two clusters and see what we get.\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nThis is our usual linear model:\n\nm_lpr <- lm(y ~ x, data = df)\n\ntbl_regression(m_lpr, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-0.18\n-0.21, -0.14\n<0.001\n    x\n0.14\n0.13, 0.14\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nOur estimation of \\(y_i\\) can (and often does) take on values other than 0 or 1. This is because we can interpret the coefficients of this model as differences in the probability of success (\\(y = 1\\)). We can see that increasing \\(x\\) by one unit increases the probability that \\(y = 1\\) by 13.5%. We have a very high level of confidence in this, with \\(p<0.001\\).\n\nIssues with LPM\nWe run into difficulties using LPM for prediction. First, our model can predict probabilities of success less than 0 and greater than 1. Second, and relatedly, we lose information treating these discrete outcomes (0 or 1) as continuous."
  },
  {
    "objectID": "contents/binary_intro_binary.html#latent-variable-approach",
    "href": "contents/binary_intro_binary.html#latent-variable-approach",
    "title": "Introduction to Binary Response Modelling",
    "section": "Latent Variable Approach",
    "text": "Latent Variable Approach\nWe can only observe one of two outcomes: \\(y = 0\\) (failure) or \\(y = 1\\) (success). The linear model provided above does not account for this very well. How can we improve this model? The latent variable approach assumes a continuous relationship exists between our observed outcome (\\(Y\\)) and our independent variables (\\(X\\)). This continuous relationship is driven by an unobserved outcome: \\(Z\\).\n\\[\nz_i = X_i\\beta + \\epsilon_i\n\\]\nThis set up is familiar to us: it’s a linear model. Critically, though, we need to understand the shape of that independent error term, \\(\\epsilon_i\\). This defines the shape of the continuous relationship that takes us from \\(y = 0\\) to \\(y = 1\\). We have two common options to pick from: logistic or probit.\n\nLogistic Regression\nThe inverse logistic function suits our needs well. First, it is bounded between outcomes of 0 and 1. Second, it allows for a varying impact of a change in \\(X\\) on the probability that \\(Y = 1\\).\nFormally, the inverse logistic function is:\n\\[\nPr(Y = 1 | X) = logit^{-1}(X) = \\frac{e^X}{1 + e^X}\n\\]\nLet’s look at the shape of the inverse logistic function:\n\ntibble(x = seq(-10, 10, by = 0.5)) |> \n  mutate(y = plogis(x)) |> \n  ggplot(aes(x = x, y = y)) + \n  geom_line() + \n  theme_minimal()\n\n\n\n\n\nThe function plogis() gives you the inverse log of a number. For example, plogis(1) returns 0.7310586.\n\nHere, we have simply calculated and then plotted the inverse logit of all values sitting at 0.5 intervals between -10 and 10.\n\nThe model\nLet’s fit a logistic regression line against our data:\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"glm\", se = F, method.args = list(family = binomial(link = \"logit\"))) + \n  theme_minimal()\n\n\n\n\nOur model maps the relationship between our outcome (\\(y\\)) and our independent variable (\\(x\\)). We can interpret it as mapping the probability that \\(y = 1\\) for a given value of \\(x\\), otherwise written as \\(Pr(y = 1|x)\\).\nWe can fit this model as such:\n\nm_lr <- glm(y ~ x, data = df, family = binomial(link = \"logit\"))\n\ntbl_regression(m_lr, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      log(OR)1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-5.6\n-6.4, -5.0\n<0.001\n    x\n1.1\n1.0, 1.3\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nWe will explore how to interpret these coefficients and uncertainty in the next section.\n\n\n\nProbit Regression\nAn alternative approach is probit regression. This model is also bounded between outcomes of 0 and 1 and allows for a varying impact of a change in \\(x\\) on \\(y\\). The only real difference between the logistic and probit regression models are the ways they model the error term, \\(\\epsilon_i\\), in our latent variable \\(z_i\\). Probit replaces the logistic distribution with the normal distribution. This changes how we can interpret the coefficients on our independent variables. We will discuss that in the next section.\nFormally, the probit model is:\n\\[\nPr(Y = 1|X) = \\Phi(X\\beta)\n\\]\nLet’s look at the shape of the probit function:\n\ntibble(x = seq(-10, 10, by = 0.5)) |> \n  mutate(y = pnorm(x)) |> \n  ggplot(aes(x = x, y = y)) + \n  geom_line() + \n  theme_minimal()\n\n\n\n\n\nThe function pnorm() gives you the corresponding value for the normal cumulative distribution function. For example, pnorm(1.96) returns 0.9750021 (think confidence intervals!).\n\n\nThe model\nLet’s fit a probit regression line against our data:\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"glm\", se = F, method.args = list(family = binomial(link = \"logit\"))) + \n  theme_minimal()\n\n\n\n\nQuickly, let’s compare this probit regression (in red) to our logistic regression (in blue):\n\n\n\n\n\nThey both fit very similar models; however, the logistic regression produces fatter tails.\nOur probit model maps the relationship between our outcome (\\(y\\)) and our independent variable (\\(x\\)). Like the logistic regression, we can interpret it as mapping the probability that \\(y = 1\\) for a given value of \\(x\\), otherwise written as \\(Pr(y = 1|x)\\).\nWe can fit this model as such:\n\nm_pr <- glm(y ~ x, data = df, family = binomial(link = \"probit\"))\n\ntbl_regression(m_pr, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-2.8\n-3.1, -2.5\n<0.001\n    x\n0.56\n0.51, 0.62\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nWe will explore how to interpret these coefficients and uncertainty in the next section."
  },
  {
    "objectID": "contents/multinomial_marginal_effects.html",
    "href": "contents/multinomial_marginal_effects.html",
    "title": "Measuring Marginal Effects on Multiple Outcomes",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(nnet)\nlibrary(broom)\nlibrary(marginaleffects)\nlibrary(gtsummary)\nlibrary(ggeffects)\n\nWe will use the same data as the previous section:\n\nstudent_survey <- rio::import(\"/Users/harrietgoers/Downloads/student vote GVPT 729a.dta\") |> \n  transmute(warsup = factor(warsup, \n                            levels = c(1, 2, 3, 4),\n                            labels = c(\"Strongly support\",\n                                       \"Somewhat support\",\n                                       \"Somewhat oppose\",\n                                       \"Strongly oppose\")), \n            female = factor(female, \n                            levels = c(0, 1), \n                            labels = c(\"Not female\", \"Female\")), \n            pid = factor(pid,\n                         levels = c(1, 2, 3, 4, 5, 6, 7),\n                         labels = c(\"Strong democrat\",\n                                    \"Weak democrat\",\n                                    \"Independent democrat\",\n                                    \"Independent\",\n                                    \"Independent republican\",\n                                    \"Weak republican\",\n                                    \"Strong republican\"))) |> \n  labelled::set_variable_labels(warsup = \"Support for the war\",\n                                female = \"Female\",\n                                pid = \"Party ID\") |> \n  drop_na()\n\nhead(student_survey)\n\n            warsup     female                    pid\n1  Somewhat oppose     Female          Weak democrat\n2 Somewhat support     Female      Strong republican\n3 Strongly support Not female            Independent\n4 Strongly support     Female      Strong republican\n5 Strongly support Not female Independent republican\n6 Strongly support     Female      Strong republican"
  },
  {
    "objectID": "contents/multinomial_marginal_effects.html#multinomial-logit-regression",
    "href": "contents/multinomial_marginal_effects.html#multinomial-logit-regression",
    "title": "Measuring Marginal Effects on Multiple Outcomes",
    "section": "Multinomial logit regression",
    "text": "Multinomial logit regression\n\nInterpreting the coefficients\nLet’s take another look at the model we created previously:\n\nm1 <- multinom(warsup ~ female + pid, data = student_survey)\n\n# weights:  36 (24 variable)\ninitial  value 1599.783693 \niter  10 value 1269.729784\niter  20 value 1238.972501\niter  30 value 1232.894042\nfinal  value 1232.880028 \nconverged\n\ntbl_regression(m1, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      log(OR)1\n      95% CI1\n      p-value\n    \n  \n  \n    \n      Somewhat support\n    \n    (Intercept)\n0.24\n-0.60, 1.1\n0.6\n    Female\n\n\n\n    Not female\n—\n—\n\n    Female\n0.65\n0.30, 1.0\n<0.001\n    Party ID\n\n\n\n    Strong democrat\n—\n—\n\n    Weak democrat\n0.53\n-0.52, 1.6\n0.3\n    Independent democrat\n0.69\n-0.44, 1.8\n0.2\n    Independent\n-0.28\n-1.3, 0.76\n0.6\n    Independent republican\n-0.32\n-1.3, 0.63\n0.5\n    Weak republican\n-0.41\n-1.3, 0.48\n0.4\n    Strong republican\n-1.3\n-2.2, -0.44\n0.003\n    \n      Somewhat oppose\n    \n    (Intercept)\n1.7\n1.0, 2.4\n<0.001\n    Female\n\n\n\n    Not female\n—\n—\n\n    Female\n0.47\n0.03, 0.90\n0.035\n    Party ID\n\n\n\n    Strong democrat\n—\n—\n\n    Weak democrat\n-0.15\n-1.1, 0.79\n0.7\n    Independent democrat\n-0.11\n-1.1, 0.92\n0.8\n    Independent\n-2.5\n-3.6, -1.4\n<0.001\n    Independent republican\n-2.2\n-3.1, -1.3\n<0.001\n    Weak republican\n-3.4\n-4.3, -2.5\n<0.001\n    Strong republican\n-5.3\n-6.4, -4.1\n<0.001\n    \n      Strongly oppose\n    \n    (Intercept)\n2.4\n1.7, 3.1\n<0.001\n    Female\n\n\n\n    Not female\n—\n—\n\n    Female\n0.68\n0.26, 1.1\n0.001\n    Party ID\n\n\n\n    Strong democrat\n—\n—\n\n    Weak democrat\n-0.79\n-1.7, 0.13\n0.092\n    Independent democrat\n-0.40\n-1.4, 0.59\n0.4\n    Independent\n-2.0\n-2.9, -1.1\n<0.001\n    Independent republican\n-3.4\n-4.3, -2.5\n<0.001\n    Weak republican\n-4.7\n-5.6, -3.7\n<0.001\n    Strong republican\n-6.1\n-7.2, -5.0\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nWe have three options for interpreting coefficients in multinomial logistic regression.\n\nRelative log odds\nSimilar to log odds ratios in binary logistic regression, these can be interpreted as the effect of a one-unit change in \\(x_i\\) on the log odds of being in category 2 compared to being in category 1.\nFor example, identifying as female is associated with a 0.676 change in the log odds of strongly opposing compared to strongly supporting the war in Iraq.\nThis is the default output for our model:\n\ntidy(m1)\n\n# A tibble: 24 × 6\n   y.level          term                      estimate std.error stati…¹ p.value\n   <chr>            <chr>                        <dbl>     <dbl>   <dbl>   <dbl>\n 1 Somewhat support (Intercept)                  0.239     0.427   0.560 5.76e-1\n 2 Somewhat support femaleFemale                 0.655     0.181   3.61  3.06e-4\n 3 Somewhat support pidWeak democrat             0.535     0.540   0.991 3.22e-1\n 4 Somewhat support pidIndependent democrat      0.691     0.578   1.20  2.32e-1\n 5 Somewhat support pidIndependent              -0.284     0.531  -0.535 5.93e-1\n 6 Somewhat support pidIndependent republican   -0.323     0.484  -0.667 5.04e-1\n 7 Somewhat support pidWeak republican          -0.406     0.450  -0.902 3.67e-1\n 8 Somewhat support pidStrong republican        -1.30      0.439  -2.97  3.01e-3\n 9 Somewhat oppose  (Intercept)                  1.70      0.377   4.52  6.30e-6\n10 Somewhat oppose  femaleFemale                 0.468     0.222   2.11  3.49e-2\n# … with 14 more rows, and abbreviated variable name ¹​statistic\n\n\n\n\nRelative risk ratios\nSimilar to odds ratios in binary logistic regression, these can be interpreted as the effect of a one-unit change in \\(x_i\\) on the probability of being in category 2 compared to being in category 1.\nFor example, identifying as female is associated with a 197% change in the probability of strongly opposing compared to strongly supporting the war in Iraq.\nYou can get this using broom::tidy():\n\ntidy(m1, exponentiate = T)\n\n# A tibble: 24 × 6\n   y.level          term                      estimate std.error stati…¹ p.value\n   <chr>            <chr>                        <dbl>     <dbl>   <dbl>   <dbl>\n 1 Somewhat support (Intercept)                  1.27      0.427   0.560 5.76e-1\n 2 Somewhat support femaleFemale                 1.92      0.181   3.61  3.06e-4\n 3 Somewhat support pidWeak democrat             1.71      0.540   0.991 3.22e-1\n 4 Somewhat support pidIndependent democrat      2.00      0.578   1.20  2.32e-1\n 5 Somewhat support pidIndependent               0.753     0.531  -0.535 5.93e-1\n 6 Somewhat support pidIndependent republican    0.724     0.484  -0.667 5.04e-1\n 7 Somewhat support pidWeak republican           0.666     0.450  -0.902 3.67e-1\n 8 Somewhat support pidStrong republican         0.272     0.439  -2.97  3.01e-3\n 9 Somewhat oppose  (Intercept)                  5.49      0.377   4.52  6.30e-6\n10 Somewhat oppose  femaleFemale                 1.60      0.222   2.11  3.49e-2\n# … with 14 more rows, and abbreviated variable name ¹​statistic\n\n\n\n\nPredicted probabilities\nThis describes the probability that an observation will be in a category for a given a set of observed values.\nFor example, the predicted probability that a strong democratic female will strongly support the war in Iraq is 3%. The predicted probability that she will strongly oppose the war is 64%.\nbroom::augment() does not currently support multinomial regression. Instead, we can use base R’s predict():\n\npredict(m1, newdata = tibble(female = factor(\"Female\"), pid = factor(\"Strong democrat\")), type = \"probs\")\n\nStrongly support Somewhat support  Somewhat oppose  Strongly oppose \n      0.02910128       0.07112958       0.25530009       0.64446905 \n\n\nWe can plot these predicted probabilities across our outcomes and variable options using ggeffects::ggeffect():\n\nggeffect(m1, terms = \"female\") |> \n  plot()\n\n\n\n\n\nggeffect(m1, terms = \"pid\") |> \n  plot()"
  },
  {
    "objectID": "contents/multinomial_marginal_effects.html#marginal-effects-in-multinomial-logistic-regression",
    "href": "contents/multinomial_marginal_effects.html#marginal-effects-in-multinomial-logistic-regression",
    "title": "Measuring Marginal Effects on Multiple Outcomes",
    "section": "Marginal effects in multinomial logistic regression",
    "text": "Marginal effects in multinomial logistic regression\nWe can interpret the marginal effects of a one-unit change in \\(x_i\\) on the change in probability that an observation will fall into one category.\nFor example, moving from a strong republican to a strong democrat decreases the probability that an individual will strongly support the war in Iraq by 59%.\nYou can calculate the marginal effects of each change using marginaleffects::marginaleffects():\n\nmarginaleffects::marginaleffects(m1, variables = \"pid\", type = \"probs\") |> \n  summary()\n\n              Group Term                                 Contrast    Effect\n1  Strongly support  pid          Weak democrat - Strong democrat  0.019132\n2  Strongly support  pid   Independent democrat - Strong democrat  0.007698\n3  Strongly support  pid            Independent - Strong democrat  0.154985\n4  Strongly support  pid Independent republican - Strong democrat  0.238355\n5  Strongly support  pid        Weak republican - Strong democrat  0.344832\n6  Strongly support  pid      Strong republican - Strong democrat  0.593819\n7  Somewhat support  pid          Weak democrat - Strong democrat  0.107264\n8  Somewhat support  pid   Independent democrat - Strong democrat  0.096196\n9  Somewhat support  pid            Independent - Strong democrat  0.191194\n10 Somewhat support  pid Independent republican - Strong democrat  0.293315\n11 Somewhat support  pid        Weak republican - Strong democrat  0.396350\n12 Somewhat support  pid      Strong republican - Strong democrat  0.251273\n13  Somewhat oppose  pid          Weak democrat - Strong democrat  0.074992\n14  Somewhat oppose  pid   Independent democrat - Strong democrat  0.020728\n15  Somewhat oppose  pid            Independent - Strong democrat -0.158975\n16  Somewhat oppose  pid Independent republican - Strong democrat -0.061454\n17  Somewhat oppose  pid        Weak republican - Strong democrat -0.178737\n18  Somewhat oppose  pid      Strong republican - Strong democrat -0.246827\n19  Strongly oppose  pid          Weak democrat - Strong democrat -0.201387\n20  Strongly oppose  pid   Independent democrat - Strong democrat -0.124623\n21  Strongly oppose  pid            Independent - Strong democrat -0.187204\n22  Strongly oppose  pid Independent republican - Strong democrat -0.470216\n23  Strongly oppose  pid        Weak republican - Strong democrat -0.562446\n24  Strongly oppose  pid      Strong republican - Strong democrat -0.598266\n   Std. Error  z value   Pr(>|z|)    2.5 %   97.5 %\n1     0.02135   0.8962 0.37012019 -0.02271  0.06097\n2     0.02072   0.3716 0.71021798 -0.03291  0.04831\n3     0.04523   3.4263 0.00061186  0.06633  0.24364\n4     0.04567   5.2186 1.8032e-07  0.14883  0.32788\n5     0.04077   8.4572 < 2.22e-16  0.26492  0.42475\n6     0.03485  17.0373 < 2.22e-16  0.52551  0.66213\n7     0.03189   3.3634 0.00076985  0.04476  0.16977\n8     0.03338   2.8822 0.00394892  0.03078  0.16161\n9     0.05145   3.7159 0.00020246  0.09035  0.29204\n10    0.05066   5.7898 7.0456e-09  0.19402  0.39261\n11    0.04314   9.1868 < 2.22e-16  0.31179  0.48091\n12    0.03539   7.0998 1.2491e-12  0.18191  0.32064\n13    0.04467   1.6787 0.09320863 -0.01256  0.16255\n14    0.04576   0.4530 0.65057172 -0.06896  0.11042\n15    0.04522  -3.5155 0.00043893 -0.24761 -0.07034\n16    0.04979  -1.2343 0.21708356 -0.15904  0.03613\n17    0.03698  -4.8329 1.3458e-06 -0.25122 -0.10625\n18    0.03050  -8.0934 5.8016e-16 -0.30660 -0.18705\n19    0.04730  -4.2573 2.0690e-05 -0.29410 -0.10867\n20    0.05025  -2.4799 0.01314367 -0.22312 -0.02613\n21    0.06335  -2.9552 0.00312464 -0.31136 -0.06305\n22    0.04776  -9.8444 < 2.22e-16 -0.56383 -0.37660\n23    0.03671 -15.3225 < 2.22e-16 -0.63439 -0.49050\n24    0.03298 -18.1424 < 2.22e-16 -0.66290 -0.53363\n\nModel type:  multinom \nPrediction type:  probs"
  },
  {
    "objectID": "contents/binary_substantive_effects.html",
    "href": "contents/binary_substantive_effects.html",
    "title": "Measuring Substantive Effects on Binary Outcomes",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(gtsummary)\nlibrary(skimr)\nlibrary(mvtnorm)\n\nWe will use the dataset we set up in the previous section:\n\n\n   vote close edu7cat       homeown\n1 Voted    10       6     Homeowner\n2 Voted    29       6     Homeowner\n3 Voted    28       4     Homeowner\n4 Voted     0       5     Homeowner\n5 Voted    25       7 Not homeowner\n6 Voted    25       5 Not homeowner"
  },
  {
    "objectID": "contents/binary_substantive_effects.html#introduction",
    "href": "contents/binary_substantive_effects.html#introduction",
    "title": "Measuring Substantive Effects on Binary Outcomes",
    "section": "Introduction",
    "text": "Introduction\nPreviously, we explored the effect of a given value of \\(x_i\\) on our expected probability. We are rarely interested in this effect in political science. Rather, we are interested in the effect of a meaningful change in \\(x_i\\) on our expected probability. For example, what is the effect of changing the date of voting registration from 20 days prior to election day to election day? This section will outline how we calculate this substantive effect for binary outcomes.\nFor non-linear models, we need to specify our values carefully. Unlike linear models, the effect of a one unit change in \\(x\\) on \\(y\\) is not constant. To illustrate, let’s look at the effect on the probability of success of moving from \\(x = 2\\) to \\(x = 3\\) compared to the effect of moving from \\(x = 4\\) to \\(x = 5\\):\n\n\n\n\n\nMoving from \\(x = 2\\) (highlighted in light blue) to \\(x = 3\\) (highlighted in dark blue) increases the probability of success by 0.08, from 0.053 to 0.134. Moving the same interval of one unit from \\(x = 4\\) (highlighted in pink) to \\(x = 5\\) (highlighted in red) increases the probability of success by 0.24, from 0.297 to 0.537. That’s a 2.984 times increase in the effect of a one unit change in \\(x\\).\nTherefore, to measure the effect of moving from one value of \\(x\\) to another in a non-linear model, we need to know which values of \\(x\\) we are moving between. This should be theoretically driven: what is an interesting interval for the phenomenon you are measuring?"
  },
  {
    "objectID": "contents/binary_substantive_effects.html#measuring-substantive-effects",
    "href": "contents/binary_substantive_effects.html#measuring-substantive-effects",
    "title": "Measuring Substantive Effects on Binary Outcomes",
    "section": "Measuring substantive effects",
    "text": "Measuring substantive effects\nHow do we actually measure the effect of a change from \\(x_{i1}\\) to \\(x_{i2}\\)? We predict the estimated probability of success at \\(x_{i1}\\) and at \\(x_{i2}\\) and subtract those probabilities from one another to get the difference. Simple, right? However, there are two factors that complicate this process. First, we need to deal with the other variables in our model: what values should they be held at while we change our variable of interest, \\(x_i\\)? Second, how do we measure uncertainty surrounding this estimated effect? We will deal with these challenges in turn.\n\nWhat to do with the other independent variables\nThere are two dominant approaches to solving this challenge: the average case approach and the observed value approach.\n\nAverage case approach\nThis approach sets all other values to their mean (for continuous variables) or mode (for discrete variables).\n\nFind the mean or mode for all independent variables other than your variable of interest.\nFind your predicted probability of success with your first value of \\(x_i\\), holding all other variables at their mean or mode.\nFind your predicted probability of success with your second value of \\(x_i\\), holding all other variables at their mean or mode.\nCalculate the difference between these predicted probabilities.\nDiscuss the substantive significance of this difference.\n\nTo illustrate, let’s explore the predicted effect of changing a state’s registration voting date from 20 days prior to election day (\\(close = 20\\)) to election day (\\(close = 0\\)).\nFirst, let’s fit a logistic regression as we did in the previous section:\n\nm1 <- glm(vote ~ close + edu7cat + homeown, data = voters, family = binomial(link = \"logit\"))\n\ntbl_regression(m1, intercept = T, exponentiate = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.45\n0.11, 1.56\n0.2\n    Registration closing\n0.99\n0.98, 1.00\n0.2\n    Education level\n\n\n\n    1\n—\n—\n\n    2\n1.59\n0.43, 6.60\n0.5\n    3\n1.24\n0.35, 4.96\n0.7\n    4\n1.66\n0.48, 6.54\n0.4\n    5\n3.46\n1.00, 13.6\n0.055\n    6\n6.86\n1.95, 27.4\n0.003\n    7\n9.13\n2.49, 38.0\n0.001\n    Homeownership\n\n\n\n    Not homeowner\n—\n—\n\n    Homeowner\n2.28\n1.85, 2.83\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nNow, we create a new dataset that contains the values for close we want to test and sets the other independent variables to their mean or mode values:\n\nnew_data <- tibble(\n  close = c(0, 20),\n  edu7cat = voters |> count(edu7cat) |> filter(n == max(n)) |> pull(edu7cat),\n  homeown = voters |> count(homeown) |> filter(n == max(n)) |> pull(homeown)\n)\n\nnew_data\n\n# A tibble: 2 × 3\n  close edu7cat homeown  \n  <dbl> <fct>   <fct>    \n1     0 4       Homeowner\n2    20 4       Homeowner\n\n\nNext, we calculate the predicted probability that an individual will vote, given these values for our independent variables:\n\nresult <- augment(m1, newdata = new_data, type.predict = \"response\")\nresult\n\n# A tibble: 2 × 4\n  close edu7cat homeown   .fitted\n  <dbl> <fct>   <fct>       <dbl>\n1     0 4       Homeowner   0.629\n2    20 4       Homeowner   0.601\n\n\nNext, we calculate the difference between these predicted probabilities, noting that the only thing that changed in our model is the value of close:\n\nresult <- mutate(result, diff = (.fitted - lead(.fitted)) * 100)\nresult\n\n# A tibble: 2 × 5\n  close edu7cat homeown   .fitted  diff\n  <dbl> <fct>   <fct>       <dbl> <dbl>\n1     0 4       Homeowner   0.629  2.77\n2    20 4       Homeowner   0.601 NA   \n\n\nWe expect that the probability that an individual will vote increases 2.77 percentage points when a state’s closing date for voter registration moves from 20 days prior to election day to election day. If increasing turnout by 2.77 percentage points would swing an election in a candidate’s favour, this is a substantively significant result.1\nHowever, we have a bit of a problem. Is this really a generalisable result? Haven’t we just estimated the effect of this change in registration day closure for a homeowner with an education level of 4? This is the problem with the average case approach: we are discarding an enormous amount of rich data from our sample which is potentially compromising the generalisability of our estimates. In fact, sometimes our average case isn’t even in our dataset, which means that we are making out-of-sample predictions.\n\n\nObserved value approach\nThe observed value approach addresses this issue. It sets all other independent variables to their observed values, only aggregating the estimated effect at the end.\n\nFind your predicted probability of success with your first value of \\(x_i\\), holding all other variables at their observed values. You will get the same number of predictions as you have observations.\nFind your predicted probability of success with your second value of \\(x_i\\), holding all other variables at their observed values.\nCalculate the average predicted probability for each of these values of \\(x_i\\).\nCalculate the difference between these averages.\nDiscuss the substantive significance of this difference.\n\nLet’s explore the same question as above to illustrate.\nFirst, find the predicted probability of an individual voting when \\(close = 20\\):\n\nresult_20 <- augment(m1, newdata = mutate(voters, close = 20), type.predict = \"response\")\nresult_20\n\n# A tibble: 2,188 × 5\n   vote         close edu7cat homeown       .fitted\n   <fct>        <dbl> <fct>   <fct>           <dbl>\n 1 Voted           20 6       Homeowner       0.861\n 2 Voted           20 6       Homeowner       0.861\n 3 Voted           20 4       Homeowner       0.601\n 4 Voted           20 5       Homeowner       0.758\n 5 Voted           20 7       Not homeowner   0.784\n 6 Voted           20 5       Not homeowner   0.579\n 7 Voted           20 7       Homeowner       0.892\n 8 Voted           20 6       Homeowner       0.861\n 9 Voted           20 2       Homeowner       0.590\n10 Did not vote    20 5       Homeowner       0.758\n# … with 2,178 more rows\n\n\nNext, find the predicted probability of an individual voting when \\(close = 0\\):\n\nresult_0 <- augment(m1, newdata = mutate(voters, close = 0), type.predict = \"response\")\nresult_0\n\n# A tibble: 2,188 × 5\n   vote         close edu7cat homeown       .fitted\n   <fct>        <dbl> <fct>   <fct>           <dbl>\n 1 Voted            0 6       Homeowner       0.875\n 2 Voted            0 6       Homeowner       0.875\n 3 Voted            0 4       Homeowner       0.629\n 4 Voted            0 5       Homeowner       0.779\n 5 Voted            0 7       Not homeowner   0.803\n 6 Voted            0 5       Not homeowner   0.607\n 7 Voted            0 7       Homeowner       0.903\n 8 Voted            0 6       Homeowner       0.875\n 9 Voted            0 2       Homeowner       0.618\n10 Did not vote     0 5       Homeowner       0.779\n# … with 2,178 more rows\n\n\nNext, calculate the average predicted probability for \\(close = 20\\) and \\(close = 0\\):\n\nresult <- result_0 |> \n  bind_rows(result_20) |> \n  group_by(close) |> \n  summarise(.fitted = mean(.fitted)) |> \n  mutate(diff = (.fitted - lead(.fitted)) * 100)\n\nresult\n\n# A tibble: 2 × 3\n  close .fitted  diff\n  <dbl>   <dbl> <dbl>\n1     0   0.688  2.28\n2    20   0.666 NA   \n\n\nWe expect that the probability that an individual will vote increases 2.28 percentage points when a state’s closing date for voter registration moves from 20 days prior to election day to election day. If this increase in turnout would swing an election in a candidate’s favour, this is a substantively significant result.2\n\n\nWhich approach should you use?\nYou should use the observed values approach. Hanmer and Kalkan (2013) demonstrate using simulated data that the observed values approach consistently produces estimates closer to the population’s true probability than the average case approach. This makes sense: you are using more data to produce your estimated effects.\n\n\n\nMeasuring uncertainty when calculating substantive effects\nWe calculated these estimates using a model that includes error. We need to understand how this uncertainty impacts our estimated substantive effects. How do we get confidence intervals around our estimated effects?\nFirst, we need to go back to the fundamentals of our logistic or probit regression models. Remember from the first section that the latent variable approach assumes there is some continuous process that gets us from \\(y = 0\\) to \\(y = 1\\) based on some set of independent variables \\(X\\). This latent variable, \\(z_i\\), is modelled as:\n\\[\nz_i = \\beta_0 + X_i\\beta_i + \\epsilon_i\n\\]\nIt contains an error term: \\(\\epsilon_i\\). What that error looks like depends on whether you use a logistic or probit regression, but they are very similar. That error term applies to both \\(X_i\\) and \\(\\beta_i\\). So, we need to generate a series of estimates for both the \\(X_i\\) and \\(\\beta_i\\) to generate our confidence intervals.\nImagine we are trying to estimate the relationship between some binary outcome \\(y\\) and some independent variables \\(x\\) and \\(z\\). We take a representative sample from our population and fit a model against that data. If we were to take a different representative sample from our population and fit the same model to that data, we will probably get slightly different estimates for our \\(\\beta\\)s. This is because of the random error inherent in observational modelling. The idea behind logistic and probit regression is that if we were to do this many, many, many times (say, take 1,000 different representative samples from our population and use that data to fit 1,000 models) we would get a set of \\(\\beta\\) estimates that follow a normal (if you’re using a probit model) or inverse logistic (if you’re using a logistic model) distribution. Your original estimated \\(\\beta\\) will be within this distribution.\nWe can use this assumption to generate our confidence intervals. We just need to simulate fitting these 1,000 different models.\n\nIt is critical that your sample is representative of your population. We are not simulating drawing 1,000 different samples. Rather, we are taking our one model fitted against our one sample and drawing estimates around those \\(\\beta\\)s.\n\n\nSimulate fitting 1,000 different models by drawing 1,000 different \\(\\beta_i\\) around your estimated \\(\\beta_i\\) following your (logistic or probit) model’s distribution.\nPredict the probability of success for \\(x_{i1}\\) and \\(x_{i2}\\) using these 1,000 different model estimates.\nCalculate the difference between those predictions.\nCalculate the lower and upper confidence intervals and the mean of those differences.\n\nLet’s illustrate this by looking at our question above.\n\nLogistic regression\nRecall our logistic regression model from above:\n\ntbl_regression(m1, intercept = T, exponentiate = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.45\n0.11, 1.56\n0.2\n    Registration closing\n0.99\n0.98, 1.00\n0.2\n    Education level\n\n\n\n    1\n—\n—\n\n    2\n1.59\n0.43, 6.60\n0.5\n    3\n1.24\n0.35, 4.96\n0.7\n    4\n1.66\n0.48, 6.54\n0.4\n    5\n3.46\n1.00, 13.6\n0.055\n    6\n6.86\n1.95, 27.4\n0.003\n    7\n9.13\n2.49, 38.0\n0.001\n    Homeownership\n\n\n\n    Not homeowner\n—\n—\n\n    Homeowner\n2.28\n1.85, 2.83\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nLet’s collect those coefficients using broom::tidy():\n\ncoefs <- tidy(m1) |> pull(estimate)\ncoefs\n\n[1] -0.80698206 -0.00585002  0.46146385  0.21142686  0.50831506  1.24130230\n[7]  1.92500744  2.21140696  0.82613712\n\n\nNext, simulate fitting 1,000 different models using these estimates as our center-point using mvtnorm::rmvnorm():\n\nTODO: Learn more about the sigma.\n\n\ncoefs_sim <- rmvnorm(n = 1000, mean = coefs, sigma = vcov(m1)) |> \n  as_tibble() |> \n  set_names(tidy(m1) |> mutate(term = paste0(term, \"_beta\")) |> pull(term))\n\nhead(coefs_sim)\n\n# A tibble: 6 × 9\n  (Intercept)…¹ close_…² edu7c…³ edu7c…⁴ edu7c…⁵ edu7c…⁶ edu7c…⁷ edu7c…⁸ homeo…⁹\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1        -1.31  -5.13e-3   1.10    0.639   1.05    1.49     2.67    3.01   0.849\n2        -1.57  -4.99e-3   1.50    1.19    1.42    2.07     2.85    3.54   0.742\n3        -0.723 -7.65e-3   0.688   0.354   0.555   1.10     1.81    2.44   0.727\n4        -0.720 -6.67e-4   0.250  -0.152   0.251   0.976    1.52    2.13   0.979\n5        -1.36  -8.41e-3   1.37    0.798   1.04    1.91     2.57    2.62   0.852\n6        -1.28  -8.29e-3   1.21    0.749   1.17    1.99     2.52    2.79   0.616\n# … with abbreviated variable names ¹​`(Intercept)_beta`, ²​close_beta,\n#   ³​edu7cat2_beta, ⁴​edu7cat3_beta, ⁵​edu7cat4_beta, ⁶​edu7cat5_beta,\n#   ⁷​edu7cat6_beta, ⁸​edu7cat7_beta, ⁹​homeownHomeowner_beta\n\n\nLet’s look at the distribution of these simulated \\(\\beta\\)s for one of our variables: close:\n\nggplot(coefs_sim, aes(x = close_beta)) + \n  geom_histogram() + \n  geom_vline(xintercept = tidy(m1) |> filter(term == \"close\") |> pull(estimate)) + \n  theme_minimal()\n\n\n\n\nAs expected, this draw follows the inverse logit distribution and centered around our estimated \\(\\beta_{close}\\) (highlighted by the black line).\nNext, we need to predict the probability that an individual will vote when \\(close = 20\\) and when \\(close = 0\\) using these 1,000 different model estimates. To do this, we need to convert our categorical variables into dummy variables so that we can fit the correct \\(\\beta\\)s to them:\n\ntrans_data <- voters |> \n  transmute(\n    edu7cat2 = if_else(edu7cat == 2, 1, 0),\n    edu7cat3 = if_else(edu7cat == 3, 1, 0),\n    edu7cat4 = if_else(edu7cat == 4, 1, 0),\n    edu7cat5 = if_else(edu7cat == 5, 1, 0),\n    edu7cat6 = if_else(edu7cat == 6, 1, 0),\n    edu7cat7 = if_else(edu7cat == 7, 1, 0),\n    homeown = as.numeric(homeown) - 1\n  )\n\nhead(trans_data)\n\n  edu7cat2 edu7cat3 edu7cat4 edu7cat5 edu7cat6 edu7cat7 homeown\n1        0        0        0        0        1        0       1\n2        0        0        0        0        1        0       1\n3        0        0        1        0        0        0       1\n4        0        0        0        1        0        0       1\n5        0        0        0        0        0        1       0\n6        0        0        0        1        0        0       0\n\n\nNow, we include our close variable, set to 0 and 20 for each of these 2,188 observations. We should get a dataset of length 4,376: one set of observations for \\(close = 20\\) and one set for \\(close = 0\\).\n\nnew_data <- trans_data |> \n  mutate(close = 0) |> \n  bind_rows(mutate(trans_data, close = 20)) |> \n  group_by(close) |> \n  mutate(id = row_number()) |> \n  ungroup()\n\nnrow(new_data)\n\n[1] 4376\n\n\nNext, we need to join our datasets together, so we can calculate our predicted probability for each observation for each simulated model coefficient. We should get a dataframe with a length of 2 x 1,000 x 2,188 (number of different variables of interest x number of models x number of observations).\n\nsim_data <- coefs_sim |> \n  mutate(sim_round = row_number()) |> \n  full_join(new_data, by = character())\n\nnrow(sim_data)\n\n[1] 4376000\n\n\nWe can now estimate our logistic regression model using the 1,000 different estimated \\(\\beta\\)s for all 2,188 different observations. You need to first fit the linear model, then find the inverse logit of those results using plogis().\n\nresults <- sim_data |> \n  mutate(\n    .fitted = `(Intercept)_beta` +\n      close_beta * close +\n      edu7cat2_beta * edu7cat2 +\n      edu7cat3_beta * edu7cat3 +\n      edu7cat4_beta * edu7cat4 +\n      edu7cat5_beta * edu7cat5 +\n      edu7cat6_beta * edu7cat6 +\n      edu7cat7_beta * edu7cat7 +\n      homeownHomeowner_beta * homeown,\n    .fitted = plogis(.fitted)\n  ) |> \n  arrange(sim_round, id, close)\n\nhead(results)\n\n# A tibble: 6 × 20\n  (Intercept)…¹ close_…² edu7c…³ edu7c…⁴ edu7c…⁵ edu7c…⁶ edu7c…⁷ edu7c…⁸ homeo…⁹\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1         -1.31 -0.00513    1.10   0.639    1.05    1.49    2.67    3.01   0.849\n2         -1.31 -0.00513    1.10   0.639    1.05    1.49    2.67    3.01   0.849\n3         -1.31 -0.00513    1.10   0.639    1.05    1.49    2.67    3.01   0.849\n4         -1.31 -0.00513    1.10   0.639    1.05    1.49    2.67    3.01   0.849\n5         -1.31 -0.00513    1.10   0.639    1.05    1.49    2.67    3.01   0.849\n6         -1.31 -0.00513    1.10   0.639    1.05    1.49    2.67    3.01   0.849\n# … with 11 more variables: sim_round <int>, edu7cat2 <dbl>, edu7cat3 <dbl>,\n#   edu7cat4 <dbl>, edu7cat5 <dbl>, edu7cat6 <dbl>, edu7cat7 <dbl>,\n#   homeown <dbl>, close <dbl>, id <int>, .fitted <dbl>, and abbreviated\n#   variable names ¹​`(Intercept)_beta`, ²​close_beta, ³​edu7cat2_beta,\n#   ⁴​edu7cat3_beta, ⁵​edu7cat4_beta, ⁶​edu7cat5_beta, ⁷​edu7cat6_beta,\n#   ⁸​edu7cat7_beta, ⁹​homeownHomeowner_beta\n\n\nWe then calculate the difference between the predicted probabilities for each observation when \\(close = 0\\) and \\(close = 20\\):\n\nresults |>\n  group_by(sim_round, id) |> \n  mutate(diff = (.fitted - lead(.fitted)) * 100) |> \n  drop_na(diff) |> \n  ungroup() |> \n  summarise(`Lower bound` = quantile(diff, 0.025),\n            `Mean` = quantile(diff, 0.5),\n            `Upper bound` = quantile(diff, 0.975))\n\n# A tibble: 1 × 3\n  `Lower bound`  Mean `Upper bound`\n          <dbl> <dbl>         <dbl>\n1         -1.44  1.94          6.56\n\n\nBecause this confidence interval crosses through 0, we cannot reject the null hypothesis that this substantive effect is caused by random error.\n\n\nProbit regression\nFirst, let’s fit our probit model:\n\nm2 <- glm(vote ~ close + edu7cat + homeown, data = voters, family = binomial(link = \"probit\"))\n\ntbl_regression(m2, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-0.49\n-1.3, 0.28\n0.2\n    Registration closing\n0.00\n-0.01, 0.00\n0.2\n    Education level\n\n\n\n    1\n—\n—\n\n    2\n0.28\n-0.52, 1.1\n0.5\n    3\n0.13\n-0.64, 0.93\n0.7\n    4\n0.31\n-0.44, 1.1\n0.4\n    5\n0.76\n0.00, 1.5\n0.054\n    6\n1.2\n0.39, 1.9\n0.004\n    7\n1.3\n0.52, 2.1\n0.001\n    Homeownership\n\n\n\n    Not homeowner\n—\n—\n\n    Homeowner\n0.50\n0.37, 0.63\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nLet’s collect those coefficients using broom::tidy():\n\ncoefs <- tidy(m2) |> pull(estimate)\ncoefs\n\n[1] -0.489578055 -0.003628296  0.283498003  0.128636563  0.314455793\n[6]  0.761436748  1.153567104  1.308027905  0.501008446\n\n\nNext, simulate fitting 1,000 different models using these estimates as our center-point using mvtnorm::rmvnorm():\n\ncoefs_sim <- rmvnorm(n = 1000, mean = coefs, sigma = vcov(m2)) |> \n  as_tibble() |> \n  set_names(tidy(m2) |> mutate(term = paste0(term, \"_beta\")) |> pull(term))\n\nhead(coefs_sim)\n\n# A tibble: 6 × 9\n  (Intercept)…¹ close_…² edu7c…³ edu7c…⁴ edu7c…⁵ edu7c…⁶ edu7c…⁷ edu7c…⁸ homeo…⁹\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1         0.297 -0.00761 -0.112  -0.493  -0.339   0.122    0.439   0.830   0.453\n2        -0.561 -0.00193  0.326   0.231   0.370   0.746    1.32    1.44    0.516\n3        -0.330 -0.00402  0.0286  0.0249  0.172   0.645    1.13    1.35    0.492\n4         0.131 -0.0115  -0.218  -0.388  -0.289   0.294    0.654   0.932   0.495\n5        -0.361  0.00153 -0.119  -0.229   0.0483  0.593    0.992   1.15    0.539\n6         0.181 -0.00379 -0.521  -0.662  -0.367   0.0337   0.598   0.948   0.482\n# … with abbreviated variable names ¹​`(Intercept)_beta`, ²​close_beta,\n#   ³​edu7cat2_beta, ⁴​edu7cat3_beta, ⁵​edu7cat4_beta, ⁶​edu7cat5_beta,\n#   ⁷​edu7cat6_beta, ⁸​edu7cat7_beta, ⁹​homeownHomeowner_beta\n\n\nLet’s look at the distribution of these simulated \\(\\beta\\)s for one of our variables: close:\n\nggplot(coefs_sim, aes(x = close_beta)) + \n  geom_histogram() + \n  geom_vline(xintercept = tidy(m2) |> filter(term == \"close\") |> pull(estimate)) + \n  theme_minimal()\n\n\n\n\nAs expected, this draw is normally distributed and centered around our estimated \\(\\beta_{close}\\).\nNext, we need to predict the probability that an individual will vote when \\(close = 20\\) and when \\(close = 0\\) using these 1,000 different model estimates. To do this, we need to convert our categorical variables into dummy variables so that we can fit the correct \\(\\beta\\)s to them:\n\ntrans_data <- voters |> \n  transmute(\n    edu7cat2 = if_else(edu7cat == 2, 1, 0),\n    edu7cat3 = if_else(edu7cat == 3, 1, 0),\n    edu7cat4 = if_else(edu7cat == 4, 1, 0),\n    edu7cat5 = if_else(edu7cat == 5, 1, 0),\n    edu7cat6 = if_else(edu7cat == 6, 1, 0),\n    edu7cat7 = if_else(edu7cat == 7, 1, 0),\n    homeown = as.numeric(homeown) - 1\n  )\n\nhead(trans_data)\n\n  edu7cat2 edu7cat3 edu7cat4 edu7cat5 edu7cat6 edu7cat7 homeown\n1        0        0        0        0        1        0       1\n2        0        0        0        0        1        0       1\n3        0        0        1        0        0        0       1\n4        0        0        0        1        0        0       1\n5        0        0        0        0        0        1       0\n6        0        0        0        1        0        0       0\n\n\nNow, we include our close variable, set to 0 and 20 for each of these 2,188 observations. We should get a dataset of length 4,376: one set of observations for \\(close = 20\\) and one set for \\(close = 0\\).\n\nnew_data <- trans_data |> \n  mutate(close = 0) |> \n  bind_rows(mutate(trans_data, close = 20)) |> \n  group_by(close) |> \n  mutate(id = row_number()) |> \n  ungroup()\n\nnrow(new_data)\n\n[1] 4376\n\n\nNext, we need to join our datasets together, so we can calculate our predicted probability for each observation for each simulated model coefficients. We should get a dataframe with a length of 2 x 1,000 x 2,188 (number of different variables of interest x number of models x number of observations).\n\nsim_data <- coefs_sim |> \n  mutate(sim_round = row_number()) |> \n  full_join(new_data, by = character())\n\nnrow(sim_data)\n\n[1] 4376000\n\n\nWe can now estimate our probit model using the 1,000 different estimated \\(\\beta\\)s for all 2,188 different observations. You need to first fit the linear model, then find the probit of those results using pnorm().\n\nresults <- sim_data |> \n  mutate(\n    .fitted = `(Intercept)_beta` +\n      close_beta * close +\n      edu7cat2_beta * edu7cat2 +\n      edu7cat3_beta * edu7cat3 +\n      edu7cat4_beta * edu7cat4 +\n      edu7cat5_beta * edu7cat5 +\n      edu7cat6_beta * edu7cat6 +\n      edu7cat7_beta * edu7cat7 +\n      homeownHomeowner_beta * homeown,\n    .fitted = pnorm(.fitted)\n  ) |> \n  arrange(sim_round, id, close)\n\nhead(results)\n\n# A tibble: 6 × 20\n  (Intercept)…¹ close_…² edu7c…³ edu7c…⁴ edu7c…⁵ edu7c…⁶ edu7c…⁷ edu7c…⁸ homeo…⁹\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1         0.297 -0.00761  -0.112  -0.493  -0.339   0.122   0.439   0.830   0.453\n2         0.297 -0.00761  -0.112  -0.493  -0.339   0.122   0.439   0.830   0.453\n3         0.297 -0.00761  -0.112  -0.493  -0.339   0.122   0.439   0.830   0.453\n4         0.297 -0.00761  -0.112  -0.493  -0.339   0.122   0.439   0.830   0.453\n5         0.297 -0.00761  -0.112  -0.493  -0.339   0.122   0.439   0.830   0.453\n6         0.297 -0.00761  -0.112  -0.493  -0.339   0.122   0.439   0.830   0.453\n# … with 11 more variables: sim_round <int>, edu7cat2 <dbl>, edu7cat3 <dbl>,\n#   edu7cat4 <dbl>, edu7cat5 <dbl>, edu7cat6 <dbl>, edu7cat7 <dbl>,\n#   homeown <dbl>, close <dbl>, id <int>, .fitted <dbl>, and abbreviated\n#   variable names ¹​`(Intercept)_beta`, ²​close_beta, ³​edu7cat2_beta,\n#   ⁴​edu7cat3_beta, ⁵​edu7cat4_beta, ⁶​edu7cat5_beta, ⁷​edu7cat6_beta,\n#   ⁸​edu7cat7_beta, ⁹​homeownHomeowner_beta\n\n\nWe then calculate the difference between the predicted probabilities for each observation when \\(close = 0\\) and \\(close = 20\\):\n\nresults |>\n  group_by(sim_round, id) |> \n  mutate(diff = (.fitted - lead(.fitted)) * 100) |> \n  drop_na(diff) |> \n  ungroup() |> \n  summarise(`Lower bound` = quantile(diff, 0.025),\n            `Mean` = quantile(diff, 0.5),\n            `Upper bound` = quantile(diff, 0.975))\n\n# A tibble: 1 × 3\n  `Lower bound`  Mean `Upper bound`\n          <dbl> <dbl>         <dbl>\n1         -1.24  2.19          6.52\n\n\nBecause this confidence interval crosses through 0, we cannot reject the null hypothesis that this substantive effect is caused by random error."
  },
  {
    "objectID": "contents/binary_marginal_effects.html",
    "href": "contents/binary_marginal_effects.html",
    "title": "Measuring Marginal Effects on Binary Outcomes",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(skimr)\nlibrary(broom)\nlibrary(gtsummary)\n\nLet’s explore marginal and substantive effects using real-world data. How is a person’s decision to vote influenced by the closing date of registration to vote in US elections? Suppose we hypothesize that the further from election day registration closes, the less likely an individual is to vote. We also believe that there are other socio-economic factors that influence a person’s decision to vote for which we need to control. These are their level of education, and whether they are a homeowner.\nLet’s explore our data. First, we need to load it in. I will use rio::import().\n\nrio allows you to import or export many different file types with a single command. This makes your code robust to changes in file types: if your data source changes the file type for your data, your code will not break. I recommend it for interactive coding.1 The package documentation can be found here.\n\n\nvoters_raw <- import(\"/Users/harrietgoers/Documents/GVPT729A/class_sets/data/cps00for729a.dta\")\n\nThis dataset contains individual-level survey data of US adults. It includes: information on whether they voted (vote); the number of days prior to election day their state closes registration (close); their level of education recorded in seven, ordered categories (edu7cat); and whether or not they are a homeowner (homeown).\nNext, we need to clean this data up:\n\nvoters <- voters_raw |> \n  transmute(vote = factor(vote, levels = c(0, 1), labels = c(\"Did not vote\", \"Voted\")), \n            close = as.integer(close), \n            edu7cat = factor(edu7cat), \n            homeown = factor(homeown, levels = c(0, 1), labels = c(\"Not homeowner\", \"Homeowner\"))) |> \n  labelled::set_variable_labels(vote = \"Voted\", close = \"Registration closing\", edu7cat = \"Education level\", homeown = \"Homeownership\") |> \n  drop_na()\n\nhead(voters)\n\n   vote close edu7cat       homeown\n1 Voted    10       6     Homeowner\n2 Voted    29       6     Homeowner\n3 Voted    28       4     Homeowner\n4 Voted     0       5     Homeowner\n5 Voted    25       7 Not homeowner\n6 Voted    25       5 Not homeowner\n\n\nNote that we have removed any observations with missing values.\n\nIf your categorical variables are stored as numeric data in your dataset, your model will treat them as continuous numeric variables. It will not exclude a base category. This will cause significant problems with your model. Always convert categorical variables to factors.2\n\nNow, let’s look at a summary of our data using skimr::skim():\n\nskim(voters)\n\n\nData summary\n\n\nName\nvoters\n\n\nNumber of rows\n2188\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nvote\n0\n1\nFALSE\n2\nVot: 1450, Did: 738\n\n\nedu7cat\n0\n1\nFALSE\n7\n4: 724, 5: 623, 6: 380, 7: 186\n\n\nhomeown\n0\n1\nFALSE\n2\nHom: 1665, Not: 523\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nclose\n0\n1\n22.29\n9.87\n0\n15\n29\n30\n30\n▁▁▂▁▇\n\n\n\n\n\nOur dataset contains 2,188 observations and 4 variables. Of those 2,188 individuals, 66% voted."
  },
  {
    "objectID": "contents/binary_marginal_effects.html#introduction",
    "href": "contents/binary_marginal_effects.html#introduction",
    "title": "Measuring Marginal Effects on Binary Outcomes",
    "section": "Introduction",
    "text": "Introduction\nOur goal is to make inferences from the sample to the population about how changes in our independent variable of interest, \\(x\\), influences the probability of success in our outcome of interest, \\(y\\). We can calculate this effect for each known value of \\(x\\), or the marginal effect. We can also calculate this effect for a meaningful change in the value of \\(x\\), or the substantive effect. We will discuss this in the next section.\nThe marginal effect measures the change in the probability of success gained (or lost) from changing \\(x_i\\) by one unit. In our example, we want to understand the marginal effect of close, or how increasing the number of days prior to an election registration closes by one impacts the likelihood that an individual will vote."
  },
  {
    "objectID": "contents/binary_marginal_effects.html#linear-probability-models",
    "href": "contents/binary_marginal_effects.html#linear-probability-models",
    "title": "Measuring Marginal Effects on Binary Outcomes",
    "section": "Linear probability models",
    "text": "Linear probability models\nIn linear models, this effect is constant. To illustrate, let’s fit a linear probability model to our voting data. We will focus only on the effect of close on vote for now.\n\nNote that lm() will not allow us to fit a linear model to binary data!\n\n\nvoters_linear <- mutate(voters, vote = as.integer(vote) - 1)\n\nm1 <- lm(vote ~ close, data = voters_linear)\n\ntbl_regression(m1, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.71\n0.66, 0.75\n<0.001\n    Registration closing\n0.00\n0.00, 0.00\n0.060\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\nggplot(voters_linear, aes(x = close, y = vote)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\n\nInterpreting the coefficients\nThis model suggests that increasing the registration closing day by one decreases the likelihood than individual will vote by 0.192%, on average. However, this result is not statistically significant. A individual registered in a state with election day registration (\\(close = 0\\)) will vote 71% of the time, on average.\nThe important take away from this is that the marginal effect of close on vote is constant across all values of close. If your state changes from election day registration to closing registration one day prior to election day, your probability of voting decreases 0.192%. Similarly, if your state changes from 20 days prior to 21 days prior to election day, your probability of voting decreases 0.192%.\n\n\nPredicting outcomes using your LPM\nLet’s take a closer look at this. We can use broom::augment() to see what our model predicts the probability of an individual to vote to be for each of the plausible closing dates (0 to 30 days prior to an election):\n\npred_m1 <- augment(m1, newdata = tibble(close = 0:30), type.predict = \"response\")\nhead(pred_m1)\n\n# A tibble: 6 × 2\n  close .fitted\n  <int>   <dbl>\n1     0   0.706\n2     1   0.704\n3     2   0.702\n4     3   0.700\n5     4   0.698\n6     5   0.696\n\n\n\nggplot(pred_m1, aes(x = close, y = .fitted * 100)) + \n  geom_line() + \n  theme_minimal() + \n  labs(x = \"Closing date (days)\",\n       y = \"Predicted probability of voting (%)\")\n\n\n\n\n\n\nMarginal effects\nFinally, let’s look at the difference in the predicted probability of voting by chnaging the closing date by one day for all plausible values of close:\n\nme_m1 <- pred_m1 |> \n  arrange(close) |> \n  mutate(diff = .fitted - lag(.fitted))\n\nhead(me_m1)\n\n# A tibble: 6 × 3\n  close .fitted     diff\n  <int>   <dbl>    <dbl>\n1     0   0.706 NA      \n2     1   0.704 -0.00192\n3     2   0.702 -0.00192\n4     3   0.700 -0.00192\n5     4   0.698 -0.00192\n6     5   0.696 -0.00192\n\n\n\nggplot(me_m1, aes(x = close, y = diff * 100)) + \n  geom_line() + \n  geom_hline(yintercept = 0, colour = \"darkgrey\") + \n  theme_minimal() + \n  scale_y_continuous(limits = c(-0.3, 0.3)) + \n  labs(x = \"Closing date (days)\",\n       y = \"Difference in predicted probability of voting (%)\")\n\n\n\n\nAs expected, this difference is constant. It is, in fact, the close coefficient. In LPMs, the variable coefficients are their marginal effects."
  },
  {
    "objectID": "contents/binary_marginal_effects.html#latent-variable-models",
    "href": "contents/binary_marginal_effects.html#latent-variable-models",
    "title": "Measuring Marginal Effects on Binary Outcomes",
    "section": "Latent variable models",
    "text": "Latent variable models\nBoth logit and probit models are curved. Therefore, the effect of a one-unit increase in \\(x_i\\) on the probability that \\(Y=1\\) is not constant. In fact, the marginal effect of a one-unit change in \\(x_i\\) depends on the starting value of \\(x_i\\). As demonstrated in the figure below, the steepest change for both the logit (blue) and probit (red) models occurs around the middle values of \\(x\\).\n\n\n\n\n\nIn fact, these marginal effects are bell-shaped:\n\n\n\n\n\nWe will look at logit and probit in turn.\n\nLogistic Regression\nFirst, let’s fit our logit model, focusing only on the effect of close on vote:\n\nm2 <- glm(vote ~ close, data = voters, family = binomial(link = \"logit\"))\n\ntbl_regression(m2, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      log(OR)1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.87\n0.65, 1.1\n<0.001\n    Registration closing\n-0.01\n-0.02, 0.00\n0.061\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\n\nInterpreting the coefficients\nThe coefficients presented above are log odds ratios. We can easily interpret their statistical significance and their sign. For example, we know that our independent variable of interest, close, is not statistically significant (\\(p = 0.061\\)). We also know that its effect on an individual’s decision to vote is negative: as days before an election the date of voter registration closes increases, the likelihood that an individual will vote decreases.\nRemember that logit (and probit) models are simply transformed linear models. We take our linear model of the relationship between close and vote and reshape it to better predict probabilities (bound it between 0 and 1, and reshape it to reflect varying effects of changes of \\(x\\) on \\(y\\)). We can reshape these coefficients to make them more interpretable.\n\nOdds ratios\nThe regression coefficient provided above is a log odds ratio. Log-transformed variables are really hard to interpret. So, let’s get rid of that log. The opposite operation to taking the logarithum of a number is to exponentiate it. To demonstrate:\n\ntibble(x = 1:5,\n       log_x = log(x),\n       exp_log_x = exp(log_x))\n\n# A tibble: 5 × 3\n      x log_x exp_log_x\n  <int> <dbl>     <dbl>\n1     1 0             1\n2     2 0.693         2\n3     3 1.10          3\n4     4 1.39          4\n5     5 1.61          5\n\n\nSo, exponentiating the log odd ratio will get us the odds ratio. The odds ratio is much easier to interpret. If the probability of success of an outcome is \\(p\\) and, therefore, the probability of failure is \\(1-p\\), the the odds of success is \\(\\frac{p}{1-p}\\). Now, dividing two odds by each other gives you their odds ratio. For example, if two outcomes have the odds \\(\\frac{p_1}{1-p_1}\\) and \\(\\frac{p_2}{1-p_2}\\), then these outcomes have an odds ratio of \\(\\frac{\\frac{p_1}{1-p_1}}{\\frac{p_2}{1-p_2}}\\).\nThis is particularly useful for comparing the probability of success and failure for a given value of \\(x_i\\). When the odds ratio is 1, the odds of success are the same as the odds of failure (\\(\\frac{0.5}{0.5} = 1\\)). When the odds ratio is greater than 1, the odds of success are greater than the odds of failure (for example, \\(\\frac{0.8}{0.2} = 4\\)). Where the odds ratio of success is four, the odds of success are four times higher than the odds of failure.\nGetting back to our model, the odds ratio that an individual will vote is:\n\\[\ne^{-0.01} = 0.99\n\\]\nSo, increasing the closing date by one day makes the decreases the odds that an individual will vote by 0.01 (or \\(1 - 0.99\\)).\nHappily, gtsummary::tbl_regression() can easily present these results for us:\n\ntbl_regression(m2, intercept = T, exponentiate = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n2.39\n1.91, 3.00\n<0.001\n    Registration closing\n0.99\n0.98, 1.00\n0.061\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nAnd we can also get these results programmatically using broom::tidy():\n\ntidy(m2, exponentiate = T)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    2.39    0.115        7.60 2.90e-14\n2 close          0.991   0.00467     -1.88 6.07e- 2\n\n\n\n\nPredicted probabilities\nOdds ratios are certainly easier to interpret than log odds ratios, but they are still a bit awkward. It is easier again to discuss the effects of changing our independent variables of interest in terms of probabilities, rather than odds.\nRemember, the odds ratio is simply the odds of success divided by the odds of failure:\n\\[\nOR = \\frac{\\frac{p}{1-p}}{\\frac{1-p}{p}}\n\\]\nWe are interested in getting the odds of success (\\(p\\)). To get this:\n\\[\np = \\frac{OR}{1 + OR}\n\\]\nMore generally:\n\\[\nPr(Y = 1|X) = \\frac{e^{X\\beta}}{1 + e^{X\\beta}}\n\\]\nFor our simple model, this is:\n\\[\nPr(vote = 1 | close) = \\frac{e^{0.872 - 0.01close + \\epsilon}}{1 + e^{0.872 - 0.01close + \\epsilon}}\n\\]\nWhere \\(vote = 0.872 - 0.01 close + \\epsilon\\) is our logit model. Critically, the predicted probability still includes close. We need to provide a value for close to calculate a predicted probability.\n\n\n\nInterpreting the intercept\nAs usual, the intercept should be interpreted as the expected value when all independent variables are set to 0. This is simple to interpret in terms of the probability of success. Remember:\n\\[\nPr(Y = 1 | X) = \\frac{e^{X\\beta}}{1 + e^{X\\beta}}\n\\]\nTherefore, for our voter model:\n\\[\n\\frac{e^{\\beta_0}}{1 + e^{\\beta_0}} = \\frac{e^{0.87}}{1 + e^{0.87}} = \\frac{2.39}{3.39} = 0.705\n\\]\nThe probability that an individual in a state with election day registration will vote is 70.5%, on average.\n\n\nPredicting outcomes\nYou now have a couple of different options in terms of how you present and interpret your predicted outcomes from your logit model: log odds ratios, odds ratios, and predicted probability.\nI find predicted probabilities the easiest to interpret and with which to work. Using augment::broom(), we can easily produce our predicted probabilities for each of the plausible values of close:\n\npred_m2 <- augment(m2, newdata = tibble(close = 0:30), type.predict = \"response\")\npred_m2\n\n# A tibble: 31 × 2\n   close .fitted\n   <int>   <dbl>\n 1     0   0.705\n 2     1   0.703\n 3     2   0.701\n 4     3   0.700\n 5     4   0.698\n 6     5   0.696\n 7     6   0.694\n 8     7   0.692\n 9     8   0.690\n10     9   0.688\n# … with 21 more rows\n\n\nAll this function is doing, is calculating \\(Pr(Y = 1 | X) = \\frac{e^{X\\beta}}{1 + e^{X\\beta}}\\) for each of these values of close. You can check this yourself:\n\npred_prob_logit <- function(close) {\n  \n  intercept <- tidy(m2) |> \n    filter(term == \"(Intercept)\") |> \n    pull(estimate)\n\n  beta_close <- tidy(m2) |> \n    filter(term == \"close\") |> \n    pull(estimate)\n\n  exp(intercept + (beta_close * close)) / (1 + exp(intercept + (beta_close * close)))\n  \n}\n\n# Calculate the predicted probability an individual will vote when closing date is \n# three days prior to the election\npred_prob_logit(3)\n\n[1] 0.6996091\n\n\nVisually:\n\nggplot(pred_m2, aes(x = close, y = .fitted * 100)) + \n  geom_line() + \n  theme_minimal() + \n  labs(x = \"Closing date (days)\",\n       y = \"Predicted probability of voting (%)\")\n\n\n\n\nOkay, so this looks linear but I promise it is not! We will get to that shortly. This graph shows the predicted probability that an individual will vote for each plausible closing date.\n\n\nMarginal effects\nFinally, let’s look at the difference in the predicted probability of voting by changing the closing date by one day for all plausible values of close:\n\nme_m2 <- pred_m2 |> \n  arrange(close) |> \n  mutate(diff = .fitted - lag(.fitted))\n\nhead(me_m2)\n\n# A tibble: 6 × 3\n  close .fitted     diff\n  <int>   <dbl>    <dbl>\n1     0   0.705 NA      \n2     1   0.703 -0.00182\n3     2   0.701 -0.00183\n4     3   0.700 -0.00184\n5     4   0.698 -0.00184\n6     5   0.696 -0.00185\n\n\n\nggplot(me_m2, aes(x = close, y = diff * 100)) + \n  geom_line() + \n  geom_hline(yintercept = 0, colour = \"darkgrey\") + \n  theme_minimal() + \n  scale_y_continuous(limits = c(-0.3, 0.3)) + \n  labs(x = \"Closing date (days)\",\n       y = \"Difference in predicted probability of voting (%)\")\n\n\n\n\nThe effect change is not constant. Moving from \\(close = 0\\) to \\(close = 1\\) decreases the predicted probability that an individual will vote by 0.182%. Moving from \\(close = 20\\) to \\(close = 21\\) decreases the predicted probability that an individual will vote by 0.195%.\nThese marginal effects are really small! Does changing the registration closing date in a state have a substantial impact on the likelihood that an individual will vote? We will discuss interpreting substantive significance in the next chapter.\n\n\n\nProbit Regression\nNext, let’s fit a probit model, focusing only on the effect of close on vote:\n\nm3 <- glm(vote ~ close, data = voters, family = binomial(link = \"probit\"))\n\ntbl_regression(m3, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.54\n0.40, 0.68\n<0.001\n    Registration closing\n-0.01\n-0.01, 0.00\n0.059\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\nInterpreting the coefficients\nThe regression coefficients presented above give the change in the z-score or probit index for a one unit change in the predictor. These can be interpreted as the change in the probability of success for a one unit change in the predictor. Unlike logit, we get straight to the point.\n\n\nInterpreting the intercept\n\nTODO\n\n\n\nPredicting outcomes\nUsing augment::broom(), we can easily produce our predicted probabilities for each of the plausible values of close:\n\npred_m3 <- augment(m3, newdata = tibble(close = 0:30), type.predict = \"response\")\npred_m3\n\n# A tibble: 31 × 2\n   close .fitted\n   <int>   <dbl>\n 1     0   0.705\n 2     1   0.703\n 3     2   0.702\n 4     3   0.700\n 5     4   0.698\n 6     5   0.696\n 7     6   0.694\n 8     7   0.692\n 9     8   0.690\n10     9   0.688\n# … with 21 more rows\n\n\nVisually:\n\nggplot(pred_m3, aes(x = close, y = .fitted * 100)) + \n  geom_line() + \n  theme_minimal() + \n  labs(x = \"Closing date (days)\",\n       y = \"Predicted probability of voting (%)\")\n\n\n\n\n\n\nMarginal effects\nFinally, let’s look at the difference in the predicted probability of voting by changing the closing date by one day for all plausible values of close:\n\nme_m3 <- pred_m3 |> \n  arrange(close) |> \n  mutate(diff = .fitted - lag(.fitted))\n\nme_m3\n\n# A tibble: 31 × 3\n   close .fitted     diff\n   <int>   <dbl>    <dbl>\n 1     0   0.705 NA      \n 2     1   0.703 -0.00185\n 3     2   0.702 -0.00185\n 4     3   0.700 -0.00186\n 5     4   0.698 -0.00186\n 6     5   0.696 -0.00187\n 7     6   0.694 -0.00187\n 8     7   0.692 -0.00188\n 9     8   0.690 -0.00188\n10     9   0.688 -0.00189\n# … with 21 more rows\n\n\n\nggplot(me_m3, aes(x = close, y = diff * 100)) + \n  geom_line() + \n  geom_hline(yintercept = 0, colour = \"darkgrey\") + \n  theme_minimal() + \n  scale_y_continuous(limits = c(-0.3, 0.3)) + \n  labs(x = \"Closing date (days)\",\n       y = \"Difference in predicted probability of voting (%)\")\n\n\n\n\nAs with our logit model, the effect change is not constant. Moving from \\(close = 0\\) to \\(close = 1\\) decreases the predicted probability that an individual will vote by 0.185%. Moving from \\(close = 20\\) to \\(close = 21\\) decreases the predicted probability that an individual will vote by 0.195%."
  }
]