[
  {
    "objectID": "contents/intro_binary.html",
    "href": "contents/intro_binary.html",
    "title": "Introduction to Binary Response Modelling",
    "section": "",
    "text": "We often want to better understand binary outcomes in political science. When will a country go to war? What prompts people to vote in elections? For each of these cases, we observe one of two outcomes: war or no war, turned out or didn’t.\nLet’s start with simulated data to illustrate the theory. We will then move to a more interesting example using real-world data.\nLoad relevant packages:\nCreate simulated data:\nNow, let’s plot the relationship between our binary dependent variable, y, and our independent variable of interest, x.\nThere seems to be a pretty clear relationship between y and x (because we created the data that way). When x is less than 5, you are very likely (in fact 95 percent likely) to get a y of 0. But how do we formally measure this?\nWhen working with binary outcomes, we want to understand the probability that you will get an outcome (for example, the country went to war) for any given value of your independent variable(s). From there, you can make an informed guess as to the outcome for given values of \\(X\\). For example, where the predicted probability of success is greater than 50 percent, you can predict that \\(Y = 1\\)."
  },
  {
    "objectID": "contents/intro_binary.html#linear-probability-model",
    "href": "contents/intro_binary.html#linear-probability-model",
    "title": "Introduction to Binary Response Modelling",
    "section": "Linear Probability Model",
    "text": "Linear Probability Model\nLet’s start off simple. Let’s draw a straight line between these two clusters and see what we get.\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\") + \n  theme_custom()\n\n\n\n\nThis model is our usual linear model:\n\\[\nY_i = \\beta_0 + \\beta_1X_{1i} + \\mu_i\n\\]\nThe estimation of \\(Y_i\\) can (and often does) take on values other than 0 or 1.\n\nm1 <- lm(y ~ x, data = df)\n\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.16066 -0.17444 -0.00543  0.18671  1.17741 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.185682   0.020050  -9.261   <2e-16 ***\nx            0.136361   0.003449  39.532   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3124 on 998 degrees of freedom\nMultiple R-squared:  0.6103,    Adjusted R-squared:  0.6099 \nF-statistic:  1563 on 1 and 998 DF,  p-value: < 2.2e-16\n\n\n\nIssues with LPM"
  },
  {
    "objectID": "contents/intro_binary.html#latent-variable-approach",
    "href": "contents/intro_binary.html#latent-variable-approach",
    "title": "Introduction to Binary Response Modelling",
    "section": "Latent Variable Approach",
    "text": "Latent Variable Approach\nWe only observe one of two outcomes: \\(Y = 0\\) (failure) or \\(Y = 1\\) (success). An alternative to LPM, the Latent Variable Approach assumes a continuous relationship exists between our observed outcome (\\(Y\\)) and our independent variables (\\(X\\)). Here, we need some threshold of probability above which \\(Y = 1\\) and under which \\(Y = 0\\). We can then define the relationship between our outcome and independent variables in reference to this threshold.\nFor example, let’s set our threshold to 0. Therefore, whenever our expected value of \\(Y_i^* > 0\\) for a given value of \\(X_i\\), we predict that \\(Y_i^* = 1\\).\nIf the underlying process is defined by our independent variables of interest, we can model the relationship between \\(Y\\) and \\(X\\) as:\n\\[\nY_i^* = X_i\\beta + \\mu_i\n\\]\nTherefore, the probability of success (where \\(Y_i^* = 1\\)) is defined as the probability that our modeled relationship is greater than 0, or:\n\\[\nPr(Y_i = 1 | X_i) = Pr(X_i\\beta + u_i > 0)\n\\]\nNow we just need a theoretically-driven model of that underlying relationship. What shape should it take on? The goal of Maximum Likelihood Estimation is to maximise the probability that we observed the data that we did. This goal provides us with two common options for our model: logistic and probit regression.\n\nLogistic Regression\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"glm\", method.args = list(family = binomial(link = \"logit\"))) + \n  theme_custom()\n\n\n\n\n\n\nProbit regression\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"glm\", method.args = list(family = binomial(link=\"probit\"))) + \n  theme_custom()\n\n\n\n\n\n\nSummary of the models\nThese models appear to better estimate the relationship between our dependent and independent variables. First, they do not estimate a probability of success outside our bounds of 0 and 1. Secondly, the effect of an increase in \\(x_i\\) on \\(P(Y = 1)\\) is smaller at the extremes. In other words, for very low or very high values of \\(x_i\\), a small shift produces a small increase in the probability of success. For values of \\(x_i\\) closer to the middle (0.5) a small shift in \\(x_i\\) produces a large increase in the probability of success.\nLogit and probit models have different shapes.\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"glm\", se = FALSE, colour = \"blue\", method.args = list(family = binomial(link=\"logit\"))) + \n  geom_smooth(method = \"glm\", se = FALSE, colour = \"red\", method.args = list(family = binomial(link=\"probit\"))) + \n  theme_custom()"
  },
  {
    "objectID": "contents/binary_intro_binary.html",
    "href": "contents/binary_intro_binary.html",
    "title": "Introduction to Binary Response Modelling",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(gtsummary)"
  },
  {
    "objectID": "contents/binary_intro_binary.html#linear-probability-model",
    "href": "contents/binary_intro_binary.html#linear-probability-model",
    "title": "Introduction to Binary Response Modelling",
    "section": "Linear Probability Model",
    "text": "Linear Probability Model\nLet’s start off simple. Let’s draw a straight line between these two clusters and see what we get.\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nThis is our usual linear model:\n\nm_lpr <- lm(y ~ x, data = df)\n\ntbl_regression(m_lpr, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-0.17\n-0.21, -0.13\n<0.001\n    x\n0.13\n0.13, 0.14\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nOur estimation of \\(y_i\\) can (and often does) take on values other than 0 or 1. This is because we can interpret the coefficients of this model as differences in the probability of success (\\(y_i = 1\\)).\nWe can see that increasing \\(x\\) by one unit increases the probability that \\(y = 1\\) by 13.4%.\n\nIssues with LPM\nWe run into difficulties using LPM for prediction. First, our model can predict probabilities of success less than 0 and greater than 1. Second, and relatedly, we lose information treating these discrete outcomes (0 or 1) as continuous.\nImportantly, our theory may suggest a non-linear relationship between changes in \\(x\\) and the probability of success in \\(y\\). If this is the case, we should not use a linear model for this relationship."
  },
  {
    "objectID": "contents/binary_intro_binary.html#latent-variable-approach",
    "href": "contents/binary_intro_binary.html#latent-variable-approach",
    "title": "Introduction to Binary Response Modelling",
    "section": "Latent Variable Approach",
    "text": "Latent Variable Approach\nWe can only observe one of two outcomes: \\(y = 0\\) (failure) or \\(y = 1\\) (success). The linear model provided above does not account for this very well. How can we improve this model? The latent variable approach assumes a continuous relationship exists between our observed outcome (\\(y_i\\)) and our independent variables (\\(x_i\\)). This continuous relationship is driven by a continuous, unobserved outcome: \\(z_i\\).\n\\[\nz_i = X_i\\beta + \\epsilon_i\n\\]\nThis set up is familiar to us. Critically, though, we need to understand the shape of that independent error term, \\(\\epsilon_i\\). This defines the shape of the continuous relationship that takes us from \\(y = 0\\) to \\(y = 1\\). We have two common options to pick from: logistic or probit.\n\nLogistic Regression\nThe inverse logistic function suits our needs well. First, it is bounded between outcomes of 0 and 1. Second, it allows for a varying impact of a change in \\(x\\) on \\(y\\).\nFormally, the inverse logistic function is:\n\\[\nlogit^-1(x) = \\frac{e^x}{1 + e^x}\n\\]\nLet’s look at the shape of the inverse logistic function:\n\ntibble(x = seq(-10, 10, by = 0.5)) |> \n  mutate(y = plogis(x)) |> \n  ggplot(aes(x = x, y = y)) + \n  geom_line() + \n  theme_minimal()\n\n\n\n\n\nThe function plogis() gives you the inverse log of a number. For example, plogis(1) returns 0.7310586.\n\n\nThe model\nLet’s fit a logistic regression line against our data:\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"glm\", se = F, method.args = list(family = binomial(link = \"logit\"))) + \n  theme_minimal()\n\n\n\n\nOur model maps the relationship between our outcome (\\(y\\)) and our independent variable (\\(x\\)). We can interpret it as mapping the probability that \\(y = 1\\) for a given value of \\(x\\), otherwise written as \\(Pr(y = 1|x)\\).\nWe can fit this model as such:\n\nm_lr <- glm(y ~ x, data = df, family = binomial(link = \"logit\"))\n\ntbl_regression(m_lr, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      log(OR)1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-5.3\n-6.0, -4.7\n<0.001\n    x\n1.1\n0.95, 1.2\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nWe will explore how to interpret these coefficients and uncertainty in the next section.\n\n\n\nProbit Regression\nAn alternative approach is probit regression. This model is also bounded between outcomes of 0 and 1 and allows for a varying impact of a change in \\(x\\) on \\(y\\). The only real difference between the logistic and probit regression models are the ways they model the error term in our latent variable \\(z_i\\). Probit replaces the logistic distribution with the normal distribution.\nFormally, the probit model is:\n\\[\nPr(y_i = 1) = \\phi(X_i\\beta)\n\\]\nLet’s look at the shape of the probit function:\n\ntibble(x = seq(-10, 10, by = 0.5)) |> \n  mutate(y = pnorm(x)) |> \n  ggplot(aes(x = x, y = y)) + \n  geom_line() + \n  theme_minimal()\n\n\n\n\n\nThe function pnorm() gives you the corresponding value for the normal cumulative distribution function. For example, pnorm(1.96) returns 0.9750021 (think confidence intervals!).\n\n\nThe model\nLet’s fit a probit regression line against our data:\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"glm\", se = F, method.args = list(family = binomial(link = \"logit\"))) + \n  theme_minimal()\n\n\n\n\nQuickly, let’s compare this probit regression (in red) to our logistic regression (in blue):\n\n\n\n\n\nThey both fit very similar models; however, the logistic regression produces fatter tails.\nOur probit model maps the relationship between our outcome (\\(y\\)) and our independent variable (\\(x\\)). Like the logistic regression, we can interpret it as mapping the probability that \\(y = 1\\) for a given value of \\(x\\), otherwise written as \\(Pr(y = 1|x)\\).\nWe can fit this model as such:\n\nm_pr <- glm(y ~ x, data = df, family = binomial(link = \"probit\"))\n\ntbl_regression(m_pr, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-2.6\n-2.9, -2.4\n<0.001\n    x\n0.53\n0.48, 0.58\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nWe will explore how to interpret these coefficients and uncertainty in the next section."
  },
  {
    "objectID": "contents/binary_calc_pred_prob.html",
    "href": "contents/binary_calc_pred_prob.html",
    "title": "Calculating Predicted Probabilities",
    "section": "",
    "text": "Our goal is to make inferences from the sample to the population about how changes in \\(X\\) influence the probability of success. We are particularly interested in the substantive significant of the effect."
  },
  {
    "objectID": "contents/binary_effects.html",
    "href": "contents/binary_effects.html",
    "title": "Measuring Marginal and Substantive Effects on Binary Outcomes",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(skimr)\nlibrary(sjPlot)\nlibrary(broom)\nlibrary(gtsummary)\n\nLet’s explore marginal and substantive effects using real-world data. How is a person’s decision to vote influenced by the closing date of registration to vote in US elections? Suppose we hypothesize that the further from election registration closes, the less likely an individual is to vote. We believe that there are other socio-economic factors that influence a person’s decision to vote for which we need to control. These are their level of education, and whether they are a homeowner.\nLet’s explore our data. First, we need to load it in. I will use rio::import().\n\nvoters_raw <- rio::import(\"/Users/harrietgoers/Documents/GVPT729A/class_sets/data/cps00for729a.dta\")\n\nNext, we need to clean this data up:\n\nvoters <- voters_raw |> \n  transmute(vote = factor(vote, levels = c(0, 1), labels = c(\"Did not vote\", \"Voted\")), \n            close, \n            edu7cat = factor(edu7cat), \n            homeown = factor(homeown, levels = c(0, 1), labels = c(\"Not homeowner\", \"Homeowner\"))) |> \n  labelled::set_variable_labels(vote = \"Voted\", close = \"Registration closing\", edu7cat = \"Education level\", homeown = \"Homeownership\")\n\nhead(voters)\n\n   vote close edu7cat       homeown\n1 Voted    10       6     Homeowner\n2 Voted    29       6     Homeowner\n3 Voted    28       4     Homeowner\n4 Voted     0       5     Homeowner\n5 Voted    25       7 Not homeowner\n6 Voted    25       5 Not homeowner\n\n\n\nIf your categorical variables are stored as numeric data in your dataset, your model will treat them as continuous numeric variables. It will not exclude a base category. This will cause significant problems with your model. Always convert categorical variables to factors.\n\nNow, let’s look at a summary of our data using skimr::skim():\n\nskim(voters)\n\n\nData summary\n\n\nName\nvoters\n\n\nNumber of rows\n2446\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nvote\n258\n0.89\nFALSE\n2\nVot: 1450, Did: 738\n\n\nedu7cat\n0\n1.00\nFALSE\n7\n4: 818, 5: 699, 6: 419, 3: 204\n\n\nhomeown\n0\n1.00\nFALSE\n2\nHom: 1854, Not: 592\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nclose\n0\n1\n22.31\n9.8\n0\n15\n29\n30\n30\n▁▁▂▁▇\n\n\n\n\n\nOur dataset contains 2,446 observations and 4 variables. Each observation represents an individual. For each individual, we have information on: whether or not they voted (vote); the number of days before the election that voter registration closes in their state (close); their level of education, broken down into one of seven categories (edu7cat); and whether or not they own a home (homeown).\nNote that we are missing vote data for 258 individuals. These observations will be dropped from our models."
  },
  {
    "objectID": "contents/binary_effects.html#average-case-approach",
    "href": "contents/binary_effects.html#average-case-approach",
    "title": "Measuring Marginal and Substantive Effects",
    "section": "Average Case Approach",
    "text": "Average Case Approach\nThis approach sets all other values to means (modes for binary variables)."
  },
  {
    "objectID": "contents/binary_effects.html#observed-value-approach",
    "href": "contents/binary_effects.html#observed-value-approach",
    "title": "Measuring Marginal and Substantive Effects",
    "section": "Observed Value Approach",
    "text": "Observed Value Approach\nThis approach sets all other values to their observed values, then takes the average of those effects.\n\nBenefits\nIt provides a closer connection between the results and the theory and research design."
  },
  {
    "objectID": "contents/binary_effects.html#marginal-effects",
    "href": "contents/binary_effects.html#marginal-effects",
    "title": "Measuring Marginal and Substantive Effects on Binary Outcomes",
    "section": "Marginal Effects",
    "text": "Marginal Effects\nThe marginal effect is the effect of a given \\(x_i\\) on \\(y_i\\). In linear models, this effect is constant. However, both logit and probit models are curved. Therefore, the effect of \\(x_i\\) on \\(y_i\\) depends on your \\(x_i\\). As demonstrated in the figure below, the steepest change for both the logit (blue) and probit (red) models occurs around the middle values of \\(x_i\\).\n\n\n\n\n\nLet’s take a look at the effect of setting the closing registration date 20 days prior to the election day. We will start with a logistic regression.\n\nLogistic Regression\nFirst, let’s fit our model:\n\nm1 <- glm(vote ~ close + edu7cat + homeown, data = voters, family = binomial(link = \"logit\"))\n\ntbl_regression(m1, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      log(OR)1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-0.81\n-2.2, 0.45\n0.2\n    Registration closing\n-0.01\n-0.02, 0.00\n0.2\n    Education level\n\n\n\n    1\n—\n—\n\n    2\n0.46\n-0.84, 1.9\n0.5\n    3\n0.21\n-1.0, 1.6\n0.7\n    4\n0.51\n-0.73, 1.9\n0.4\n    5\n1.2\n0.00, 2.6\n0.055\n    6\n1.9\n0.67, 3.3\n0.003\n    7\n2.2\n0.91, 3.6\n0.001\n    Homeownership\n\n\n\n    Not homeowner\n—\n—\n\n    Homeowner\n0.83\n0.61, 1.0\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\n\nInterpreting the coefficients\nThese coefficients are difficult to interpret because the model is non-linear. With linear regression, we can interpret the coefficient of \\(x_i\\) to be the effect of a one unit change in \\(x_i\\) on the expected value of \\(y\\). This effect is the same for every value of \\(x_i\\).\nWith non-linear logistic regression, we need to transform our coefficients to meaningfully interpret them. Remember, our model estimates the probability of success as:\n\\[\nPr(y_i = 1 | x_i) = logit^{-1}(X_i\\beta)\n\\]\nThe first derivative with respect to \\(x_i\\) of this function includes \\(X_i\\). Therefore, when we talk about the effect of \\(x_i\\) on \\(y\\), we need to pick a value of \\(x_i\\): the answer to this question is different for every value of \\(x_i\\).\nThe coefficients presented above are log odds ratios. We can interpret their statistical significance and their sign. For example, we know that our independent variable of interest, close, is not statistically significant (\\(p = 0.232\\)). We also know that its effect on an individual’s decision to vote is negative: as days before an election the date of voter registration closes increases, the likelihood that an individual will vote decreases. However, we cannot meaningfully discuss this coefficient (-0.01) without first transforming it.\n\nPredicted probabilities\nWe can use the inverse logit function to discover the predicted probability that an individual will vote for a given set of predictors.\n\\[\nPr(y = 1 | X_i) = \\frac{e^{\\beta_i}}{1 + e^{\\beta_i}}\n\\]\n\n\nOdds ratios\nYou can interpret the coefficient in terms of its odds ratio. If the probability of success of an outcome is \\(p\\) and, therefore, the probability of failure is \\(1-p\\), the the odds of success is \\(\\frac{p}{1-p}\\). Now, dividing two odds by each other gives you the odds ratio. For example, if two outcomes have the odds \\(\\frac{p_1}{1-p_1}\\) and \\(\\frac{p_2}{1-p_2}\\), then these outcomes have an odds ratio of \\(\\frac{\\frac{p_1}{1-p_1}}{\\frac{p_2}{1-p_2}}\\).\nThis is particularly useful for comparing the probability of success and failure for a given value of \\(x_i\\). When the odds ratio is 1, the odds of success are the same as the odds of failure (\\(\\frac{0.5}{0.5} = 1\\)). When the odds ratio is greater than 1, the odds of success are greater than the odds of failure (for example, \\(\\frac{0.8}{0.2} = 4\\)).\nTo get the odds ratio from the coefficients presented above, we exponentiate them:\n\\[\ne^\\beta\n\\]\n\nExponentiation is the opposite operation to log transformation. So, to get from the log odds ratio presented in the table above to the odds ratio, we simply need to get rid of the log (leaving the odds ratio).\n\nFor home ownership:\n\\[\ne^\\beta = e^{0.83} = 2.29\n\\]\nThis means that a homeowner is 2.29 times more likely to vote than a non-homeowner (our reference category), holding all other variables a fixed values.\nFor education level 5:\n\\[\ne^\\beta = e^{1.2} = 3.32\n\\]\nThis means that a person with a level of education in category 5 is 3.32 times more likely to vote than someone with a level of education in category 1 (our reference category), holding all other variables a fixed values. You can calculate this for any education level. The interpretation should always be in reference to your reference category.\nContinuous variables are trickier to interpret. For close:\n\\[\ne^\\beta = e^{-0.006} = 0.994\n\\]\nThis means that a one unit increase in close (increasing the days before the election that registration closes by one day), decreases the odds of voting by a factor of 0.994, holding all other variables a fixed values.\n\nTODO: Check this.\n\n\nTo get the exponential of a number in R, use exp().\n\nHappily, gtsummary::tbl_regression() can easily present these results for us:\n\ntbl_regression(m1, intercept = T, exponentiate = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.45\n0.11, 1.56\n0.2\n    Registration closing\n0.99\n0.98, 1.00\n0.2\n    Education level\n\n\n\n    1\n—\n—\n\n    2\n1.59\n0.43, 6.60\n0.5\n    3\n1.24\n0.35, 4.96\n0.7\n    4\n1.66\n0.48, 6.54\n0.4\n    5\n3.46\n1.00, 13.6\n0.055\n    6\n6.86\n1.95, 27.4\n0.003\n    7\n9.13\n2.49, 38.0\n0.001\n    Homeownership\n\n\n\n    Not homeowner\n—\n—\n\n    Homeowner\n2.28\n1.85, 2.83\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nWe can also get these results programmatically using broom::tidy():\n\ntidy(m1, exponentiate = T)\n\n# A tibble: 9 × 5\n  term             estimate std.error statistic  p.value\n  <chr>               <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)         0.446   0.655      -1.23  2.18e- 1\n2 close               0.994   0.00490    -1.19  2.32e- 1\n3 edu7cat2            1.59    0.679       0.679 4.97e- 1\n4 edu7cat3            1.24    0.658       0.321 7.48e- 1\n5 edu7cat4            1.66    0.645       0.788 4.31e- 1\n6 edu7cat5            3.46    0.647       1.92  5.50e- 2\n7 edu7cat6            6.86    0.656       2.94  3.33e- 3\n8 edu7cat7            9.13    0.679       3.26  1.13e- 3\n9 homeownHomeowner    2.28    0.108       7.62  2.55e-14\n\n\nFinally, we can visualise these results using sjPlot::plot_model():\n\nplot_model(m1, sort.est = T, show.values = T, value.offset = .3)\n\n\n\n\n\n\n\nInterpreting the intercept\nAs usual, the intercept should be interpreted as the expected value when all independent variables are set to 0.\nThis is simple to interpret in terms of the probability of success. Remember:\n\\[\nPr(y = 1 | X_i) = \\frac{e^{\\beta_i}}{1 + e^{\\beta_i}}\n\\]\nTherefore, for our voter model:\n\\[\n\\frac{e^{\\beta_0}}{1 + e^{\\beta_0}} = \\frac{e^{-0.81}}{1 + e^{-0.81}} = \\frac{0.445}{1.445} = 0.308\n\\]\nThe probability that an individual in a state with election day registration, who has an education level of category 1, and who does not own a house is 0.308 or 30.8%."
  },
  {
    "objectID": "contents/binary_effects.html#substantive-effects",
    "href": "contents/binary_effects.html#substantive-effects",
    "title": "Measuring Marginal and Substantive Effects on Binary Outcomes",
    "section": "Substantive Effects",
    "text": "Substantive Effects\n\nAverage Case Approach\nThis approach sets all other values to means (modes for binary variables).\n\n\nObserved Value Approach\nThis approach sets all other values to their observed values, then takes the average of those effects.\n\nBenefits\nIt provides a closer connection between the results and the theory and research design."
  },
  {
    "objectID": "contents/binary_effects.html#introduction",
    "href": "contents/binary_effects.html#introduction",
    "title": "Measuring Marginal and Substantive Effects on Binary Outcomes",
    "section": "Introduction",
    "text": "Introduction\nOur goal is to make inferences from the sample to the population about how changes in our independent variable of interest, \\(x\\), influences the probability of success in our outcome of interest, \\(y\\). We can calculate this effect for each known value of \\(x\\), or the marginal effect. We can also calculate this effect for a meaningful change in the value of \\(x\\), or the substantive effect. We will discuss this in the next section."
  },
  {
    "objectID": "contents/binary_intro_binary.html#introduction",
    "href": "contents/binary_intro_binary.html#introduction",
    "title": "Introduction to Binary Response Modelling",
    "section": "Introduction",
    "text": "Introduction\nWe often want to better understand binary outcomes in political science.\nLet’s start with simulated data to illustrate the theory. Let’s create some data:\n\ndf <- tibble(x = runif(1000, 0, 10)) |> \n  mutate(y = if_else(x < 5, \n                     sample(0:1, 1000, replace = T, prob = c(0.95, 0.05)),\n                     sample(0:1, 1000, replace = T, prob = c(0.05, 0.95))))\n\nhead(df)\n\n# A tibble: 6 × 2\n      x     y\n  <dbl> <int>\n1  8.66     1\n2  8.75     1\n3  6.19     1\n4  7.68     1\n5  3.06     0\n6  3.50     1\n\n\nNow, let’s plot the relationship between our binary dependent variable, y, and our independent variable of interest, x.\n\nggplot(df, aes(x = x, y = y)) + \n  geom_point(alpha = 0.5) + \n  theme_minimal()\n\n\n\n\nThere seems to be a pretty clear relationship between y and x (because we created the data that way). When x is less than 5, you are very likely (in fact 95 percent likely) to get a y of 0. But how do we formally measure this?\nWhen working with binary outcomes, we want to understand the probability that you will get an outcome (for example, the country went to war) for any given value of your independent variable(s). From there, you can make an informed guess as to the outcome for given values of \\(X\\). For example, where the predicted probability of success is greater than 50 percent, you can predict that \\(y = 1\\)."
  },
  {
    "objectID": "contents/binary_effects.html#making-predictions",
    "href": "contents/binary_effects.html#making-predictions",
    "title": "Measuring Marginal and Substantive Effects on Binary Outcomes",
    "section": "Making predictions",
    "text": "Making predictions\nWe can use our model and broom::augment() to predict the probability of success for a given value of \\(x\\). To illustrate, let’s predict the probability of success where \\(x = 6\\):\n\naugment(m_lr, newdata = tibble(x = 6), type.predict = \"response\")\n\n# A tibble: 1 × 2\n      x .fitted\n  <dbl>   <dbl>\n1     6   0.756\n\n\nNotice how our output does not display uncertainty. However, our coefficients are estimates and come with uncertainty. We need to generate an understanding of this uncertainty ourselves.\nFirst, we assume that the range of possible estimates of our coefficients are normally distributed. That is, if we collected sufficiently large and representative repeated samples of our population, we would fit models that estimated the coefficients that converged on the population’s true coefficient. Therefore, to produce our confidence intervals around our estimated coefficients, we should simulate this process.\nFirst, we need to collect our coefficient estimates:\n\ncoefs <- tidy(m_lr) |> \n  pull(estimate)\n\ncoefs\n\n[1] -6.73204  1.31022\n\n\nNext, we need to simulate drawing these coefficients a sufficient number of times. Here, we are going to make 1,000 draws.\n\nTODO: Learn more about the sigma argument.\n\n\ncoefs_sim <- rmvnorm(n = 1000, mean = coefs, sigma = vcov(m_lr)) |> \n  as_tibble() |> \n  set_names(tidy(m_lr) |> pull(term))\n\nhead(coefs_sim)\n\n# A tibble: 6 × 2\n  `(Intercept)`     x\n          <dbl> <dbl>\n1         -7.45  1.43\n2         -7.10  1.36\n3         -6.93  1.34\n4         -7.42  1.49\n5         -7.61  1.47\n6         -6.41  1.25\n\n\nLet’s take a look at this:\n\np1 <- ggplot(coefs_sim, aes(x = `(Intercept)`)) + \n  geom_histogram() + \n  geom_vline(xintercept = coefs[1], colour = \"red\") + \n  geom_vline(xintercept = mean(coefs_sim$`(Intercept)`), colour = \"blue\") + \n  theme_minimal()\n\np2 <- ggplot(coefs_sim, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = coefs[2], colour = \"red\") + \n  geom_vline(xintercept = mean(coefs_sim$x), colour = \"blue\") + \n  theme_minimal()\n\np1 | p2\n\n\n\n\nOur model’s estimates (highlighted by the red line) are very close to the mean of the simulated coefficients (highlighted by the blue line). This is good! We have now simulated taking 1000 different representative samples from the population and estimating 1000 different models from those samples. From this, we can get an understanding of the uncertainty surrounding our estimated coefficients:\n\ntibble(Term = c(\"Lower CI\", \"Mean\", \"Upper CI\"),\n       Estimate = quantile(coefs_sim$x, c(0.025, 0.5, 0.975)))\n\n# A tibble: 3 × 2\n  Term     Estimate\n  <chr>       <dbl>\n1 Lower CI     1.14\n2 Mean         1.31\n3 Upper CI     1.47\n\n\n\nImportantly, this assumes that your sample is representative and that the errors surrounding our model estimates are normally distributed."
  },
  {
    "objectID": "contents/binary_marginal_effects.html",
    "href": "contents/binary_marginal_effects.html",
    "title": "Measuring Marginal Effects on Binary Outcomes",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(skimr)\nlibrary(sjPlot)\nlibrary(broom)\nlibrary(gtsummary)\n\nLet’s explore marginal and substantive effects using real-world data. How is a person’s decision to vote influenced by the closing date of registration to vote in US elections? Suppose we hypothesize that the further from election registration closes, the less likely an individual is to vote. We believe that there are other socio-economic factors that influence a person’s decision to vote for which we need to control. These are their level of education, and whether they are a homeowner.\nLet’s explore our data. First, we need to load it in. I will use rio::import().\n\nvoters_raw <- rio::import(\"/Users/harrietgoers/Documents/GVPT729A/class_sets/data/cps00for729a.dta\")\n\nNext, we need to clean this data up:\n\nvoters <- voters_raw |> \n  transmute(vote = factor(vote, levels = c(0, 1), labels = c(\"Did not vote\", \"Voted\")), \n            close, \n            edu7cat = factor(edu7cat), \n            homeown = factor(homeown, levels = c(0, 1), labels = c(\"Not homeowner\", \"Homeowner\"))) |> \n  labelled::set_variable_labels(vote = \"Voted\", close = \"Registration closing\", edu7cat = \"Education level\", homeown = \"Homeownership\")\n\nhead(voters)\n\n   vote close edu7cat       homeown\n1 Voted    10       6     Homeowner\n2 Voted    29       6     Homeowner\n3 Voted    28       4     Homeowner\n4 Voted     0       5     Homeowner\n5 Voted    25       7 Not homeowner\n6 Voted    25       5 Not homeowner\n\n\n\nIf your categorical variables are stored as numeric data in your dataset, your model will treat them as continuous numeric variables. It will not exclude a base category. This will cause significant problems with your model. Always convert categorical variables to factors.\n\nNow, let’s look at a summary of our data using skimr::skim():\n\nskim(voters)\n\n\nData summary\n\n\nName\nvoters\n\n\nNumber of rows\n2446\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nvote\n258\n0.89\nFALSE\n2\nVot: 1450, Did: 738\n\n\nedu7cat\n0\n1.00\nFALSE\n7\n4: 818, 5: 699, 6: 419, 3: 204\n\n\nhomeown\n0\n1.00\nFALSE\n2\nHom: 1854, Not: 592\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nclose\n0\n1\n22.31\n9.8\n0\n15\n29\n30\n30\n▁▁▂▁▇\n\n\n\n\n\nOur dataset contains 2,446 observations and 4 variables. Each observation represents an individual. For each individual, we have information on: whether or not they voted (vote); the number of days before the election that voter registration closes in their state (close); their level of education, broken down into one of seven categories (edu7cat); and whether or not they own a home (homeown).\nNote that we are missing vote data for 258 individuals. These observations will be dropped from our models."
  },
  {
    "objectID": "contents/binary_marginal_effects.html#introduction",
    "href": "contents/binary_marginal_effects.html#introduction",
    "title": "Measuring Marginal Effects on Binary Outcomes",
    "section": "Introduction",
    "text": "Introduction\nOur goal is to make inferences from the sample to the population about how changes in our independent variable of interest, \\(x\\), influences the probability of success in our outcome of interest, \\(y\\). We can calculate this effect for each known value of \\(x\\), or the marginal effect. We can also calculate this effect for a meaningful change in the value of \\(x\\), or the substantive effect. We will discuss this in the next section."
  },
  {
    "objectID": "contents/binary_marginal_effects.html#marginal-effects",
    "href": "contents/binary_marginal_effects.html#marginal-effects",
    "title": "Measuring Marginal Effects on Binary Outcomes",
    "section": "Marginal Effects",
    "text": "Marginal Effects\nThe marginal effect is the effect of a given \\(x_i\\) on \\(y_i\\). In linear models, this effect is constant. However, both logit and probit models are curved. Therefore, the effect of \\(x_i\\) on \\(y_i\\) depends on your \\(x_i\\). As demonstrated in the figure below, the steepest change for both the logit (blue) and probit (red) models occurs around the middle values of \\(x_i\\).\n\n\n\n\n\nLet’s take a look at the effect of setting the closing registration date 20 days prior to the election day. We will start with a logistic regression.\n\nLogistic Regression\nFirst, let’s fit our model:\n\nm1 <- glm(vote ~ close + edu7cat + homeown, data = voters, family = binomial(link = \"logit\"))\n\ntbl_regression(m1, intercept = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      log(OR)1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-0.81\n-2.2, 0.45\n0.2\n    Registration closing\n-0.01\n-0.02, 0.00\n0.2\n    Education level\n\n\n\n    1\n—\n—\n\n    2\n0.46\n-0.84, 1.9\n0.5\n    3\n0.21\n-1.0, 1.6\n0.7\n    4\n0.51\n-0.73, 1.9\n0.4\n    5\n1.2\n0.00, 2.6\n0.055\n    6\n1.9\n0.67, 3.3\n0.003\n    7\n2.2\n0.91, 3.6\n0.001\n    Homeownership\n\n\n\n    Not homeowner\n—\n—\n\n    Homeowner\n0.83\n0.61, 1.0\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\n\nInterpreting the coefficients\nThese coefficients are difficult to interpret because the model is non-linear. With linear regression, we can interpret the coefficient of \\(x_i\\) to be the effect of a one unit change in \\(x_i\\) on the expected value of \\(y\\). This effect is the same for every value of \\(x_i\\).\nWith non-linear logistic regression, we need to transform our coefficients to meaningfully interpret them. Remember, our model estimates the probability of success as:\n\\[\nPr(y_i = 1 | x_i) = logit^{-1}(X_i\\beta)\n\\]\nThe coefficients presented above are therefore log odds ratios. We can interpret their statistical significance and their sign. For example, we know that our independent variable of interest, close, is not statistically significant (\\(p = 0.232\\)). We also know that its effect on an individual’s decision to vote is negative: as days before an election the date of voter registration closes increases, the likelihood that an individual will vote decreases. However, we cannot meaningfully discuss this coefficient (-0.01) without first transforming it.\n\nPredicted probabilities\nWe can use the inverse logit function to discover the predicted probability that an individual will vote for a given set of predictors.\n\\[\nPr(y = 1 | X_i) = \\frac{e^{\\beta_i}}{1 + e^{\\beta_i}}\n\\]\n\n\nOdds ratios\nYou can interpret the coefficient in terms of its odds ratio. If the probability of success of an outcome is \\(p\\) and, therefore, the probability of failure is \\(1-p\\), the the odds of success is \\(\\frac{p}{1-p}\\). Now, dividing two odds by each other gives you the odds ratio. For example, if two outcomes have the odds \\(\\frac{p_1}{1-p_1}\\) and \\(\\frac{p_2}{1-p_2}\\), then these outcomes have an odds ratio of \\(\\frac{\\frac{p_1}{1-p_1}}{\\frac{p_2}{1-p_2}}\\).\nThis is particularly useful for comparing the probability of success and failure for a given value of \\(x_i\\). When the odds ratio is 1, the odds of success are the same as the odds of failure (\\(\\frac{0.5}{0.5} = 1\\)). When the odds ratio is greater than 1, the odds of success are greater than the odds of failure (for example, \\(\\frac{0.8}{0.2} = 4\\)).\nTo get the odds ratio from the coefficients presented above, we exponentiate them:\n\\[\ne^\\beta\n\\]\n\nExponentiation is the opposite operation to log transformation. So, to get from the log odds ratio presented in the table above to the odds ratio, we simply need to get rid of the log (leaving the odds ratio).\n\nFor home ownership:\n\\[\ne^\\beta = e^{0.83} = 2.29\n\\]\nThis means that a homeowner is 2.29 times more likely to vote than a non-homeowner (our reference category), holding all other variables a fixed values.\nFor education level 5:\n\\[\ne^\\beta = e^{1.2} = 3.32\n\\]\nThis means that a person with a level of education in category 5 is 3.32 times more likely to vote than someone with a level of education in category 1 (our reference category), holding all other variables a fixed values. You can calculate this for any education level. The interpretation should always be in reference to your reference category.\nContinuous variables are trickier to interpret. For close:\n\\[\ne^\\beta = e^{-0.006} = 0.994\n\\]\nThis means that a one unit increase in close (increasing the days before the election that registration closes by one day), decreases the odds of voting by a factor of 0.994, holding all other variables a fixed values.\n\nTODO: Check this.\n\n\nTo get the exponential of a number in R, use exp().\n\nHappily, gtsummary::tbl_regression() can easily present these results for us:\n\ntbl_regression(m1, intercept = T, exponentiate = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.45\n0.11, 1.56\n0.2\n    Registration closing\n0.99\n0.98, 1.00\n0.2\n    Education level\n\n\n\n    1\n—\n—\n\n    2\n1.59\n0.43, 6.60\n0.5\n    3\n1.24\n0.35, 4.96\n0.7\n    4\n1.66\n0.48, 6.54\n0.4\n    5\n3.46\n1.00, 13.6\n0.055\n    6\n6.86\n1.95, 27.4\n0.003\n    7\n9.13\n2.49, 38.0\n0.001\n    Homeownership\n\n\n\n    Not homeowner\n—\n—\n\n    Homeowner\n2.28\n1.85, 2.83\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nWe can also get these results programmatically using broom::tidy():\n\ntidy(m1, exponentiate = T)\n\n# A tibble: 9 × 5\n  term             estimate std.error statistic  p.value\n  <chr>               <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)         0.446   0.655      -1.23  2.18e- 1\n2 close               0.994   0.00490    -1.19  2.32e- 1\n3 edu7cat2            1.59    0.679       0.679 4.97e- 1\n4 edu7cat3            1.24    0.658       0.321 7.48e- 1\n5 edu7cat4            1.66    0.645       0.788 4.31e- 1\n6 edu7cat5            3.46    0.647       1.92  5.50e- 2\n7 edu7cat6            6.86    0.656       2.94  3.33e- 3\n8 edu7cat7            9.13    0.679       3.26  1.13e- 3\n9 homeownHomeowner    2.28    0.108       7.62  2.55e-14\n\n\nFinally, we can visualise these results using sjPlot::plot_model():\n\nplot_model(m1, sort.est = T, show.values = T, value.offset = .3)\n\n\n\n\n\n\n\nInterpreting the intercept\nAs usual, the intercept should be interpreted as the expected value when all independent variables are set to 0.\nThis is simple to interpret in terms of the probability of success. Remember:\n\\[\nPr(y = 1 | X_i) = \\frac{e^{\\beta_i}}{1 + e^{\\beta_i}}\n\\]\nTherefore, for our voter model:\n\\[\n\\frac{e^{\\beta_0}}{1 + e^{\\beta_0}} = \\frac{e^{-0.81}}{1 + e^{-0.81}} = \\frac{0.445}{1.445} = 0.308\n\\]\nThe probability that an individual in a state with election day registration, who has an education level of category 1, and who does not own a house is 0.308 or 30.8%."
  },
  {
    "objectID": "contents/binary_substantive_effects.html",
    "href": "contents/binary_substantive_effects.html",
    "title": "Measuring Substantive Effects on Binary Outcomes",
    "section": "",
    "text": "This section uses the following packages:\n\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(skimr)\nlibrary(sjPlot)\nlibrary(broom)\nlibrary(gtsummary)\n\nWe will use the dataset we set up in the previous section:\n\n\n   vote close edu7cat       homeown\n1 Voted    10       6     Homeowner\n2 Voted    29       6     Homeowner\n3 Voted    28       4     Homeowner\n4 Voted     0       5     Homeowner\n5 Voted    25       7 Not homeowner\n6 Voted    25       5 Not homeowner"
  },
  {
    "objectID": "contents/binary_substantive_effects.html#introduction",
    "href": "contents/binary_substantive_effects.html#introduction",
    "title": "Measuring Substantive Effects on Binary Outcomes",
    "section": "Introduction",
    "text": "Introduction\nPreviously, we explored the effect of a given value of \\(x_i\\) on our expected probability. We are rarely interested in this effect in political science. Rather, we are interested in the effect of a meaningful change in \\(x_i\\) on our expected probability. For example, what is the effect of changing the date of voting registration from 20 days prior to election day to election day? This section will outline how we calculate this substantive effect for binary outcomes.\nFor non-linear models, we need to specify our values carefully. Unlike linear models, the effect of a one unit change in \\(x\\) on \\(y\\) is not constant. To illustrate, let’s look at the effect on the probability of success of moving from \\(x = 2\\) to \\(x = 3\\) compared to the effect of moving from \\(x = 4\\) to \\(x = 5\\):\n\n\n\n\n\nMoving from \\(x = 2\\) (highlighted in light blue) to \\(x = 3\\) (highlighted in dark blue) increases the probability of success by 0.057, from 0.027 to 0.084. Moving the same interval of one unit from \\(x = 4\\) (highlighted in pink) to \\(x = 5\\) (highlighted in red) increases the probability of success by 0.267, from 0.231 to 0.498. That’s a 4.706 times increase in the effect of a one unit change in \\(x\\).\nTherefore, to measure the effect of moving from one value of \\(x\\) to another in a non-linear model, we need to know which values of \\(x\\) we are moving between. This should be theoretically driven: what is an interesting interval for the phenomenon you are measuring?"
  },
  {
    "objectID": "contents/binary_substantive_effects.html#measuring-substantive-effects",
    "href": "contents/binary_substantive_effects.html#measuring-substantive-effects",
    "title": "Measuring Substantive Effects on Binary Outcomes",
    "section": "Measuring substantive effects",
    "text": "Measuring substantive effects\nHow do we actually measure the effect of a change from \\(x_{i1}\\) to \\(x_{i2}\\)? We predict the estimated probability of success at \\(x_{i1}\\) and at \\(x_{i2}\\) and subtract those probabilities from one another to get the difference. However, there are two factors that complicate this process. First, we need to deal with the other variables in our model: what values should they be held at while we change our variable of interest, \\(x_i\\)? Second, how do we measure uncertainty surrounding this estimated effect? We will deal with these challenges in turn.\n\nWhat to do with the other independent variables\nThere are two dominant approaches to solving this challenge: the average case approach and the observed value approach.\n\nAverage case approach\nThis approach sets all other values to their mean (for continuous variables) or mode (for discrete variables).\n\nFind the mean or mode for all independent variables other than your variable of interest.\nFind your predicted probability of success with your first value of \\(x_i\\), holding all other variables at their mean or mode.\nFind your predicted probability of success with your second value of \\(x_i\\), holding all other variables at their mean or mode.\nCalculate the difference between these predicted probabilities.\nDiscuss the substantive significance of this difference.\n\nTo illustrate, let’s explore the predicted effect of changing a state’s registration voting date from 20 days prior to election day (\\(close = 20\\)) to election day (\\(close = 0\\)).\nFirst, let’s fit a logistic regression as we did in the previous section:\n\nm1 <- glm(vote ~ close + edu7cat + homeown, data = voters, family = binomial(link = \"logit\"))\n\ntbl_regression(m1, intercept = T, exponentiate = T)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.45\n0.11, 1.56\n0.2\n    Registration closing\n0.99\n0.98, 1.00\n0.2\n    Education level\n\n\n\n    1\n—\n—\n\n    2\n1.59\n0.43, 6.60\n0.5\n    3\n1.24\n0.35, 4.96\n0.7\n    4\n1.66\n0.48, 6.54\n0.4\n    5\n3.46\n1.00, 13.6\n0.055\n    6\n6.86\n1.95, 27.4\n0.003\n    7\n9.13\n2.49, 38.0\n0.001\n    Homeownership\n\n\n\n    Not homeowner\n—\n—\n\n    Homeowner\n2.28\n1.85, 2.83\n<0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nNow, we create a new dataset that contains the values for close we want to test and sets the other independent variables to their mean or mode values:\n\nnew_data <- tibble(\n  close = c(0, 20),\n  edu7cat = voters |> count(edu7cat) |> filter(n == max(n)) |> pull(edu7cat),\n  homeown = voters |> count(homeown) |> filter(n == max(n)) |> pull(homeown)\n)\n\nnew_data\n\n# A tibble: 2 × 3\n  close edu7cat homeown  \n  <dbl> <fct>   <fct>    \n1     0 4       Homeowner\n2    20 4       Homeowner\n\n\nNext, we calculate the predicted probability that an individual will vote, given these values for our independent variables:\n\nresult_0_20 <- augment(m1, newdata = new_data, type.predict = \"response\")\nresult_0_20\n\n# A tibble: 2 × 4\n  close edu7cat homeown   .fitted\n  <dbl> <fct>   <fct>       <dbl>\n1     0 4       Homeowner   0.629\n2    20 4       Homeowner   0.601\n\n\nNext, we calculate the difference between these predicted probabilities, noting that the only thing that changed in our model is the value of close:\n\nresult_0_20 <- mutate(result_0_20, diff = (.fitted - lead(.fitted)) * 100)\nresult_0_20\n\n# A tibble: 2 × 5\n  close edu7cat homeown   .fitted  diff\n  <dbl> <fct>   <fct>       <dbl> <dbl>\n1     0 4       Homeowner   0.629  2.77\n2    20 4       Homeowner   0.601 NA   \n\n\nWe expect that the probability that an individual will vote increases 2.77 percentage points when a state’s closing date for voter registration moves from 20 days prior to election day to election day. If increasing turnout by 2.77 percentage points would swing an election in a candidate’s favour, this is a substantively significant result.1\nHowever, we have a bit of a problem. Is this really a generalisable result? Haven’t we just estimated the effect of this change in registration day closure for a homeowner with an education level of 4? This is the problem with the average case approach: we are discarding an enormous amount of rich data about our sample which is potentially compromising the generalisability of our estimates. In fact, sometimes our average case isn’t even in our dataset, which means that we are making out-of-sample predictions.\n\n\nObserved value approach\nThe observed value approach addresses this issue. It sets all other independent variables to their observed values, only aggregating the estimated effect at the end.\n\nFind your predicted probability of success with your first value of \\(x_i\\), holding all other variables at their observed values. You will get the same number of predictions as you have observations.\nFind your predicted probability of success with your second value of \\(x_i\\), holding all other variables at their observed values.\nCalculate the average predicted probability for each of these values of \\(x_i\\).\nCalculate the difference between these averages.\nDiscuss the substantive significance of this difference.\n\nLet’s explore the same question as above to illustrate.\nFirst, find the predicted probability of an individual voting when \\(close = 20\\):\n\nresult_20 <- augment(m1, newdata = mutate(voters, close = 20), type.predict = \"response\")\nresult_20\n\n# A tibble: 2,446 × 5\n   vote         close edu7cat homeown       .fitted\n   <fct>        <dbl> <fct>   <fct>           <dbl>\n 1 Voted           20 6       Homeowner       0.861\n 2 Voted           20 6       Homeowner       0.861\n 3 Voted           20 4       Homeowner       0.601\n 4 Voted           20 5       Homeowner       0.758\n 5 Voted           20 7       Not homeowner   0.784\n 6 Voted           20 5       Not homeowner   0.579\n 7 Voted           20 7       Homeowner       0.892\n 8 Voted           20 6       Homeowner       0.861\n 9 Voted           20 2       Homeowner       0.590\n10 Did not vote    20 5       Homeowner       0.758\n# … with 2,436 more rows\n\n\nNext, find the predicted probability of an individual voting when \\(close = 0\\):\n\nresult_0 <- augment(m1, newdata = mutate(voters, close = 0), type.predict = \"response\")\nresult_0\n\n# A tibble: 2,446 × 5\n   vote         close edu7cat homeown       .fitted\n   <fct>        <dbl> <fct>   <fct>           <dbl>\n 1 Voted            0 6       Homeowner       0.875\n 2 Voted            0 6       Homeowner       0.875\n 3 Voted            0 4       Homeowner       0.629\n 4 Voted            0 5       Homeowner       0.779\n 5 Voted            0 7       Not homeowner   0.803\n 6 Voted            0 5       Not homeowner   0.607\n 7 Voted            0 7       Homeowner       0.903\n 8 Voted            0 6       Homeowner       0.875\n 9 Voted            0 2       Homeowner       0.618\n10 Did not vote     0 5       Homeowner       0.779\n# … with 2,436 more rows\n\n\nNext, calculate the average predicted probability for \\(close = 20\\) and \\(close = 0\\):\n\navg_result_20 <- summarise(result_20, avg_20 = mean(.fitted))\navg_result_20\n\n# A tibble: 1 × 1\n  avg_20\n   <dbl>\n1  0.664\n\n\n\navg_result_0 <- summarise(result_0, avg_0 = mean(.fitted))\navg_result_0\n\n# A tibble: 1 × 1\n  avg_0\n  <dbl>\n1 0.686\n\n\nNext, calculate the difference between these averages as a percentage:\n\ndiff <- (avg_result_0 - avg_result_20) * 100\ndiff\n\n     avg_0\n1 2.289573\n\n\nWe expect that the probability that an individual will vote increases 2.29 percentage points when a state’s closing date for voter registration moves from 20 days prior to election day to election day. If this increase in turnout would swing an election in a candidate’s favour, this is a substantively significant result.\n\n\nWhich approach should you use?\nYou should use the observed values approach. Hanmer and Kalkan (2013) demonstrate using simulated data that the observed values approach consistently produces estimates closer to the population’s true probability than the average case approach. This makes sense: you are using more data to produce your estimated effects."
  }
]